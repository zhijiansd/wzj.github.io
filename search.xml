<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[通过Dex和openLDAP进行Kubernetes身份验证]]></title>
    <url>%2Fblog%2Fdex%2F</url>
    <content type="text"><![CDATA[Dex是一种身份服务，它使用OpenID Connect(简称OIDC)来驱动其他应用程序的身份验证。 Dex通过“connectors.”充当其他身份提供商的门户。这使得dex可以将身份验证延迟(找不到很好的词来形容，只能硬翻了)到LDAP服务器、SAML提供程序或已建立的身份提供程序（如GitHub，Google和Active Directory）。客户端编写一次身份验证逻辑与dex通信，然后dex处理给定后端的协议。 OAuth2OAuth（开放授权）是一个开放标准，允许用户授权第三方移动应用访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方移动应用或分享他们数据的所有内容。 我相信大家都使用过类似“使用QQ登录”诸如此类的按钮来登录一些第三方的应用或者网站。在这些情况下，第三方应用程序(网站)选择让外部提供商（在这种情况下为QQ）证明您的身份，而不是让您使用应用程序本身设置用户名和密码。 服务器端应用程序的一般流程是： 新用户访问应用程序。 用户点击网站上的登录按钮(诸如“使用QQ登录”)，该应用程序将用户重定向到QQ。 用户登录QQ，然后QQ会提示该应用程序会获取的相应权限。 如果用户单击“授权并登录”，则QQ会连同获取的Access Token使用代码将用户重定向回该应用程序。 应用程序通过Access Token获取用户的OpenID；调用OpenAPI，来请求访问或修改用户授权的资源。 在这些情况下，dex充当QQ（在OpenID Connect中称为“provider”），而客户端应用程序重定向到它以获得最终用户的身份。 关于OAuth: https://oauth.net/2/ ID TokensID Tokens是OpenID Connect和dex主要功能引入的OAuth2扩展。ID Tokens是由dex签名的JSON Web令牌（JWT），作为OAuth2响应的一部分返回，用于证明最终用户的身份。 OpenID Connect的OAuth2主要扩展名是令牌响应中返回的额外令牌，称为ID Tokens。此令牌是由OpenID Connect服务器签名的JSON Web令牌，具有用户ID，名称，电子邮件等众所周知的字段。 Connectors当用户通过dex登录时，用户的身份通常存储在另一个用户管理系统中：LDAP目录，GitHub组织等。Dex充当客户端应用程序和上游身份提供者之间的中间人。客户端只需要了解OpenID Connect来查询dex，而dex实现了一组用于查询其他用户管理系统的协议。 OpenID Connect TokensOpenID Connect 1.0是OAuth 2.0协议之上的简单身份层。它允许客户端根据授权服务器执行的身份验证来验证最终用户的身份，以及以可互操作和类似REST的方式获取有关最终用户的基本配置文件信息。 OpenID Connect允许所有类型的客户端（包括基于Web，移动和JavaScript客户端）请求和接收有关经过身份验证的会话和最终用户的信息。规范套件是可扩展的，允许参与者在对它们有意义时使用可选功能，例如身份数据加密，OpenID提供程序的发现和会话管理。 协议的OAuth2的主要扩展是返回的附加字段，其中访问令牌称为ID Token。此令牌是JSON Web令牌（JWT），具有由服务器签名的众所周知的字段，例如用户的电子邮件。 为了识别用户，验证者使用OAuth2 令牌响应中的id_token（而不是access_token） 作为承载令牌。 来自OpenID Connect提供商的令牌响应包括一个称为ID令牌的签名JWT。ID令牌包含名称，电子邮件，唯一标识符，在dex的情况下，包含一组可用于标识用户的组。像dex这样的OpenID Connect提供程序发布公钥; Kubernetes API服务器了解如何使用它们来验证ID令牌。 关于OpenID Connect: https://openid.net/connect/ 身份验证流程如下所示： OAuth2客户端通过dex登录用户。 在与Kubernetes API通信时，该客户端使用返回的ID令牌作为承载令牌。 Kubernetes使用dex的公钥来验证ID令牌。 指定为用户名（以及可选的组信息）的声明将与该请求相关联。 用户名和组信息可以与Kubernetes 授权插件（例如基于角色的访问控制（RBAC））结合使用以实施策略。 dex有自己的用户概念，但它允许它们以不同的方式进行身份验证，称为connectors。目前，dex提供两种类型的连接器：local连接器和OIDC连接器。使用local连接器进行身份验证时，用户使用电子邮件和密码登录，并使用dex本身提供的可自定义UI。使用OIDC连接器，用户可以通过登录到另一个OIDC身份提供商（如Google或Salesforce）进行身份验证。 从dex请求ID令牌直接使用dex对用户进行身份验证的应用程序使用OAuth2代码流来请求令牌响应。采取的确切步骤是： 用户访问客户端应用。 客户端应用程序通过OAuth2请求将用户重定向到dex。 Dex确定用户的身份。 Dex使用代码将用户重定向到客户端。 客户端使用dex为id_token交换代码。 部署 openLDAP配置PVC12345678910111213141516171819202122232425262728293031323334# vim openldap-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-data namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi"---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-config namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "1Gi"# kubectl create -f openldap-pvc.yaml persistentvolumeclaim/ldap-data createdpersistentvolumeclaim/ldap-config created 部署openLDAP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# vim openldap.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ldap labels: app: ldapspec: replicas: 1 template: metadata: labels: app: ldap spec: containers: - name: ldap image: 192.168.100.100/library/openldap:1.2.2 volumeMounts: - name: ldap-data mountPath: /var/lib/ldap - name: ldap-config mountPath: /etc/ldap/slapd.d - name: ldap-certs mountPath: /container/service/slapd/assets/certs ports: - containerPort: 389 name: openldap env: - name: LDAP_LOG_LEVEL value: "256" - name: LDAP_ORGANISATION value: "zhi" - name: LDAP_DOMAIN value: "flywzj.com" - name: LDAP_ADMIN_PASSWORD value: "admin" - name: LDAP_CONFIG_PASSWORD value: "config" - name: LDAP_READONLY_USER value: "false" - name: LDAP_READONLY_USER_USERNAME value: "readonly" - name: LDAP_READONLY_USER_PASSWORD value: "readonly" - name: LDAP_RFC2307BIS_SCHEMA value: "false" - name: LDAP_BACKEND value: "mdb" - name: LDAP_TLS value: "true" - name: LDAP_TLS_CRT_FILENAME value: "ldap.crt" - name: LDAP_TLS_KEY_FILENAME value: "ldap.key" - name: LDAP_TLS_CA_CRT_FILENAME value: "ca.crt" - name: LDAP_TLS_ENFORCE value: "false" - name: LDAP_TLS_CIPHER_SUITE value: "SECURE256:+SECURE128:-VERS-TLS-ALL:+VERS-TLS1.2:-RSA:-DHE-DSS:-CAMELLIA-128-CBC:-CAMELLIA-256-CBC" - name: LDAP_TLS_VERIFY_CLIENT value: "demand" - name: LDAP_REPLICATION value: "false" - name: LDAP_REPLICATION_CONFIG_SYNCPROV value: "binddn=\"cn=admin,cn=config\" bindmethod=simple credentials=$LDAP_CONFIG_PASSWORD searchbase=\"cn=config\" type=refreshAndPersist retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_DB_SYNCPROV value: "binddn=\"cn=admin,$LDAP_BASE_DN\" bindmethod=simple credentials=$LDAP_ADMIN_PASSWORD searchbase=\"$LDAP_BASE_DN\" type=refreshAndPersist interval=00:00:00:10 retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_HOSTS value: "#PYTHON2BASH:['ldap://ldap-one-service', 'ldap://ldap-two-service']" - name: KEEP_EXISTING_CONFIG value: "false" - name: LDAP_REMOVE_CONFIG_AFTER_SETUP value: "true" - name: LDAP_SSL_HELPER_PREFIX value: "ldap" volumes: - name: ldap-data persistentVolumeClaim: claimName: ldap-data #hostPath: # path: "/data/ldap/db" - name: ldap-config persistentVolumeClaim: claimName: ldap-config #hostPath: # path: "/data/ldap/config" - name: ldap-certs hostPath: path: "/data/ldap/certs"---apiVersion: v1kind: Servicemetadata: labels: app: ldap name: ldapspec: type: NodePort ports: - port: 389 nodePort: 38989 selector: app: ldap# kubectl create -f openldap.yaml deployment.extensions/ldap createdservice/ldap created# kubectl get pods -l app=ldapNAME READY STATUS RESTARTS AGEldap-65f5786ff8-xrtkk 1/1 Running 0 28d 部署openldapadmin12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# vim phpldapadmin.yaml apiVersion: v1kind: ReplicationControllermetadata: name: phpldapadmin-controller labels: app: phpldapadminspec: replicas: 1 selector: app: phpldapadmin template: metadata: labels: app: phpldapadmin spec: containers: - name: phpldapadmin image: 192.168.100.100/library/phpldapadmin:0.7.2 volumeMounts: - name: phpldapadmin-certs mountPath: /container/service/phpldapadmin/assets/apache2/certs - name: ldap-client-certs mountPath: /container/service/ldap-client/assets/certs ports: - containerPort: 443 env: - name: PHPLDAPADMIN_LDAP_HOSTS #value: "#PYTHON2BASH:[&#123;'ldap-service': [&#123;'server': [&#123;'tls': 'true'&#125;]&#125;]&#125;]" value: "ldap" - name: PHPLDAPADMIN_SERVER_ADMIN value: "wangzhijiansd@qq.com" - name: PHPLDAPADMIN_SERVER_PATH value: "/phpldapadmin" - name: PHPLDAPADMIN_HTTPS value: "true" - name: PHPLDAPADMIN_HTTPS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_HTTPS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_HTTPS_CA_CRT_FILENAME value: "ca.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS value: "true" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_REQCERT value: "demand" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CA_CRT_FILENAME value: "ca.crt" volumes: - name: phpldapadmin-certs hostPath: path: "/data/phpldapadmin/ssl/" - name: ldap-client-certs hostPath: path: "/data/phpldapadmin/ldap-client-certs/"---apiVersion: v1kind: Servicemetadata: labels: app: phpldapadmin name: phpldapadminspec: type: NodePort ports: - port: 443 nodePort: 32001 selector: app: phpldapadmin# kubectl create -f phpldapadmin.yaml replicationcontroller/phpldapadmin-controller createdservice/phpldapadmin created# kubectl get pods -l app=phpldapadminNAME READY STATUS RESTARTS AGEphpldapadmin-controller-4cwlb 1/1 Running 0 28d 关于 openLDAP 的相关镜像详见: https://github.com/osixia/docker-openldap https://github.com/osixia/docker-openldap-backup https://github.com/osixia/docker-phpLDAPadmin 配置 openLDAP使用 phpldapadmin 进行配置管理 输入 https://nodeip:nodeport 登录 login: Login DN: cn=admin,dc=flywzj,dc=com Password: admin 点击 Authenticate 登录 登录LDAP容器进行配置管理 注1:Dex目前允许不安全的连接，但是Dex官方强烈建议使用TLS，通过使用端口636而不是389来实现。这里使用的是不安全的389端口来实现，请知悉。 注2:这里配置两个组，组k8s关联用户wang，组test关联用户zhi 查看当前LDAP配置1234567891011121314151617181920212223242526272829303132# kubectl exec -it ldap-65f5786ff8-xrtkk /bin/bashroot@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NFpVZU5IaGhDSzZ4OWF2KzBCSjlZOUY4SzRhWTdpWUk=# search resultsearch: 2result: 0 Success# numResponses: 3# numEntries: 2 配置新建OU和组123456789101112131415161718192021222324252627282930313233root@ldap-65f5786ff8-xrtkk:/# cat &lt;&lt;EOF &gt; container/service/slapd/assets/groups.ldifdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: topdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: topdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberuid: wangdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberuid: zhiEOFroot@ldap-65f5786ff8-xrtkk:/# ldapadd -x -D "cn=admin,dc=flywzj,dc=com" -w admin -f /container/service/slapd/assets/groups.ldif -H ldap:/// adding new entry "ou=Groups,dc=flywzj,dc=com"adding new entry "ou=People,dc=flywzj,dc=com"adding new entry "cn=k8s,ou=Groups,dc=flywzj,dc=com"adding new entry "cn=test,ou=Groups,dc=flywzj,dc=com" 如上可以看出，这里新建了两个OU: ou=Groups,dc=flywzj,dc=com ou=People,dc=flywzj,dc=com 同时新建了两个组: cn=k8s,ou=Groups,dc=flywzj,dc=com cn=test,ou=Groups,dc=flywzj,dc=com 特别说明，所有配置是都可以放在一个 ldif 文件中来进行配置的，这里分成两个 ldif 文件来配置就是为了方便理解和排版。 查看配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859root@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NHNid2tMZCtnSS9LazJieGRsdWdhZFN1OGR0ZE4wVjA=# Groups, flywzj.comdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, flywzj.comdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, flywzj.comdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberUid: wang# test, Groups, flywzj.comdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberUid: zhi# search resultsearch: 2result: 0 Success# numResponses: 9# numEntries: 8 配置用户123456789101112131415161718192021222324252627282930313233343536373839# cat &lt;&lt;EOF &gt; /container/service/slapd/assets/users.ldif dn: uid=wang,ou=People,dc=flywzj,dc=comcn: wanggidnumber: 500givenname: wanghomedirectory: /home/users/wangloginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: wangzhijiansd@qq.comsn: wanguid: wanguidnumber: 1000userpassword: wangzhijiandn: uid=zhi,ou=People,dc=flywzj,dc=comhomedirectory: /home/users/zhiloginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: zhijiansd@163.comcn: zhigivenname: zhisn: zhiuid: zhiuidnumber: 1001gidnumber: 501userpassword: zhijianEOFroot@ldap-65f5786ff8-xrtkk:/# ldapadd -x -D "cn=admin,dc=flywzj,dc=com" -w admin -f /container/service/slapd/assets/users.ldif -H ldap:/// adding new entry "uid=wang,ou=People,dc=flywzj,dc=com"adding new entry "uid=zhi,ou=People,dc=flywzj,dc=com" 查看配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495root@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NHNid2tMZCtnSS9LazJieGRsdWdhZFN1OGR0ZE4wVjA=# Groups, flywzj.comdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, flywzj.comdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, flywzj.comdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberUid: wang# test, Groups, flywzj.comdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberUid: zhi# wang, People, flywzj.comdn: uid=wang,ou=People,dc=flywzj,dc=comcn: wanggidNumber: 500givenName: wanghomeDirectory: /home/users/wangloginShell: /bin/shobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: wangzhijiansd@qq.comsn: wanguid: wanguidNumber: 1000userPassword:: d2FuZ3poaWppYW4=# zhi, People, flywzj.comdn: uid=zhi,ou=People,dc=flywzj,dc=comhomeDirectory: /home/users/zhiloginShell: /bin/shobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: zhijiansd@163.comcn: zhigivenName: zhisn: zhiuid: zhiuidNumber: 1001gidNumber: 501userPassword:: emhpamlhbg==# search resultsearch: 2result: 0 Success# numResponses: 9# numEntries: 8 这时候你也可以使用 phpldapadmin 登录查看，特别说明，如果登录进去出现了一些”?”提示，那是因为某些模板没有导入导致的，可忽略。 部署 Dex Dex详见:https://github.com/dexidp/dex 生成证书并配置secret 生成dex server和login application相关证书和secret 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# vim gencert.sh#!/bin/bashmkdir -p sslcat &lt;&lt; EOF &gt; ssl/req.cnf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = dexDNS.2 = dex.svc.cluster.localDNS.3 = loginappDNS.4 = loginapp.svc.cluster.localDNS.5 = login.flywzj.comIP.1 = 192.168.100.181IP.2 = 192.168.100.182IP.3 = 192.168.100.183EOFopenssl genrsa -out ssl/dex-ca-key.pem 2048openssl req -x509 -new -nodes -key ssl/dex-ca-key.pem -days 1000 -out ssl/dex-ca.pem -subj "/CN=kube-ca"openssl genrsa -out ssl/dex-app-key.pem 2048openssl req -new -key ssl/dex-app-key.pem -out ssl/dex-app-csr.pem -subj "/CN=kube-ca" -config ssl/req.cnfopenssl x509 -req -in ssl/dex-app-csr.pem -CA ssl/dex-ca.pem -CAkey ssl/dex-ca-key.pem -CAcreateserial -out ssl/dex-app.pem -days 1000 -extensions v3_req -extfile ssl/req.cnfkubectl create secret tls dex --cert=ssl/dex-app.pem --key=ssl/dex-app-key.pem kubectl create secret tls loginapp --cert=ssl/dex-app.pem --key=ssl/dex-app-key.pem # ./gencert.sh Generating RSA private key, 2048 bit long modulus........................+++.......................................+++e is 65537 (0x10001)Generating RSA private key, 2048 bit long modulus...........................................+++........................+++e is 65537 (0x10001)Signature oksubject=/CN=kube-caGetting CA Private Keysecret/dex createdsecret/loginapp created# kubectl get secret dexNAME TYPE DATA AGEdex kubernetes.io/tls 2 8h# kubectl get secret loginappNAME TYPE DATA AGEloginapp kubernetes.io/tls 2 8h 复制证书 用于为Dex签署SSL证书的CA文件需要复制到apiserver可以读取的位置 1# cp ssl/dex-ca.pem /etc/kubernetes/ssl/ 部署 Dex123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163# wget https://raw.githubusercontent.com/dexidp/dex/master/examples/k8s/dex.yaml# vim dex.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: dex name: dex namespace: default spec: replicas: 1 template: metadata: labels: app: dex spec: serviceAccountName: dex # This is created below containers: - image: 192.168.100.100/coreos/dex:v2.10.0 name: dex command: ["/usr/local/bin/dex", "serve", "/etc/dex/cfg/config.yaml"] ports: - name: https containerPort: 5556 volumeMounts: - name: config mountPath: /etc/dex/cfg - name: tls mountPath: /etc/dex/tls volumes: - name: config configMap: name: dex items: - key: config.yaml path: config.yaml - name: tls secret: secretName: dex---kind: ConfigMapapiVersion: v1metadata: name: dex namespace: default data: config.yaml: | issuer: https://192.168.100.185:32000 storage: type: kubernetes config: inCluster: true web: https: 0.0.0.0:5556 tlsCert: /etc/dex/tls/tls.crt tlsKey: /etc/dex/tls/tls.key logger: level: "debug" format: text connectors: - type: ldap id: ldap name: LDAP config: host: ldap:389 insecureNoSSL: true insecureSkipVerify: true bindDN: cn=admin,dc=flywzj,dc=com bindPW: admin userSearch: baseDN: ou=People,dc=flywzj,dc=com filter: "(objectClass=posixAccount)" username: mail idAttr: uid emailAttr: mail nameAttr: uid groupSearch: baseDN: ou=Groups,dc=flywzj,dc=com filter: "(objectClass=posixGroup)" userAttr: uid groupAttr: memberUid nameAttr: cn oauth2: skipApprovalScreen: true staticClients: - id: login redirectURIs: - 'https://192.168.100.185:32002/callback' name: 'Login App' secret: 4TORGiNV9M54BTk1v7dNuFSaI6hUjfjr enablePasswordDB: true staticPasswords: - email: "wangzhijiansd@qq.com" # bcrypt hash of the string "password" hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W" username: "admin" userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"---apiVersion: v1kind: Servicemetadata: name: dex namespace: default spec: type: NodePort ports: - name: dex port: 5556 protocol: TCP targetPort: 5556 nodePort: 32000 selector: app: dex---apiVersion: v1kind: ServiceAccountmetadata: labels: app: dex name: dex namespace: default ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: dexrules:- apiGroups: ["dex.coreos.com"] # API group created by dex resources: ["*"] verbs: ["*"]- apiGroups: ["apiextensions.k8s.io"] resources: ["customresourcedefinitions"] verbs: ["create"] # To manage its own resources, dex must be able to create customresourcedefinitions---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: dexroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: dexsubjects:- kind: ServiceAccount name: dex # Service account assigned to the dex pod, created above namespace: default # The namespace dex is running in# kubectl create -f dex.yaml deployment.extensions/dex createdconfigmap/dex createdservice/dex createdserviceaccount/dex createdclusterrole.rbac.authorization.k8s.io/dex createdclusterrolebinding.rbac.authorization.k8s.io/dex created# kubectl get pod --show-labels -l app=dex# kubectl get services dex 注意1: yaml文件虽然是从官方的，但是做了一些改动，特别是这里将官方的3个副本更改为了1个副本，因为使用3个副本事会产生错误，我在issue中翻阅到有说是NTP时钟不同步造成的。 注意2: 这里的 connectors 是LDAP，详细文档见:https://github.com/dexidp/dex/blob/master/Documentation/connectors/ldap.md 查看 OpenID Connect 发现12345678910111213141516171819202122232425262728293031323334353637# curl -k https://192.168.100.185:32000/.well-known/openid-configuration&#123; "issuer": "https://192.168.100.185:32000", "authorization_endpoint": "https://192.168.100.185:32000/auth", "token_endpoint": "https://192.168.100.185:32000/token", "jwks_uri": "https://192.168.100.185:32000/keys", "response_types_supported": [ "code" ], "subject_types_supported": [ "public" ], "id_token_signing_alg_values_supported": [ "RS256" ], "scopes_supported": [ "openid", "email", "groups", "profile", "offline_access" ], "token_endpoint_auth_methods_supported": [ "client_secret_basic" ], "claims_supported": [ "aud", "email", "email_verified", "exp", "iat", "iss", "locale", "name", "sub" ]&#125; 查看 JSON Web Key123456789101112# curl -k https://192.168.100.185:32000/keys&#123; "keys": [ &#123; "use": "sig", "kty": "RSA", "kid": "a813de5c6100949abc59317714e3b09abecf8641", "alg": "RS256", "n": "u7G_RoZEuDwiW7kLBCMjjJMm1NgnHIXiTznxABe3uW8GsdASqRhUsDH2zFceZZObKchHWrKpkPZS4SjvcThF785xoJ4-FlAcrsUd4agyN9uwrAeL_luOrXvl-i0QAUKIHlqbTfZmzBIaFhHnG0yXKgqkXzTarQxDeynWVrVTdWsm7P_BYjQ5dnIlZu1xeRzw-NWf5UAi9Csh1x82XMtlAbMgWlJoWI36yVCCGUdJYintSp-tOfjkPBUghIO7ju8fb22X5uOgRFMq_RkIpXs2asf5FapVQMpcX_WAK3vUhmfH5F0lQZ9Cv9U__k3rHKRS7XwkcSQ4OKf7Vxrx4LQEcQ", "e": "AQAB" &#125;&#125; 配置 Login App 一旦启动并运行dex，下一步就是编写使用dex驱动身份验证的应用程序。具体详见:https://github.com/dexidp/dex/blob/master/Documentation/using-dex.md。 这里我们使用 loginapp 来配置:https://github.com/fydrah/loginapp。 配置 Configmap12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# cat ssl/dex-ca.pem -----BEGIN CERTIFICATE-----MIIC9zCCAd+gAwIBAgIJAJPzlo1fzzxqMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNVBAMMB2t1YmUtY2EwHhcNMTkwMTA0MDYyNzM5WhcNMjEwOTMwMDYyNzM5WjASMRAwDgYDVQQDDAdrdWJlLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxnFM6sykmKdmr1Z4DADujeIUZXvz5ajy+xdjpfjrNhnr3002GsthAlN6lWjJP6dnuVN20M1W/oozF7OzMuSRNO6zLepAG2TrRl/1DZG9EFSN7m65HtxK0DA3RyZc4CuDj3ADT899yziyaVTvUjR8MxLWHeibgAanZ2Bdc8icd4zt7QCGmy3hYbZZMw6UhSlgKUT24m6hw+W167knbjG/U64x1Qzik0DCx0fqY26jdJVZN6AnOpAFoIJ6RNIS1X/fbPa5kzUeTNSbgCW64LS0uOtXBh9uICxaxhHthgKtcOXFn7UrLWVnMfmd0fVi3nV3Tu5j8swrXlDVX+vaTJvDawIDAQABo1AwTjAdBgNVHQ4EFgQUv7Ta12WZE11NQhjpFOWxHG+wcZUwHwYDVR0jBBgwFoAUv7Ta12WZE11NQhjpFOWxHG+wcZUwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAsixLWA/28uVQMPfsWAv2cfdJMMCCvI6Y7uXsLUrNiONG6Ay/pU+6Qc/2kmZJlo0bHLx7P8ncKNHyTPzO1IzMDvvu65TUFdNAnKkqK90IRfjwY+RJv2J2NEKXmWRCxaJG522Uc0aGaYo6BPfrrYInr1fTh6i+/ovF4smyCo2bMX1dI5i7TlhAD9qsOA36XVYg+w5w7jXoJTYh14oKsE23pXpTiuSah6KdnvrjilwuBpV/SG40TsnITwyeaQFWMwbzJkhbvMgRK2crRruUhcIumIE5TTfLVgKzJ5oisjgjrZ3oFw871L2HTPFI3C6Xpp6yV6LNpFGOizT8HTLp63CwQw==-----END CERTIFICATE-----# vim ca-cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: ca namespace: defaultdata: ca.pem: | -----BEGIN CERTIFICATE----- MIIC9zCCAd+gAwIBAgIJAJPzlo1fzzxqMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV BAMMB2t1YmUtY2EwHhcNMTkwMTA0MDYyNzM5WhcNMjEwOTMwMDYyNzM5WjASMRAw DgYDVQQDDAdrdWJlLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA xnFM6sykmKdmr1Z4DADujeIUZXvz5ajy+xdjpfjrNhnr3002GsthAlN6lWjJP6dn uVN20M1W/oozF7OzMuSRNO6zLepAG2TrRl/1DZG9EFSN7m65HtxK0DA3RyZc4CuD j3ADT899yziyaVTvUjR8MxLWHeibgAanZ2Bdc8icd4zt7QCGmy3hYbZZMw6UhSlg KUT24m6hw+W167knbjG/U64x1Qzik0DCx0fqY26jdJVZN6AnOpAFoIJ6RNIS1X/f bPa5kzUeTNSbgCW64LS0uOtXBh9uICxaxhHthgKtcOXFn7UrLWVnMfmd0fVi3nV3 Tu5j8swrXlDVX+vaTJvDawIDAQABo1AwTjAdBgNVHQ4EFgQUv7Ta12WZE11NQhjp FOWxHG+wcZUwHwYDVR0jBBgwFoAUv7Ta12WZE11NQhjpFOWxHG+wcZUwDAYDVR0T BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAsixLWA/28uVQMPfsWAv2cfdJMMCC vI6Y7uXsLUrNiONG6Ay/pU+6Qc/2kmZJlo0bHLx7P8ncKNHyTPzO1IzMDvvu65TU FdNAnKkqK90IRfjwY+RJv2J2NEKXmWRCxaJG522Uc0aGaYo6BPfrrYInr1fTh6i+ /ovF4smyCo2bMX1dI5i7TlhAD9qsOA36XVYg+w5w7jXoJTYh14oKsE23pXpTiuSa h6KdnvrjilwuBpV/SG40TsnITwyeaQFWMwbzJkhbvMgRK2crRruUhcIumIE5TTfL VgKzJ5oisjgjrZ3oFw871L2HTPFI3C6Xpp6yV6LNpFGOizT8HTLp63CwQw== -----END CERTIFICATE-----# kubectl create -f ca-cm.yaml configmap/ca created# kubectl get configmap caNAME DATA AGEca 1 27d 配置 Login App Configmap123456789101112131415161718192021222324252627# vim loginapp-cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: loginapp namespace: defaultdata: config.yaml: | debug: false client_id: "login" client_secret: 4TORGiNV9M54BTk1v7dNuFSaI6hUjfjr issuer_url: "https://192.168.100.185:32000" issuer_root_ca: "/etc/ssl/ca.pem" redirect_url: "https://192.168.100.185:32002/callback" tls_enabled: true tls_cert: "/etc/loginapp/tls/tls.crt" tls_key: "/etc/loginapp/tls/tls.key" listen: "https://0.0.0.0:5555" disable_choices: false extra_scopes: "groups" name: "Kubernetes Auth"# kubectl create -f loginapp-cm.yaml configmap/loginapp created# kubectl get configmap loginappNAME DATA AGEloginapp 1 24d 部署 Login App1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# vim loginapp-deploy.yml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: loginapp namespace: defaultspec: replicas: 3 template: metadata: labels: app: loginapp spec: containers: - image: 192.168.100.100/library/login-app:latest name: loginapp ports: - name: https containerPort: 5555 volumeMounts: - name: ca mountPath: /etc/ssl/ - name: config mountPath: /app/ - name: tls mountPath: /etc/loginapp/tls volumes: - name: ca configMap: name: ca items: - key: ca.pem path: ca.pem - name: config configMap: name: loginapp items: - key: config.yaml path: config.yaml - name: tls secret: secretName: loginapp---apiVersion: v1kind: Servicemetadata: name: loginapp namespace: defaultspec: type: NodePort ports: - name: loginapp port: 5555 protocol: TCP targetPort: 5555 nodePort: 32002 selector: app: loginapp# kubectl create -f loginapp-deploy.yml# kubectl get pod --show-labels -l app=loginapp# kubectl get service loginapp 配置 kubernetes 配置K8s Apiserver以使用OpenID Connect 身份验证插件，详见:https://github.com/dexidp/dex/blob/master/Documentation/kubernetes.md 配置 kube-apiserver12345--oidc-issuer-url=https://192.168.100.185:32000--oidc-client-id=loginapp--oidc-ca-file=/etc/kubernetes/ssl/dex-ca.pem--oidc-username-claim=email--oidc-groups-claim=groups 配置 RBAC赋予 k8s 组 cluster-admin 角色123456789101112131415# vim k8s.yml apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: ldap-cluster-admin namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: Group name: k8s# kubectl create -f k8s.yml clusterrolebinding.rbac.authorization.k8s.io/ldap-cluster-admin created 赋予 test 组相应权限123456789101112131415161718192021222324252627# vim test.yaml kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: ldap-test namespace: testrules:- apiGroups: [""] resources: ["pods"] verbs: ["get", "watch", "list"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: ldap-test namespace: testsubjects:- kind: Group name: test apiGroup: ""roleRef: kind: Role name: ldap-test apiGroup: ""# kubectl create -f test.yaml role.rbac.authorization.k8s.io/ldap-test createdrolebinding.rbac.authorization.k8s.io/ldap-test created 登录 Dex 获取 ID Tokens 浏览器输入 https://192.168.100.185:32002/ (loginapp的nodeport)进行登录 Authentication for clients : login(login 对应之前 dex 和 Login App Configmap 的配置) 点击 “Request Token” 进行登录 之后，会跳转至 Dex 的网址 https://192.168.100.185:32000 点击 “Log in with LDAP” 输入 Username 和 Password ，点击 “Login” 登录生成 id-token 根据提示将文件复制至~/.kube/config ID Tokens 简单解释 id-token 实际上有三个部分，每个部分都是Base64编码的JSON，以”.”来分割。第一部分提供令牌的元数据。第二部分提供身份信息，这称为有效负载。第三部分是签名，用于验证令牌是否由可信方发出。 安装包以使用 jq 命令1# yum -y install jq 解码第一部分12345# echo eyJhbGciOiJSUzI1NiIsImtpZCI6Ijg1MzQ0ZTZlYjk4N2Y5ODA2MjRhODY2MTM2ZWFmOTFmNjFkNDNlYWEifQ | base64 -d | jq&#123; "alg": "RS256", "kid": "85344e6eb987f980624a866136eaf91f61d43eaa"&#125; 解码第二部分12345678910111213141516# echo eyJpc3MiOiJodHRwczovLzE5Mi4xNjguMTAwLjE4NTozMjAwMCIsInN1YiI6IkNnUjNZVzVuRWdSc1pHRnciLCJhdWQiOiJsb2dpbiIsImV4cCI6MTU0Nzc5MTAzMCwiaWF0IjoxNTQ3NzA0NjMwLCJhenAiOiJsb2dpbiIsImF0X2hhc2giOiJvTzBLcTBTYy1qdE9ybVpUbTJpbG5RIiwiZW1haWwiOiJ3YW5nemhpamlhbnNkQHFxLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJncm91cHMiOlsiazhzIl0sIm5hbWUiOiJ3YW5nIn0 | base64 -d | jq&#123; "iss": "https://192.168.100.185:32000", "sub": "CgR3YW5nEgRsZGFw", "aud": "login", "exp": 1547791030, "iat": 1547704630, "azp": "login", "at_hash": "oO0Kq0Sc-jtOrmZTm2ilnQ", "email": "wangzhijiansd@qq.com", "email_verified": true, "groups": [ "k8s" ], "name": "wang"&#125; 检查用户权限 根据之前配置的 RBAC 来测试用户拥有的权限 测试用户 wang12345# kubectl get nodes --user=wangNAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 174d v1.13.0node02 Ready &lt;none&gt; 174d v1.13.0node03 Ready &lt;none&gt; 174d v1.13.0 测试用户 zhi123456# kubectl get nodes --user=zhiError from server (Forbidden): nodes is forbidden: User &quot;zhijiansd@163.com&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope# kubectl get pod -n test --user=zhiNAME READY STATUS RESTARTS AGEtest-nginx-75677f8b58-p8w6d 1/1 Running 0 35h 最后要感谢如下文章及其作者: https://kairen.github.io/2018/04/15/kubernetes/k8s-integration-ldap/ https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/ https://github.com/ObjectifLibre/k8s-ldap https://thenewstack.io/author/joel-speed/]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>dex</tag>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署openLDAP]]></title>
    <url>%2Fblog%2Fopenldap%2F</url>
    <content type="text"><![CDATA[LDAP 代表 轻量级目录访问协议。顾名思义，它是一种用于访问目录服务的基于X.500协议的轻量级目录服务。 LDAP信息模型基于条目。条目是具有全局唯一性的属性集合专有名称（DN）。DN用于明确指代条目。每个条目的属性都有一个类型和一个或多个值。这些类型通常是助记符字符串，例如“ cn ”表示公用名，或“ mail ”表示电子邮件地址。值的语法取决于属性类型。 在LDAP中，目录条目以分层树状结构排列。此外，LDAP允许您通过使用名为objectClass的特殊属性来控制条目中所需和允许的属性。objectClass属性的值确定条目必须遵守的模式规则。 更多简介请访问:http://www.openldap.org/doc/admin24/intro.html 部署 openLDAP安装 openLDAP12# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo# yum -y install openldap-servers openldap-clients openldap-devel OpenLDAP 2.3及更高版本已转换为使用动态运行时配置引擎slapd-config:http://www.openldap.org/doc/admin24/slapdconf2.html 生成LDAP密码12# slappasswd -s zhijian&#123;SSHA&#125;pQdy+1y8IfIw9ZgIExIdOsjC/tqsmb86 复制相关文件12# cp /usr/share/openldap-servers/slapd.ldif /etc/openldap/# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG 配置 slapd.ldif12345678910111213141516171819202122232425# vim /etc/openldap/slapd.ldif ## Server status monitoring#dn: olcDatabase=monitor,cn=configobjectClass: olcDatabaseConfigolcDatabase: monitorolcAccess: to * by dn.base="gidNumber=0+uidNumber=0,cn=peercred,cn=external,c n=auth" read by dn.base="cn=admin,dc=openldap,dc=flywzj,dc=com" read by * none## Backend database definitions#dn: olcDatabase=hdb,cn=configobjectClass: olcDatabaseConfigobjectClass: olcHdbConfigolcDatabase: hdbolcSuffix: dc=openldap,dc=flywzj,dc=comolcRootDN: cn=admin,dc=openldap,dc=flywzj,dc=comolcRootPW: &#123;SSHA&#125;OLvPaV6PzzgRSCDivSzY8xVwk4fEWfZ9olcDbDirectory: /var/lib/ldapolcDbIndex: objectClass eq,presolcDbIndex: ou,cn,mail,surname,givenname eq,pres,sub 测试配置12# slaptest -u config file testing succeeded 删除原始配置文件并重置配置123# rm -rf /etc/openldap/slapd.d/*# slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif# chown -R ldap:ldap /etc/openldap/slapd.d/ 启用 openLDAP12# systemctl start slapd# systemctl status slapd 检查服务器是否正在运行并正确配置12345678910111213141516171819# ldapsearch -x -b '' -s base '(objectclass=*)' namingContexts# extended LDIF## LDAPv3# base &lt;&gt; with scope baseObject# filter: (objectclass=*)# requesting: namingContexts ##dn:namingContexts: dc=openldap,dc=flywzj,dc=com# search resultsearch: 2result: 0 Success# numResponses: 2# numEntries: 1 部署 phpldapadmin安装 phpldapadmin1# yum -y install httpd php-ldap phpldapadmin 配置访问控制123456# vim /etc/httpd/conf.d/phpldapadmin.conf &lt;IfModule mod_authz_core.c&gt; # Apache 2.4 Require local Require ip 192.168.100 &lt;/IfModule&gt; 注:如上为允许本地和192.168.100.0网段访问phpldapadmin，其他Apache Require访问控制指令请自行搜索。 更改登录方式1234# vim /etc/phpldapadmin/config.php// line 397$servers-&gt;setValue('login','attr','dn');// $servers-&gt;setValue('login','attr','uid'); 导入默认 schema 模块12345678910111213141516171819202122232425262728# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=cosine,cn=schema,cn=config"# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=nis,cn=schema,cn=config"# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=inetorgperson,cn=schema,cn=config"# ldapsearch -LLLQY EXTERNAL -H ldapi:/// -b cn=schema,cn=config "(objectClass=olcSchemaConfig)" dndn: cn=schema,cn=configdn: cn=&#123;0&#125;core,cn=schema,cn=configdn: cn=&#123;1&#125;cosine,cn=schema,cn=configdn: cn=&#123;2&#125;nis,cn=schema,cn=configdn: cn=&#123;3&#125;inetorgperson,cn=schema,cn=config 注1:这里的模版对应在 phpldapadmin 里 “创建一个子条目” 下的 “Select a template for the creation process” 模版。 注2: 如出现 “Automatically removed objectClass from templateCourier Mail: Account: courierMailAccount removed from template as it is not defined in the schema” 问题，那既是没有导入相关 objectClass 模版的原因。我曾试着将余下的 schema 也导入进去，但是没有成功，余下的几个模版依然是disable的。 配置 DN12345678910# vim /etc/openldap/base.ldif dn: dc=openldap,dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: flywzj # ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/base.ldif Enter LDAP Password: adding new entry "dc=openldap,dc=flywzj,dc=com" 注:如未配置默认DC即登录 phpldapadmin 会提示 “This base cannot be created with PLA.”问题。 启用 phpldapadmin1# systemctl start httpd 浏览器使用 http://yourip/ldapadmin 进入 phpldapadmin,点击登录，在“登录DN”中输入”cn=admin,dc=openldap,dc=flywzj,dc=com”并输入相应密码即可登录创建条目。 使用命令创建 OU1234567891011121314151617181920212223242526272829303132# vim /etc/openldap/group.ldif dn: ou=Groups,dc=openldap,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: topdn: ou=People,dc=openldap,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: topdn: cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topdn: cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: top# ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/group.ldif Enter LDAP Password: adding new entry "ou=Groups,dc=openldap,dc=flywzj,dc=com"adding new entry "ou=People,dc=openldap,dc=flywzj,dc=com"adding new entry "cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=com"adding new entry "cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=com" 注:可使用 phpldapadmin 来创建。 创建用户12345678910111213141516171819202122232425262728293031323334353637383940# vim /etc/openldap/user.ldif dn: uid=wang,ou=People,dc=openldap,dc=flywzj,dc=comcn: wanggidnumber: 500givenname: wanghomedirectory: /home/users/wangloginshell: /bin/shmail: wangzhijiansd@qq.comobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: wanguid: wanguidnumber: 1000userpassword: wangzhijiandn: uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=comcn: zhigidnumber: 501givenname: zhihomedirectory: /home/users/zhiloginshell: /bin/shmail: wangzhijiansd@qq.comobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: zhiuid: zhiuidnumber: 1001userpassword: zhijian# ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/user.ldif Enter LDAP Password: adding new entry "uid=wang,ou=People,dc=openldap,dc=flywzj,dc=com"adding new entry "uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=com" 查看具体用户123456789101112131415# ldapsearch -x -W -D "uid=wang,ou=People,dc=openldap,dc=flywzj,dc=com" -b "uid=wang,dc=openldap,ou=People,dc=flywzj,dc=com"Enter LDAP Password: # extended LDIF## LDAPv3# base &lt;uid=wang,dc=openldap,ou=People,dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## search resultsearch: 2result: 32 No such object# numResponses: 1 查看该CN下的所有条目1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# ldapsearch -x -H ldap:/// -b dc=openldap,dc=flywzj,dc=com -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -WEnter LDAP Password: # extended LDIF## LDAPv3# base &lt;dc=openldap,dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## openldap.flywzj.comdn: dc=openldap,dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: flywzjdc: openldap# Groups, openldap.flywzj.comdn: ou=Groups,dc=openldap,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, openldap.flywzj.comdn: ou=People,dc=openldap,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, openldap.flywzj.comdn: cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: top# test, Groups, openldap.flywzj.comdn: cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: top# wang, People, openldap.flywzj.comdn: uid=wang,ou=People,dc=openldap,dc=flywzj,dc=comcn: wanggidNumber: 500givenName: wanghomeDirectory: /home/users/wangloginShell: /bin/shmail: wangzhijiansd@qq.comobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: wanguid: wanguidNumber: 1000userPassword:: d2FuZ3poaWppYW4=# zhi, People, openldap.flywzj.comdn: uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=comcn: zhigidNumber: 501givenName: zhihomeDirectory: /home/users/zhiloginShell: /bin/shmail: wangzhijiansd@qq.comobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: zhiuid: zhiuidNumber: 1001userPassword:: emhpamlhbg==# search resultsearch: 2result: 0 Success# numResponses: 8# numEntries: 7 注: 亦可使用 phpldapadmin 创建。 配置 zabbix 使用 LDAP 这里以配置 zabbix 使用 LDAP 来进行测试。 配置 zabbix 的 “管理” ===&gt; “认证” 下的 “LDAP settings”项: Enable LDAP authentication: √ LDAP主机: yourip 端口: 389(默认端口,如使用使用 ldaps 协议则为636) 基于 DN: dc=openldap,dc=flywzj,dc=com 搜索属性: uid 绑定 DN: cn=admin,dc=openldap,dc=flywzj,dc=com 绑定密码: 输入该DN的密码 测试认证[必需为一个正确的LDAP用户]-登录: 如如上配置的wang 测试认证[必需为一个正确的LDAP用户]-用户密码:如如上配置的wangzhijian 点击 “测试”，成功则会提示“LDAP登录成功”。 注: 如要使用 LDAP 登录 zabbix，还需在 zabbix 的 “用户群组” 下创建同名的用户并赋予其权限，之后配置”更新”默认认证方式为“LDAP”即可使用LDAP下的该用户进行登录。 使用 kubernetes 部署 openLDAP 详询: docker-openldap，在部署时请查看对比构建方的环境变量。如需进行定制构建，构建方也给了构建方法。 配置 PVC123456789101112131415161718192021222324252627282930313233# vim openldap-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-data namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi"---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-config namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "1Gi"# kubectl create -f openldap-pvc.yaml persistentvolumeclaim/ldap-data createdpersistentvolumeclaim/ldap-config created 配置openLDAP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# vim openldap.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ldap labels: app: ldapspec: replicas: 1 template: metadata: labels: app: ldap spec: containers: - name: ldap image: 192.168.100.100/library/openldap:1.2.2 volumeMounts: - name: ldap-data mountPath: /var/lib/ldap - name: ldap-config mountPath: /etc/ldap/slapd.d - name: ldap-certs mountPath: /container/service/slapd/assets/certs ports: - containerPort: 389 name: openldap env: - name: LDAP_LOG_LEVEL value: "256" - name: LDAP_ORGANISATION value: "zhi" - name: LDAP_DOMAIN value: "flywzj.com" - name: LDAP_ADMIN_PASSWORD value: "admin" - name: LDAP_CONFIG_PASSWORD value: "config" - name: LDAP_READONLY_USER value: "false" - name: LDAP_READONLY_USER_USERNAME value: "readonly" - name: LDAP_READONLY_USER_PASSWORD value: "readonly" - name: LDAP_RFC2307BIS_SCHEMA value: "false" - name: LDAP_BACKEND value: "mdb" - name: LDAP_TLS value: "true" - name: LDAP_TLS_CRT_FILENAME value: "ldap.crt" - name: LDAP_TLS_KEY_FILENAME value: "ldap.key" - name: LDAP_TLS_CA_CRT_FILENAME value: "ca.crt" - name: LDAP_TLS_ENFORCE value: "false" - name: LDAP_TLS_CIPHER_SUITE value: "SECURE256:+SECURE128:-VERS-TLS-ALL:+VERS-TLS1.2:-RSA:-DHE-DSS:-CAMELLIA-128-CBC:-CAMELLIA-256-CBC" - name: LDAP_TLS_VERIFY_CLIENT value: "demand" - name: LDAP_REPLICATION value: "false" - name: LDAP_REPLICATION_CONFIG_SYNCPROV value: "binddn=\"cn=admin,cn=config\" bindmethod=simple credentials=$LDAP_CONFIG_PASSWORD searchbase=\"cn=config\" type=refreshAndPersist retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_DB_SYNCPROV value: "binddn=\"cn=admin,$LDAP_BASE_DN\" bindmethod=simple credentials=$LDAP_ADMIN_PASSWORD searchbase=\"$LDAP_BASE_DN\" type=refreshAndPersist interval=00:00:00:10 retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_HOSTS value: "#PYTHON2BASH:['ldap://ldap-one-service', 'ldap://ldap-two-service']" - name: KEEP_EXISTING_CONFIG value: "false" - name: LDAP_REMOVE_CONFIG_AFTER_SETUP value: "true" - name: LDAP_SSL_HELPER_PREFIX value: "ldap" volumes: - name: ldap-data persistentVolumeClaim: claimName: ldap-data #hostPath: # path: "/data/ldap/db" - name: ldap-config persistentVolumeClaim: claimName: ldap-config #hostPath: # path: "/data/ldap/config" - name: ldap-certs hostPath: path: "/data/ldap/certs"---apiVersion: v1kind: Servicemetadata: labels: app: ldap name: ldap-servicespec: type: NodePort ports: - port: 389 nodePort: 38989 selector: app: ldap# kubectl create -f openldap.yaml deployment.extensions/ldap createdservice/ldap-service created# kubectl get pods -l app=ldapNAME READY STATUS RESTARTS AGEldap-65f5786ff8-b9dnx 1/1 Running 1 11d 使用 kubernetes 部署 openldapadmin 详询: docker-phpLDAPadmin，部署时请详细查看对比构建方给定的环境变量。如需进行定制构建，构建方也给出了构建方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# vim phpldapadmin.yaml apiVersion: v1kind: ReplicationControllermetadata: name: phpldapadmin-controller labels: app: phpldapadminspec: replicas: 1 selector: app: phpldapadmin template: metadata: labels: app: phpldapadmin spec: containers: - name: phpldapadmin image: 192.168.100.100/library/phpldapadmin:0.7.2 volumeMounts: - name: phpldapadmin-certs mountPath: /container/service/phpldapadmin/assets/apache2/certs - name: ldap-client-certs mountPath: /container/service/ldap-client/assets/certs ports: - containerPort: 443 env: - name: PHPLDAPADMIN_LDAP_HOSTS #value: "#PYTHON2BASH:[&#123;'ldap-service': [&#123;'server': [&#123;'tls': 'true'&#125;]&#125;]&#125;]" value: "ldap-service" - name: PHPLDAPADMIN_SERVER_ADMIN value: "wangzhijiansd@qq.com" - name: PHPLDAPADMIN_SERVER_PATH value: "/phpldapadmin" - name: PHPLDAPADMIN_HTTPS value: "true" - name: PHPLDAPADMIN_HTTPS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_HTTPS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_HTTPS_CA_CRT_FILENAME value: "ca.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS value: "true" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_REQCERT value: "demand" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CA_CRT_FILENAME value: "ca.crt" volumes: - name: phpldapadmin-certs hostPath: path: "/data/phpldapadmin/ssl/" - name: ldap-client-certs hostPath: path: "/data/phpldapadmin/ldap-client-certs/"---apiVersion: v1kind: Servicemetadata: labels: app: phpldapadmin name: phpldapadmin-servicespec: type: NodePort ports: - port: 443 nodePort: 32001 selector: app: phpldapadmin# kubectl create -f phpldapadmin.yaml replicationcontroller/phpldapadmin-controller createdservice/phpldapadmin-service created# kubectl get pods -l app=phpldapadminNAME READY STATUS RESTARTS AGEphpldapadmin-controller-mprjw 1/1 Running 2 14d 输入 https://nodeip:nodeport 点击”login”进行登陆: Login DN: cn=admin,dc=flywzj,dc=com Password: admin 点击 Authenticate 登陆 不在演示如何在 kubernetes 的 openLDAP 上创建用户，既然安装了 phpldapadmin ,用web来创建比较简单(phpldapadmin界面粗糙)。]]></content>
      <categories>
        <category>openldap</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[切换 zabbix 认证模式]]></title>
    <url>%2Fblog%2Fb5448413%2F</url>
    <content type="text"><![CDATA[以 zabbix 配置了 openLDAP 认证，现在想切换为默认的认证方式为例。 登陆数据库查看当前认证方式1234567891011121314151617# mysql -u root -pMariaDB [(none)]&gt; use zabbix;Database changedMariaDB [zabbix]&gt; show tables like 'config';+---------------------------+| Tables_in_zabbix (config) |+---------------------------+| config |+---------------------------+1 row in set (0.01 sec)MariaDB [zabbix]&gt; select authentication_type from config ;+---------------------+| authentication_type |+---------------------+| 1 |+---------------------+1 row in set (0.00 sec) 0 代表Internal,1 代表LDAP，2 代表HTTP。 更改认证方式为默认认证方式123456MariaDB [zabbix]&gt; update config set authentication_type=0;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0MariaDB [zabbix]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 更新 Admin 密码(如需要)12345678910# 查询Admin用户的IDMariaDB [zabbix]&gt; select * from users;# 更新Admin密码MariaDB [zabbix]&gt; update users set passwd=md5("zabbix") where userid='1';Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0MariaDB [zabbix]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)]]></content>
      <categories>
        <category>openldap</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署基于 Web 的 Kubernetes 多集群管理平台 -- 360 Wayne]]></title>
    <url>%2Fblog%2F360wayne%2F</url>
    <content type="text"><![CDATA[Wayne 是一个通用的、基于 Web 的 Kubernetes 多集群管理平台。通过可视化 Kubernetes 对象模板编辑的方式，降低业务接入成本， 拥有完整的权限管理系统，适应多租户场景，是一款适合企业级集群使用的发布平台。 特性 基于 RBAC（Role based access control）的权限管理：用户通过角色与部门和项目关联，拥有部门角色允许操作部门资源，拥有项目角色允许操作项目资源，更加适合多租户场景。 简化 Kubernetes 对象创建：提供基础 Kubernetes 对象配置文件添加方式，同时支持高级模式直接编辑 Json/Yaml文件创建 Kubernetes 对象。 LDAP/OAuth 2.0/DB 多种登录模式支持：集成企业级 LDAP 登录及 DB 登录模式，同时还可以实现 OAuth2 登录。 支持多集群、多租户：可以同时管理多个 Kubernetes 集群，并针对性添加特定配置，更方便的多集群、多租户管理。 提供完整审计模块：每次操作都会有完整的审计功能，追踪用于操作历史，同时支持用户自定义 webhook。 提供基于 APIKey 的开放接口调用：用户可自主申请相关 APIKey 并管理自己的部门和项目，运维人员也可以申请全局 APIKey 进行特定资源的全局管理。 保留完整的发布历史：用户可以便捷的找到任何一次历史发布，并可轻松进行回滚，以及基于特定历史版本更新 Kubernetes 资源。 具备完善的资源报表：用户可以轻松获取各项目的资源使用占比和历史上线频次（天级）以及其他基础数据的报表和图表。 提供基于严密权限校验的 Web shell：用户可以通过 Web shell 的形式进入发布的 Pod 进行操作，自带完整的权限校验。 提供站内通知系统：方便管理员推送集群、业务通知和故障处理报告等。 架构 部署 Wayne下载相关文件并部署 Wayne 依赖 MySQL 和 RabbitMQ，其中 MySQL 是必须的服务，用户存储系统的各种数据，RabbitMQ 是可选的，主要用户扩展审计功能使用。 这里使用了 ceph 进行数据持久化 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# git clone https://github.com/Qihoo360/wayne.git# cd wayne/hack/kubernetes/# vim dependency/mysql-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-wayne-pvc namespace: default labels: app: mysql-waynespec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "10Gi"# vim dependency/rabbitmq-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: rabbitmq-wayne-pvc namespace: default labels: app: rabbitmq-waynespec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi" # kubectl apply -f dependency/persistentvolumeclaim/mysql-wayne-pvc createddeployment.extensions/mysql-wayne createdservice/mysql-wayne createdpersistentvolumeclaim/rabbitmq-wayne-pvc createddeployment.extensions/rabbitmq-wayne createdservice/rabbitmq-wayne created# kubectl get podNAME READY STATUS RESTARTS AGEmysql-wayne-75947575d-mc972 1/1 Running 0 107srabbitmq-wayne-7c6dd8f475-l4pqj 1/1 Running 0 106s# kubectl apply -f wayne/configmap/infra-wayne createddeployment.extensions/infra-wayne createddeployment.extensions/infra-wayne-woker createddeployment.extensions/infra-wayne-webhook createdservice/infra-wayne created# kubectl get podNAME READY STATUS RESTARTS AGEinfra-wayne-5d84cf49b4-lggzs 1/1 Running 0 7m44sinfra-wayne-webhook-85dcf87c48-w4tcj 1/1 Running 0 7m44sinfra-wayne-woker-84bff6f8c9-mt7h5 1/1 Running 0 7m44s 现在可以通过 http://yourip:NodePort 访问 Wayne 平台，默认管理员账号 密码admin:admin。 注: 项目启动后还需要配置集群和 Namespace 等信息才可正常使用。 配置集群和 Namespace配置集群 进入后台创建集群并将 .kube/config 复制并粘贴至该集群下 1# cat .kube/config 配置 namespace 在 wayne 后台创建命名空间(需在 kubernetes 集群中进行创建,然后与 wayne 进行绑定) 12# kubectl create namespace testnamespace/test created 查看资源状况 配置完成,在左侧边栏的 kubernetes 菜单栏可以查看当前集群的相关 node 信息、deployment 信息以及 PV 信息。 应用 Wayne创建项目 返回前台，切换至当前集群的选项卡”创建项目” 创建部署 进入该项目部署页“创建部署”进行部署，之后“创建部署模板”，之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 部署成功后，可以选择“重启”、“下线”: 点击上线机房，通过弹出的选择框可以“进入容器”、“查看日志”: 创建负载均衡 点击该项目下左侧边栏的“负载均衡”项，之后“创建负载均衡”，配置“名称”和“机房”并提交。之后“创建负载均衡模板”,之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 创建ingress 点击该项目下左侧边栏的“ingress”项，之后“创建ingress”，配置“名称”和“机房”并提交。之后“创建ingress模板”,之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 确认部署情况 注: 这里使用了 https 是因为我之前部署了 TLS 认证的 traefik。 更多详情请访问 Wayne 官方 wiki: wayne]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>wayne</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对etcd集群及kubernetes集群进行升级]]></title>
    <url>%2Fblog%2F819d304e%2F</url>
    <content type="text"><![CDATA[我的etcd集群和kubernetes集群都是二进制安装的，所以升级主要就是替换二进制文件。 这里我将原版本为3.3.8的etcd集群升级到3.3.10版本，将原版本为v1.11.1的kubernetes集群升级到v1.13.0版本，而我这里的kubernetes集群使用keepalived+haproxy做了双master的高可用、负载均衡，所以并无集群下线之忧。 升级 Etcd 集群升级检查查看集群健康状况12345678# ETCDCTL_API=3 # etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy 查看版本12# curl https://192.168.100.181:2379/version&#123;"etcdserver":"3.3.8","etcdcluster":"3.3.0"&#125; 使用快照备份 Etcd 集群 etcd leader拥有最新的应用程序数据，从leader获取快照etcd_server_is_leader 是1即为leader，否则为0。 123456789# curl -sL https://192.168.100.181:2379/metrics | grep etcd_server_is_leader# HELP etcd_server_is_leader Whether or not this member is a leader. 1 if is, 0 otherwise.# TYPE etcd_server_is_leader gaugeetcd_server_is_leader 0# curl -sL https://192.168.100.182:2379/metrics | grep etcd_server_is_leader# HELP etcd_server_is_leader Whether or not this member is a leader. 1 if is, 0 otherwise.# TYPE etcd_server_is_leader gaugeetcd_server_is_leader 1 当然，也可以使用该命令查看谁是leader 123456# etcdctl --endpoints=https://192.168.100.181:2379 member list3a406a85e3de7ef5: name=etcd-184 peerURLs=https://192.168.100.184:2380 clientURLs=https://192.168.100.184:2379 isLeader=false695714eeb38cebbe: name=etcd-181 peerURLs=https://192.168.100.181:2380 clientURLs=https://192.168.100.181:2379 isLeader=falseab8f0f710ce0bf85: name=etcd-183 peerURLs=https://192.168.100.183:2380 clientURLs=https://192.168.100.183:2379 isLeader=falsec5cb8024e23348b6: name=etcd-182 peerURLs=https://192.168.100.182:2380 clientURLs=https://192.168.100.182:2379 isLeader=trueceb2db537a9ec20d: name=etcd-185 peerURLs=https://192.168.100.185:2380 clientURLs=https://192.168.100.185:2379 isLeader=false 使用快照备份集群12345678# ETCDCTL_API=3 etcdctl --endpoints https://192.168.100.182:2379 snapshot save snapshotdbSnapshot saved at snapshotdb# ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb+----------+----------+------------+------------+| HASH | REVISION | TOTAL KEYS | TOTAL SIZE |+----------+----------+------------+------------+| c09e95e0 | 11794749 | 1226 | 19 MB |+----------+----------+------------+------------+ 下载并解压 Etcd1# tar -zxvf etcd-v3.3.10-linux-amd64.tar.gz 停止一个现有 Etcd 服务器1# systemctl stop etcd 替换 Etcd 二进制文件，使用相同配置重启 Etcd 服务器123456789101112# cp etcd-v3.3.10-linux-amd64/etcd /usr/bin/# cp etcd-v3.3.10-linux-amd64/etcdctl /usr/bin/# systemctl start etcd# systemctl status etcd# etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy 对其余成员重复如上步骤 在未升级的成员将记录以下警告，直到升级整个集群 123# systemctl status etcdthe local etcd version 3.3.8 is not up-to-datemember 695714eeb38cebbe has a higher version 3.3.10 查看集群成员健康状况和版本123456789101112131415161718# etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy# curl https://192.168.100.181:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.182:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.183:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.184:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.185:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125; 升级 Kubernetes 集群查看当前集群版本12345# kubectl get node NAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 131d v1.11.1node02 Ready &lt;none&gt; 131d v1.11.1node03 Ready &lt;none&gt; 131d v1.11.1 下载并解压文件12# tar -zxvf kubernetes-server-linux-amd64.tar.gz# cd kubernetes/server/bin 升级 Master 节点停止 Master 节点相关组件123# systemctl stop kube-apiserver# systemctl stop kube-controller-manager# systemctl stop kube-scheduler 替换 Master 节点二进制组件1# cp kube-apiserver kube-controller-manager kube-scheduler kubeadm /usr/bin/ 重新启用 Master 节点12345678# systemctl start kube-apiserver# systemctl status kube-apiserver# systemctl start kube-controller-manager# systemctl status kube-controller-manager# systemctl start kube-scheduler# systemctl status kube-scheduler 在其他 Master 节点重复如上步骤进行升级 升级 Node 节点标记节点为不可调度 设置为不可调度后，新的 pod 不会迁移或者部署在该节点 12345# kubectl cordon node01node/node01 cordoned# kubectl get node | grep node01node01 Ready,SchedulingDisabled &lt;none&gt; 131d v1.11.1 迁移该节点的 Pod 迁移时注意系统瓶颈，当其他节点的CPU、内存或者本地存储资源不足，kubernetes都不会调用pod，pod会处于pending状态，直到重新上线该节点(或者扩容节点资源)，pod才会重新上线。 1234567# kubectl drain --ignore-daemonsets --delete-local-data node01 kubectl drain node01 --ignore-daemonsets --delete-local-datanode/node01 already cordonedWARNING: Ignoring DaemonSet-managed pods: ......; Deleting pods with local storage: ......pod/my-nginx-7ff9b54467-vk572 evicted......node/node01 evicted 注:对于DaemonSet-managed pods需要使用参数–ignore-daemonsets;迁移使用本地存储的pods需要使用参数–delete-local-data(移动到其他节点将清空数据)。 查看节点上是否还存在 Pods(DaemonSet pods忽略)1# kubectl get pod -o wide --all-namespaces | grep node01 查看 Pods 是否已移动到其他节点1# kubectl get pod -o wide --all-namespaces 停用该节点 Kubelet 和 Kube-proxy12# systemctl stop kubelet# systemctl stop kube-proxy 复制并替换相应二进制文件12# scp root@master1:/root/kubernetes/server/bin/kubelet /usr/bin/# scp root@master1:/root/kubernetes/server/bin/kube-proxy /usr/bin/ 启用该 Node 节点1234# systemctl start kubelet# systemctl status kubelet# systemctl start kube-proxy# systemctl status kube-proxy 在 Master 节点上解锁(重新上线)该 Node 节点12345# kubectl uncordon node01node/node01 uncordoned# kubectl get node | grep node01node01 Ready &lt;none&gt; 131d v1.13.0 在其他 Node 节点重复如上步骤以升级 Node 节点 查看系统是否升级成功12345# kubectl get node NAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 131d v1.13.0node02 Ready &lt;none&gt; 131d v1.13.0node03 Ready &lt;none&gt; 131d v1.13.0]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>etcd</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Harbor从1.5.1升级和迁移到1.6.2]]></title>
    <url>%2Fblog%2Fharbor%2F</url>
    <content type="text"><![CDATA[这次升级还算比较顺利，以前我从1.2版本升级到1.5版本没有升级成功，镜像全洗白了，所以这次升级我及其谨慎，官方文档看了又看(主要是文档排版太糟糕了)，生怕又给洗白了，当然结果是好的，成功升级。 官方改了三次数据库，从最早使用的MySQL迁移到MariaDB，从1.6.0开始又迁移到了Postgresql 在1.5.1版中我并没有安装运行Notary和Clair这两个组件 升级到1.6.2版后我新部署了Notary，Clair和Helm Chart这3个组件 备份Harbor停止Harbor12# cd harbor# docker-compose down 备份Harbor的当前文件,以便在必要时回滚到当前版本12# cd ..# mv harbor harbor-backup 下载迁移工具12# docker pull goharbor/harbor-migrator:v1.6.0goharbor/harbor-migrator v1.6.0 22775c4e4066 2 months ago 803MB 备份数据1234567# mkdir backup# docker run -it --rm -e DB_USR=root -e DB_PWD=root123 -v /data/database:/var/lib/mysql -v /root/harbor-backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg -v /root/backup:/harbor-migration/backup goharbor/harbor-migrator:v1.6.0 backup......Backup performed.Success to backup harbor.cfg.# ls backupharbor.cfg registry.sql 命令参考: docker run -it –rm -e DB_USR=root -e DB_PWD={db_pwd} -v ${harbor_db_path}:/var/lib/mysql -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg -v ${backup_path}:/harbor-migration/backup goharbor/harbor-migrator:[tag] backup 升级数据库架构、harbor.cfg并迁移数据 注意：您必须在启动Harbor之前运行Notary和Clair的DB的迁移。注意：在v1.6.0中，您需要执行三个连续步骤才能完全迁移Harbor，Notary和Clair的DB。 1234567891011# docker run -it --rm -e DB_USR=root -e DB_PWD=root123 -v /data/database:/var/lib/mysql -v /root/backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:v1.6.0 upPlease backup before upgrade, Enter y to continue updating or n to abort: yTrying to start mysql server...Waiting for MySQL start.........server stoppedThe path of the migrated harbor.cfg is not set, the input file will be overwritten.input version: 1.5.0, migrator chain: ['1.6.0']migrating to version 1.6.0Written new values to /harbor-migration/harbor-cfg/harbor.cfg 命令参考: docker run -it –rm -e DB_USR=root -e DB_PWD={db_pwd} -v ${harbor_db_path}:/var/lib/mysql -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg -v ${backup_path}:/harbor-migration/backup goharbor/harbor-migrator:[tag] backup 将harbor.cfg迁移至新版本的安装目录123456# docker run -it --rm -v /root/backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:v1.6.0 --cfg up# grep ^[a-z] backup/harbor.cfg# tar -zxvf harbor-offline-installer-v1.6.2.tgz # cd harbor# mv harbor.cfg harbor.cfg.bak# cp /root/backup/harbor.cfg /root/harbor 命令参考: docker run -it –rm -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:[tag] –cfg up 安装Harbor载入镜像12# docker load -i harbor.v1.6.2.tar.gz# docker images|grep 1.6.2 安装Notary，Clair和Helm Chart服务1234567# ./install.sh --with-notary --with-clair --with-chartmuseum......✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at https://192.168.100.100. For more details, please visit https://github.com/goharbor/harbor . 在安装升级过程中我又重新使用docker-compose命令安装了一次，供参考123456789101112131415161718192021# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml down -v# vim harbor.cfg# ./prepare --with-notary --with-clair --with-chartmuseum......The configuration files are ready, please use docker-compose to start the service.# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml up -d# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml psName Command State Ports -----------------------------------------------------------------------------chartmuseum /docker-entrypoint.sh Up (healthy) 9999/tcp clair /docker-entrypoint.sh Up (healthy) 6060/tcp, 6061/tcp harbor-adminserver /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-ui /harbor/start.sh Up (healthy) nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpnotary-server /bin/server-start.sh Up notary-signer /bin/signer-start.sh Up redis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp 如果要同时安装Notary，Clair和Helm Chart服务，则应在docker-compose和prepare命令中包含所有组件. 如上，harbor已经完成升级，可使用浏览器登陆harbor查看是否成功升级. Notary 使用 如果要启用内容信任以确保图像已签名，请在推送或拉取任何图像之前在命令行中设置两个环境变量： 12# export DOCKER_CONTENT_TRUST=1# export DOCKER_CONTENT_TRUST_SERVER=https://192.168.100.100:4443 这里以上传kubernetes-dashboard为例子说明notary的使用. 1234567891011121314151617181920212223# docker push 192.168.100.100/google_containers/kubernetes-dashboard-amd64:v1.10.0The push refers to repository [192.168.100.100/google_containers/kubernetes-dashboard-amd64]5f222ffea122: Pushed v1.10.0: digest: sha256:1d2e1229a918f4bc38b5a3f9f5f11302b3e71f8397b492afac7f273a0008776a size: 529Signing and pushing trust metadataYou are about to create a new root signing key passphrase. This passphrasewill be used to protect the most sensitive key in your signing system. Pleasechoose a long, complex passphrase and be careful to keep the password and thekey file itself secure and backed up. It is highly recommended that you use apassword manager to generate the passphrase and keep it safe. There will be noway to recover this key. You can find the key in your config directory.## 第一次push镜像，系统将要求您输入根密钥密码Enter passphrase for new root key with ID 7ffe68f: Repeat passphrase for new root key with ID 7ffe68f: ## 密码设置弱系统会进行提示Enter passphrase for new repository key with ID e8c208d: Passphrase is too short. Please use a password manager to generate and store a good random passphrase.Enter passphrase for new repository key with ID e8c208d: Repeat passphrase for new repository key with ID e8c208d: Finished initializing "192.168.100.100/google_containers/kubernetes-dashboard-amd64"Successfully signed 192.168.100.100/google_containers/kubernetes-dashboard-amd64:v1.10.0 注1: 根密钥生成于: /root/.docker/trust/private/镜像密码生成于: /root/.docker/trust/tuf/[registry name]/[imagepath]注2: 要使用notary，必须在Harbor中启用HTTPS.注3: 当镜像被签名时，它在UI中显示勾号; 否则，显示交叉符号（X）。注4:如果您省略标签，则跳过内容信任。提示”No tag specified, skipping trust metadata push”，所以即便是 latest 也需要提供镜像 tag 值。 通过Clair进行漏洞扫描Clair依靠漏洞元数据来完成分析过程。第一次初始安装后，Clair将自动开始从不同的漏洞存储库更新元数据数据库。更新过程可能需要一段时间，具体取决于数据大小和网络连接。如果数据库尚未完全填充，则存储库数据网格视图的页脚会显示警告消息。 数据库准备就绪后，整个数据库更新的时间戳将显示在“管理”下“ 配置”部分的“漏洞”选项卡中。这时候就可以进行漏洞扫描了。 注意：只有具有“项目管理员”角色的用户才有权启动分析过程。 分析过程可能显示如下状态： 未扫描：标签从未被扫描过。 排队：扫描任务已安排但尚未执行。 扫描：扫描过程正在进行中。 错误：扫描过程未能完成。 完成：扫描过程已成功完成。 关于漏洞的严重级别: 红色： 高安全漏洞的级别 橙色： 中等级别的漏洞 黄色： 漏洞程度低 灰色： 未知级别的漏洞 绿色： 没有漏洞 由于Harbor是由VMware中国的团队研发并开源的，对中文支持友好，对于使用问题无需过多担心。 附:有关Notary和Docker Content Trust的更多信息，请参阅Docker的文档：https://docs.docker.com/engine/security/trust/content_trust/关于Clair:https://github.com/coreos/clairHarbor用户指南: https://github.com/goharbor/harbor/blob/master/docs/user_guide.md]]></content>
      <categories>
        <category>镜像仓库</category>
      </categories>
      <tags>
        <tag>harbor</tag>
        <tag>docker</tag>
        <tag>registry</tag>
        <tag>notary</tag>
        <tag>clair</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github构建博客]]></title>
    <url>%2Fblog%2Fhexo%2F</url>
    <content type="text"><![CDATA[在开始构建博客前，你需要在Github拥有一个账号,之后新建一个存储库(比如:zhijiansd.github.io),这里我就不再赘述了.接下来,我们需要为GitHub添加SSH key. 配置SSH key在本地创建秘钥,并将该秘钥复制下来12# ssh-keygen -t rsa -C "wangzhijiansd@qq.com"# cat /root/.ssh/id_rsa.pub 登录GitHub进行配置打开Github主页，依次点击Settings -&gt; SSH and GPG keys -&gt; New SSH key设置,自定义好Title,然后将上面复制的秘钥粘贴在Key下.进行测试1# ssh -T git@github.com 如果提示Are you sure you want to continue connecting (yes/no)?,输入yes.看到如下信息说明SSH已配置成功: Hi zhijiansd! You’’ve successfully authenticated, but GitHub does not provide shell access. 配置用户信息12# git config --global user.name "zhijiansd"# git config --global user.email "wangzhijiansd@qq.com" 安装Hexo并下载Next主题123456# yum -y install git nodejs # npm install hexo-cli -g# hexo init blog# cd blog# npm install# git clone https://github.com/theme-next/hexo-theme-next themes/next 更多主题详见Hexo. 配置Hexo更改默认主题为Next1# sed -i "s/landscape/next/g" _config.yml ###更改默认语言为汉语 12# grep language _config.yml language: zh-CN 配置Next更改Next主题外观123# grep scheme themes/next/_config.yml|grep Pisces scheme: Pisces# Only fit scheme Pisces 设置菜单1234567# vim themes/next/_config.ymlmenu: home: / || home //首页 #about: /about/ || user //关于 tags: /tags/ || tags //标签 categories: /categories/ || th //分类 archives: /archives/ || archive //归档 创建标签文件夹并添加type1234# hexo new page "tags"# vim source/tags/index.mdtype: "tags"comments: false 创建分类文件夹并添加type1234# hexo new page "categories"# vim source/categories/index.mdtype: "categories"comments: false 创建归档文件夹并添加type1234# hexo new page "archives"# vim source/archives/index.mdtype: archivescomments: false 设置头像123456# mkdir source/images# ls source/images/avatar.jpg# vim themes/next/_config.ymlavatar: url: /images/avatar.jpg 请自行将头像图片上传至source/images/文件夹下 修改文章内链接文本样式1234567891011# vim themes/next/source/css/_common/components/post/post.styl.post-body p a&#123; color: #0593d3; //原始链接颜色 border-bottom: none; border-bottom: 1px solid #0593d3; //底部分割线颜色 &amp;:hover &#123; color: #fc6423; //鼠标经过颜色 border-bottom: none; border-bottom: 1px solid #fc6423; //底部分割线颜色 &#125;&#125; 在文章末尾添加结束语 新建并配置passage-end-tag.swig文件 123456# vim themes/next/layout/_macro/passage-end-tag.swig&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt;-------------本文结束,感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 在post.swig文件的post-body之后，post-footer之前添加以下代码 123456# vim themes/next/layout/_macro/post.swig &lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125; &lt;/div&gt; 修改主题配置文件_config.yml，在末尾添加:如下: 123# vim themes/next/_config.ymlpassage_end_tag: enabled: true 实现文章统计功能 安装插件 1# npm install hexo-symbols-count-time --save 配置启用hexo配置文件的symbols项 1234567# vim _config.yml# Writingsymbols_count_time: symbols: true time: true total_symbols: true total_time: true 配置启用next主题配置文件的symbols项 1234567# vim themes/next/_config.ymlsymbols_count_time: separated_meta: true item_text_post: true item_text_total: true awl: 2 ##平均单词长度（单词的计数）。默认值:4。CN≈2 EN≈5 wpm: 300 ##每分钟的单词。默认值:275。缓慢≈200 正常≈275 快≈350 设置页面文章的篇数1234567891011# vim themes/next/_config.ymlindex_generator: per_page: 5archive_generator: per_page: 20 yearly: true monthly: truetag_generator: per_page: 10 启用访客量以及文章阅读量统计123456789101112131415161718192021# vim themes/next/_config.yml busuanzi_count: enable: true...... site_uv: true site_uv_header: 本站访客数 site_uv_footer: 人次 site_pv: true site_pv_header: 本站总访问量 site_pv_footer: 次 page_pv: true page_pv_header: 本文总阅读量 page_pv_footer: 次# vim themes/next/layout/_third-party/analytics/busuanzi-counter.swig ...... 本站总访问量&lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次...... 本站总访问量&lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次 给文章增加阴影效果12345678# vim themes/next/source/css/_custom/custom.styl.post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);&#125; 添加站内搜索12345# npm install hexo-generator-search --save# npm install hexo-generator-searchdb --save# vim themes/next/_config.yml local_search: enable: true 设置动态背景 配置启用next主题配置文件的canvas_nest项 123# vim themes/next/_config.yml canvas_nest: enable: true 在如下文件的行尾之前添加代码 1234# vim themes/next/layout/_layout.swig &#123;% if theme.canvas_nest %&#125; &lt;script type="text/javascript" color="255,0,255" opacity='0.7' zIndex="-2" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"&gt;&lt;/script&gt; &#123;% endif %&#125; 修改标签样式12# vim themes/next/layout/_macro/post.swig&lt;a href="&#123;&#123; url_for(tag.path) &#125;&#125;" rel="tag"&gt;&lt;i class="fa fa-tag"&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt; 在文件中搜索 rel=”tag”&gt;#,将 # 换成 ,不过这里的注释会直接显示改后的样式，上面就是更改后的样式，请参考. 在文章中插入图片123# npm install hexo-asset-image --save# vim _config.ymlpost_asset_folder: true 新建文章,/source/_posts文件夹内除了abcd.md文件还有一个同名的文件夹，在文章中按照默认格式即可在文章中插入图片(图片地址使用相对地址即可) 绘制流程图1# npm install --save hexo-filter-mermaid-diagrams 实验很多遍都没绘制出来,在这里只是想告诉大家 hexo 可以绘制流程图.详询:mermaid 修改永久链接的默认格式12345678# npm install hexo-abbrlink --save# vim _config.yml#permalink: :year/:month/:day/:title/#permalink_defaults:permalink: blog/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex 使用了该插件的话，同时使用了本地图片插件，注意图片路径的变化. 生成Hexo1# hexo g 开启预览访问端口123# hexo server -i 192.168.100.122INFO Start processingINFO Hexo is running at http://192.168.100.122:4000 . Press Ctrl+C to stop. 浏览器输入如上IP和端口即可在本地访问该博客 配置部署Hexo博客到GitHub 配置Hexo配置文件 12345# vim _config.ymldeploy: type: git repository: https://github.com/zhijiansd/zhijiansd.github.io branch: master 安装插件 1# npm install hexo-deployer-git --save 将Hexo部署到GitHub，之后浏览器输入zhijiansd.github.io查看 1# hexo d hexo的常用命令如下: 命令 解释 hexo init 初始化 hexo g 生成静态网页 hexo s 启动服务预览 hexo d 部署hexo hexo clean 清除缓存 hexo n 新建文章 hexo publish 草稿 注: NexT中文文档 Hexo中文文档]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
