<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Drone + Gogs 执行自动构建]]></title>
    <url>%2Fblog%2Fdrone%2F</url>
    <content type="text"><![CDATA[Drone 是一种基于容器技术的持续交付系统。Drone 使用YAML配置文件来定义和执行Docker容器中的Pipelines。 Gogs 是一个用Go编写的轻量级git服务器，它设计简单，易于设置和操作。Gogs提供存储库文件查看和编辑，项目问题跟踪以及项目文档的内置wiki。 Drone 作为一个CI工具，当然会出现与Jenkins的对比，从docker镜像来看，Jenkins的镜像达到了702M，而Drone的最新的docker镜像才62.1M。最重要的是drone使用的是yaml文件来进行构建，这对使用docker的人来说是相当友好的，而Jenkins的pipeline的编写对大多数人来说都是一种挑战。就像Jenkins官方所说的一样，“Jenkins是开源CI&amp;CD软件领导者， 提供超过1000个插件来支持构建、部署、自动化， 满足任何项目的需要。”，而drone所提供的插件相对来说很少，只有百十个的样子，所以这也是Jenkins的优势。 当然，对于Gitlab-runner，它也有同样的问题，gitlab-runner的v11.9.0的镜像达到了403M，而且Gitlab-runner是与Gitlab深度集成的，如果你使用了Gitlab，那这很好；而如果你使用的是GitHub这些源代码管理系统，那这就是问题了。 Drone适用于在Docker容器内运行的任何语言，数据库或服务。Drone使用容器将预先配置的步骤放入pipeline中。从现有插件或通过创建自己的插件来进行构建。当然，正如前面说的，drone使用yaml来定义pipeline，它的语法简洁明了，很容易上手。关于drone的更多介绍详见:https://drone.io/。 还是从外在的地方来对比，我使用的Gitlab-CE的11.7.5版本，docker镜像达到了1.59G，而最新版的gogs，它的镜像才99.1M，足够轻量级。Gogs在GitHub上的star也突破了三万，这也证明了gogs的受欢迎程度，同时gogs对中文支持很好。其他不再细讲，Gitlab官方也对Gogs做了对比，详见:https://about.gitlab.com/devops-tools/gogs-vs-gitlab.html。 这里对于Drone和Gogs的部署是使用docker-compose单机模式部署的。在使用docker-compose部署之前，我首先想的是将drone部署在kubernetes上，但是drone对此的支持不是很好，drone需要可路由的域名，无论是使用traefik部署的内部域名，还是service，Gitlab和Gogs都无法连接该域名或service，这就导致Gitlab和Gogs无法将代码推送到drone。drone的文档也还不够丰富，我也查阅了官方论坛，依然没能解决，所以很遗憾。 配置 docker-compose123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# vim docker-compose.yaml version: '3.5'services: drone-server: container_name: drone-server image: 192.168.100.100/drone/drone:1.1.0 restart: always labels: - "traefik.docker.network: traefik" - "traefik.frontend.rule=Host:drone.flywzj.com" networks: traefik: aliases: - drone.flywzj.com ports: - "18080:80" - "4433:443" volumes: - /data/drone:/data - /var/run/docker.sock:/var/run/docker.sock environment: - DRONE_GIT_ALWAYS_AUTH=false - DRONE_RUNNER_CAPACITY=2 - DRONE_RUNNER_NETWORKS=traefik - DRONE_SERVER_HOST=drone-server - DRONE_SERVER_PROTO=http - DRONE_LOGS_DEBUG=true - DRONE_TLS_AUTOCERT=false - DRONE_GOGS_SKIP_VERIFY=false - DRONE_GOGS_SERVER=http://gogs.flywzj.com:3000 - DRONE_PROVIDER=gogs - DRONE_DATABASE_DATASOURCE=/data/drone.sqlite - DRONE_DATABASE_DRIVER=sqlite3 gogs: container_name: gogs image: 192.168.100.100/gogs/gogs:latest restart: always labels: - "traefik.docker.network: traefik" - "traefik.frontend.rule=Host:gogs.flywzj.com" - "traefik.http.port=3000" - "traefik.ssh.port=22" networks: traefik: aliases: - gogs.flywzj.com ports: - "10022:22" - "3000:3000" volumes: - /data/gogs:/data depends_on: - mysql mysql: container_name: mysql image: 192.168.100.100/library/mysql:5.7 restart: always labels: - "traefik.docker.network: traefik" networks: - traefik volumes: - /data/mysql:/var/lib/mysql - /var/run/docker.sock:/var/run/docker.sock ports: - "3306:3306" command: --explicit_defaults_for_timestamp=true --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci environment: MYSQL_ROOT_PASSWORD: 'passw0rd' MYSQL_DATABASE: 'gogs' MYSQL_USER: 'gogs' MYSQL_PASSWORD: 'gogs' TZ: Asia/Shanghai traefik-proxy: container_name: traefik image: 192.168.100.100/library/traefik:1.7 restart: always labels: - "traefik.docker.network: traefik" networks: - traefik command: --api --docker ports: - "80:80" - "8080:8080" volumes: - /var/run/docker.sock:/var/run/docker.socknetworks: traefik: name: traefik external: false 部署 docker-compose12345678910111213# docker-compose up -dCreating mysql ... doneCreating gogs ... doneCreating drone-server ... Creating traefik ... Creating gogs ... # docker-compose ps Name Command State Ports -----------------------------------------------------------------------------------------------------drone-server /bin/drone-server Up 0.0.0.0:4433-&gt;443/tcp, 0.0.0.0:18080-&gt;80/tcp gogs /app/gogs/docker/start.sh ... Up 0.0.0.0:10022-&gt;22/tcp, 0.0.0.0:3000-&gt;3000/tcpmysql docker-entrypoint.sh --exp ... Up 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp traefik /traefik --api --docker Up 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:8080-&gt;8080/tcp 部署 Gogs 这里通过traefik配置的gogs.flywzj.com:3000即可登录配置界面，安装后创建一个新的仓库 注: 数据库主机 —-&gt; mysql:3306 (这里的mysql是container_name) 启用 Drone 这里通过traefik配置的drone.flywzj.com即可登录，之后输入在gogs上创建的账号和密码即可登录，登录后drone会自动进行同步 点击”ACTIVATE”激活该代码库 进入该代码库，点击”SETTINGS”之后，点击”ACTIVATE REPOSITORY”激活储存库 激活后配置项如下图所示 推送代码到 Gogs 注:新建的代码库是无法直接在gogs的web端进行编辑的，需要从远端推送一次后才能在gogs的web端进行编辑 123456789101112131415161718192021222324252627# git clone http://gogs.flywzj.com:3000/zhi/test.git# cd test/# vim DockerfileFROM alpineRUN echo hello-world# vim .drone.ymlkind: pipelinename: defaultsteps:- name: build image: plugins/docker settings: username: from_secret: docker_username password: from_secret: docker_password repo: wangzhijian/test tags: latest dry_run: true# git add --all .# git commit -m "update"[master（根提交） 7dbb251] update 2 files changed, 14 insertions(+) create mode 100644 .drone.yml create mode 100644 Dockerfile# git push -u origin master 注:”from_secret”里的密钥需在”SETTINGS” ===&gt; “Secrets” 下进行配置；使用 “dry_run: true” 将不推送到镜像仓库 查看 Drone 构建中的状态显示如下: 构建完成显示如下: 点击即可进入查看构建的详细信息: 附: docker-compose文档:https://docs.docker.com/compose/compose-file/ traefik文档页:https://docs.traefik.io/ gogs的GitHub:https://github.com/gogs/gogs drone的GitHub:https://github.com/drone/drone drone文档页:https://docs.drone.io/ drone的插件:http://plugins.drone.io/ drone官方构建示例代码:https://github.com/drone/hello-world/tree/test-docker-plugin drone官方构建示例:https://cloud.drone.io/drone/hello-world/7/1/2]]></content>
      <categories>
        <category>CI</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>gogs</tag>
        <tag>drone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在kubernetes上部署GitLab Runner]]></title>
    <url>%2Fblog%2Frunner%2F</url>
    <content type="text"><![CDATA[GitLab Runner是一个开源项目，用于运行您的jobs并将结果发送回GitLab。它与GitLab CI一起使用，GitLab CI是GitLab随附的开源持续集成服务，用于协调jobs。 注册Runner查看配置信息在Kubernetes上设置Gitlab Runner的第一步是获取身份验证令牌。此令牌非常重要，因为它会向Gitlab验证您的跑步者。要注册Runner，我们需要从Gitlab获取配置详细信息并完成Runner的注册过程。 这里我是直接使用root账户来注册的，查看位于Admin Area &gt; Runners 的“Set up a shared Runner manually”(手动设置共享Runner)，这里包含使用Gitlab注册新Runner所需的配置详细信息，记住URL和token。 注意: 使用其他普通账户可以注册Specific Runners(特定的runner)，配置信息位于相关项目的Settings &gt; CI/CD &gt; Runners 下。 注册Runner接下来，我们需要完成注册过程以连接新的Runner。注册Runner的最简单方法是使用Runner 在本地启动Docker容器来进行注册: 123456789101112131415161718192021# docker run --rm -it --entrypoint /bin/bash gitlab/gitlab-runner:latestroot@ceb637716b3c:/# gitlab-runner register ##注册runnerRuntime platform arch=amd64 os=linux pid=30 revision=692ae235 version=11.9.0Running in system-mode. Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/):http://192.168.100.128:32626/ ##配置gitlab-ci的URL，通常为域名Please enter the gitlab-ci token for this runner:5cy-91nwWtcqsuNXWZmF ##输入gitlab-ci tokenPlease enter the gitlab-ci description for this runner:[ceb637716b3c]: Kubernetes Gitlab-Runner ##输入gitlab-ci的描述Please enter the gitlab-ci tags for this runner (comma separated):kubernetes,gitlab-runner ##输入该runner的tagRegistering runner... succeeded runner=5cy-91nwPlease enter the executor: shell, virtualbox, docker+machine, docker-ssh+machine, kubernetes, parallels, docker-ssh, ssh, docker:kubernetes ##输入executorRunner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded! root@ceb637716b3c:/# grep token /etc/gitlab-runner/config.toml ##获取身份验证令牌 token = "-YGyg2YY_E4z9siNhpxj" bearer_token_overwrite_allowed = false 注意:复制此令牌并确保不会丢失它,这是验证Runner连接Gitlab的唯一身份令牌。保存令牌后，为安全计，请删除Docker实例。 在Kubernetes上部署Runner为Runner创建RBAC(角色访问控制)1234567891011121314151617181920212223242526# vim gitlab-runner-rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: gitlab-runner---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: gitlab-runnerrules: - apiGroups: [""] resources: ["*"] verbs: ["*"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: gitlab-runnersubjects: - kind: ServiceAccount name: gitlab-runnerroleRef: kind: Role name: gitlab-runner apiGroup: rbac.authorization.k8s.io 创建ConfigMap1234567891011121314151617181920# vim config.toml concurrent = 4[[runners]] name = "Kubernetes Gitlab-Runner" url = "http://192.168.100.128:32626" token = "-YGyg2YY_E4z9siNhpxj" executor = "kubernetes" [runners.kubernetes] namespace = "default" privileged = true poll_timeout = 600 cpu_request = "1" service_cpu_request = "200m" [[runners.kubernetes.volumes.host_path]] name = "docker" mount_path = "/var/run/docker.sock" host_path = "/var/run/docker.sock"# kubectl create configmap gitlab-runner-config --from-file=config.tomlconfigmap/gitlab-runner-config created 关于Kubernetes executor详询:https://docs.gitlab.com/runner/executors/kubernetes.html 部署Runner12345678910111213141516171819202122232425262728293031323334353637383940# vim gitlab-runner.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: gitlab-runnerspec: replicas: 1 selector: matchLabels: name: gitlab-runner template: metadata: labels: name: gitlab-runner spec: serviceAccountName: gitlab-runner containers: - name: gitlab-runner image: 192.168.100.100/gitlab/gitlab-runner:v11.9.0 imagePullPolicy: Always resources: requests: cpu: "100m" limits: cpu: "100m" volumeMounts: - name: config mountPath: /etc/gitlab-runner/config.toml readOnly: true subPath: config.toml volumes: - name: config configMap: name: gitlab-runner-config restartPolicy: Always# kubectl apply -f .serviceaccount/gitlab-runner createdrole.rbac.authorization.k8s.io/gitlab-runner createdrolebinding.rbac.authorization.k8s.io/gitlab-runner createddeployment.extensions/gitlab-runner created 查看连接情况 注意:再次说明，使用root账户创建的runner是共享的，其他项目、其他用户的项目都可以使用。而在某个用户下创建的项目为特定runner，只执行该项目jobs。 使用Runner新建一个项目配置环境变量 配置项位于该项目 Settings &gt; CI/CD &gt; Environment variables 下 新建一个简单的dockerfile文件123FROM 192.168.100.100/library/alpine:3.9WORKDIR /appRUN echo "hello" &gt; hh.txt 新建gitlab-ci.yml配置文件12345678910111213141516image: name: 192.168.100.100/library/alpine:3.9stages: - test test: stage: test image: 192.168.100.100/docker/docker:18 script: - docker info - docker login -u $harbor_username -p $harbor_password 192.168.100.100 - docker build -t 192.168.100.100/docker/test:latest . - docker push 192.168.100.100/docker/test:latest tags: - kubernetes 这里要注意的是，我截的图着重将clone的地址也截了下来，这里的gitlab为service。如果你是用的域名，而且域名可达，可忽略该问题。如果域名不可达，或者使用的是nodeport，又或者像我这样使用的是service，就需要更改gitlab的配置文件，不然gitlab-runner会出现无法Cloning repository的问题。这里我通过configmap挂载gitlab.rb配置文件到/etc/gitlab/目录来实现，gitlab.rb只需配置如下即可:external_url “http://gitlab&quot; 之后gitlab-runner会自动进行构建，构建成功会显示“passed”，失败则会显示“failed”。 点击“Stages”可查看相关构建阶段的详细信息 参照文章:https://adambcomer.com/blog/setup-gitlab-cicd-on-kubernetes.html]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>gitlab</tag>
        <tag>runnner</tag>
        <tag>CI</tag>
        <tag>CD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes+GitLab+Jenkins使用Pipeline构建镜像]]></title>
    <url>%2Fblog%2Fops%2F</url>
    <content type="text"><![CDATA[GitLab和Jenkins都是作为Pod部署在kubernetes上的，Jenkins通过创建的pipeline流水线任务从gitlab拉取构建文件以进行构建。 注:Pod都使用了持久化存储来部署,同时使用的是私有仓库镜像。 部署GitLab部署Redis12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# vim redis.yaml apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: redis namespace: defaultspec: serviceName: redis replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: redis spec: terminationGracePeriodSeconds: 30 containers: - name: redis image: 192.168.100.100/library/redis:4 ports: - containerPort: 6379 resources: requests: memory: "1Gi" cpu: "1000m" limits: memory: "2Gi" cpu: "2000m" livenessProbe: exec: command: - redis-cli - ping initialDelaySeconds: 30 timeoutSeconds: 5 readinessProbe: exec: command: - redis-cli - ping initialDelaySeconds: 5 timeoutSeconds: 1 volumeMounts: - name: redis mountPath: /var/lib/redis volumeClaimTemplates: - metadata: name: redis annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi---apiVersion: v1kind: Servicemetadata: name: redis namespace: defaultspec: ports: - name: redis port: 6379 targetPort: redis selector: name: redis# kubectl create -f redis.yaml statefulset.apps/redis createdservice/redis created 部署PostgreSQL1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# vim postgresql.yaml apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: postgresql namespace: defaultspec: serviceName: postgresql replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: postgresql spec: terminationGracePeriodSeconds: 30 containers: - name: postgresql image: 192.168.100.100/library/postgres:11 ports: - containerPort: 5432 resources: requests: memory: "1Gi" cpu: "1000m" limits: memory: "2Gi" cpu: "2000m" env: - name: DB_USER value: gitlab - name: DB_PASS value: passw0rd - name: DB_NAME value: gitlab_production - name: DB_EXTENSION value: pg_trgm livenessProbe: exec: command: - pg_isready - -h - localhost - -U - postgres initialDelaySeconds: 30 timeoutSeconds: 5 readinessProbe: exec: command: - pg_isready - -h - localhost - -U - postgres initialDelaySeconds: 5 timeoutSeconds: 1 volumeMounts: - name: postgresql mountPath: /var/lib/postgresql volumeClaimTemplates: - metadata: name: postgresql annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi---apiVersion: v1kind: Servicemetadata: name: postgresql namespace: defaultspec: ports: - name: postgresql port: 5432 targetPort: postgresql selector: name: postgresql# kubectl create -f postgresql.yaml statefulset.apps/postgresql createdservice/postgresql created 部署GitLab123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176# vim gitlab.yaml apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: gitlab namespace: defaultspec: serviceName: gitlab replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: gitlab labels: name: gitlab spec: terminationGracePeriodSeconds: 30 containers: - name: gitlab image: 192.168.100.100/gitlab/gitlab-ce:11.7.5-ce resources: requests: memory: "1Gi" cpu: "1000m" limits: memory: "4Gi" cpu: "2000m" env: - name: TZ value: Asia/BeiJing - name: GITLAB_TIMEZONE value: BeiJing - name: GITLAB_SECRETS_DB_KEY_BASE value: rVhTJmMMnn3RCwxXV7HCTNTkJXWLPjJhNHM3sWTgKwnqnNVTwNCVJ7zPLzKgVz7z - name: GITLAB_SECRETS_SECRET_KEY_BASE value: VkT7bxtXRsgvLpqRgbVnJKfjTMs7TsWrkXfmntPvbNWRLc4dxK9cWkLNpnNNvmsM - name: GITLAB_SECRETS_OTP_KEY_BASE value: Xgksv7WKTvWVrNm9vcvbdgMnVqXLPb3F9XvbFLXNLTTXtzcvJgp7nTrdbxN444Jt - name: GITLAB_ROOT_PASSWORD value: wangzhijian - name: GITLAB_ROOT_EMAIL value: wangzhijiansd@qq.com - name: GITLAB_HOST value: git.default - name: GITLAB_PORT value: "80" - name: GITLAB_SSH_PORT value: "22" - name: GITLAB_NOTIFY_ON_BROKEN_BUILDS value: "true" - name: GITLAB_NOTIFY_PUSHER value: "false" - name: GITLAB_BACKUP_SCHEDULE value: daily - name: GITLAB_BACKUP_TIME value: 01:00 - name: DB_TYPE value: postgresql - name: DB_HOST value: postgresql - name: DB_PORT value: "5432" - name: DB_USER value: gitlab - name: DB_PASS value: passw0rd - name: DB_NAME value: gitlab_production - name: REDIS_HOST value: redis - name: REDIS_PORT value: "6379" - name: SMTP_ENABLED value: "true" - name: SMTP_DOMAIN value: www.flywzj.com - name: SMTP_HOST value: smtp.qq.com - name: SMTP_PORT value: "465" - name: SMTP_USER value: wangzhijiansd@qq.com - name: SMTP_PASS value: "********" - name: SMTP_STARTTLS value: "true" - name: SMTP_AUTHENTICATION value: login - name: IMAP_ENABLED value: "false" - name: IMAP_HOST value: imap.gmail.com - name: IMAP_PORT value: "993" - name: IMAP_USER value: mailer@example.com - name: IMAP_PASS value: password - name: IMAP_SSL value: "true" - name: IMAP_STARTTLS value: "false" ports: - name: http containerPort: 80 - name: ssh containerPort: 22 volumeMounts: - name: gitlab-data mountPath: /var/opt/gitlab - name: gitlab-config mountPath: /etc/gitlab - name: gitlab-logs mountPath: /var/log/gitlab volumeClaimTemplates: - metadata: name: gitlab-data annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi - metadata: name: gitlab-config annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 100Mi - metadata: name: gitlab-logs annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 100Mi# vim gitlab-svc.yaml apiVersion: v1kind: Servicemetadata: name: gitlab namespace: defaultspec: type: LoadBalancer ports: - name: http port: 80 targetPort: http - name: ssh port: 22 targetPort: ssh selector: name: gitlab# kubectl create -f gitlab.yaml statefulset.apps/gitlab created# kubectl create -f gitlab-svc.yaml service/gitlab created 关于变量的赋予请参考:https://github.com/sameersbn/docker-gitlab 如需进行特殊配置，请参考gitlab相关文档将相应配置写入gitlab.rb并使用configmap将其挂载到/etc/gitlab/目录下 部署Jenkins1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# vim jenkins.yamlapiVersion: apps/v1beta1kind: StatefulSetmetadata: name: jenkinsspec: serviceName: jenkins replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: jenkins labels: name: jenkins spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins containers: - name: jenkins image: 192.168.100.100/jenkins/jenkins:lts imagePullPolicy: Always ports: - containerPort: 8080 - containerPort: 50000 resources: limits: cpu: 2 memory: 2Gi requests: cpu: 1 memory: 1Gi env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS value: -Duser.timezone=Asia/Shanghai -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 volumeMounts: - name: jenkins-home mountPath: /var/jenkins_home livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 # ~2 minutes readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 # ~2 minutes securityContext: fsGroup: 1000 volumeClaimTemplates: - metadata: name: jenkins-home annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi---apiVersion: v1kind: Servicemetadata: name: jenkins namespace: defaultspec: type: LoadBalancer ports: - name: http port: 8080 targetPort: 8080 - name: agent port: 50000 targetPort: 50000 selector: name: jenkins# kubectl apply -f jenkins.yamlstatefulset.apps/jenkins configuredservice/jenkins configured 配置RBAC12345678910111213141516171819202122232425262728293031323334353637383940# vim jenkins-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: jenkins---kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: jenkinsrules:- apiGroups: [""] resources: ["pods"] verbs: ["create","delete","get","list","patch","update","watch"]- apiGroups: [""] resources: ["pods/exec"] verbs: ["create","delete","get","list","patch","update","watch"]- apiGroups: [""] resources: ["pods/log"] verbs: ["get","list","watch"]- apiGroups: [""] resources: ["secrets"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: jenkinsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkinssubjects:- kind: ServiceAccount name: jenkins# kubectl apply -f jenkins-rbac.yamlserviceaccount/jenkins configuredrole.rbac.authorization.k8s.io/jenkins configuredrolebinding.rbac.authorization.k8s.io/jenkins configured# kubectl create clusterrolebinding cluster-admin-jenkins --clusterrole=cluster-admin --serviceaccount=default:default 查看相关部署情况查看 StatefulSet123456# kubectl get statefulsetNAME READY AGEgitlab 1/1 41hjenkins 1/1 41hpostgresql 1/1 41hredis 1/1 41h 查看 PV和PVC123456789101112# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-18ec78b1-4ee5-11e9-b373-000c292a7b79 10Gi RWO Delete Bound default/redis-redis-0 ceph-rbd 41hpvc-48a7f8c4-4ee9-11e9-b373-000c292a7b79 10Gi RWO Delete Bound default/postgresql-postgresql-0 ceph-rbd 41hpvc-49035b17-4eeb-11e9-b373-000c292a7b79 10Gi RWO Delete Bound default/gitlab-gitlab-0 ceph-rbd 41hpvc-b5bbbd9b-4af6-11e9-9bd3-000c292a7b79 10Gi RWO Delete Bound default/jenkins-home-jenkins-0 ceph-rbd 41h# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEgitlab-gitlab-0 Bound pvc-49035b17-4eeb-11e9-b373-000c292a7b79 10Gi RWO ceph-rbd 41hjenkins-home-jenkins-0 Bound pvc-b5bbbd9b-4af6-11e9-9bd3-000c292a7b79 10Gi RWO ceph-rbd 41hpostgresql-postgresql-0 Bound pvc-48a7f8c4-4ee9-11e9-b373-000c292a7b79 10Gi RWO ceph-rbd 41hredis-redis-0 Bound pvc-18ec78b1-4ee5-11e9-b373-000c292a7b79 10Gi RWO ceph-rbd 41h 查看 Pod123456# kubectl get podNAME READY STATUS RESTARTS AGEgitlab-0 1/1 Running 0 41hjenkins-0 1/1 Running 0 41hpostgresql-0 1/1 Running 0 41hredis-0 1/1 Running 0 41h 查看 Service123456# kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgitlab LoadBalancer 10.96.131.243 &lt;pending&gt; 80:30284/TCP,22:32377/TCP 41hjenkins LoadBalancer 10.106.240.93 &lt;pending&gt; 8080:31906/TCP,50000:31611/TCP 41hpostgresql ClusterIP 10.106.35.107 &lt;none&gt; 5432/TCP 41hredis ClusterIP 10.99.125.252 &lt;none&gt; 6379/TCP 41h 配置Jenkins 当您使用nodeport方式打开Jenkins的时候，首先会出现“Unlocking Jenkins”，这里的“Administrator password”存放在logs中，你可以通过查看日志获取，也可通过如下命令获取: 12# kubectl exec -it jenkins-0 cat /var/jenkins_home/secrets/initialAdminPassword1f297e8179664848a3e0b672ceafce1f 在之后会提示您安装插件(这里我安装了推荐插件)和配置管理员账号，请自行安装和配置。 登录后，在“系统管理”===&gt;“插件管理”===&gt;“available”(可用插件)===&gt;安装Kubernetes和GitLab插件 关于Gitlab插件的配置 在“系统管理”===&gt;“系统设置”下进行设置 Connection name: gitlab(可自行更改) Gitlab host URL: http://gitlab(该gitlab与kubernetes的service对应) Credentials: 点击“Add”或者在首页的凭据栏配置全局凭据，如下所示: 配置完成后点击“Test Connection”进行测试，如出现“Success”即为成功 关于GitLab API token 登录相关Gitlab账户，在该用户的“Setting”===&gt;“Access Tokens”下添加个人访问令牌的名称、到期时间以及授予的权限,如下所示: 之后会生成个人访问令牌，请注意保存，之后将无法再查看该token。将该token复制到Jenkins以添加凭据 关于kubernetes插件的配置 在“系统管理”===&gt;“系统设置”===&gt;拖到最后找到“云”===&gt;“新增一个云” name: kubernetes(可名称) kubernetes 地址: https://kubernetes.default(可配置为apiserver地址) Jenkins 地址: http://jenkins:8080(配置的是service名，也可以直接配置http://ip:port) Jenkins 通道: jenkins:50000(配置的是service:port，也可以配置ip:port) 这里没有配置凭据，因为上面的部署中已经赋予了相应权限，如果是集群外安装则需要添加凭据 点击“Test Connection”，如出现“Connection test successful”则说明 Jenkins已经可以和 Kubernetes 通信了 关于添加凭据 Jenkins支持添加多种类型凭据，除了上面介绍的“Gitlab API token”还有“SSH Username with private key”、“Certificate”以及常见的用户名密码类型，这里说的就是用户名密码类型，这里添加harbor镜像仓库以及Gitlab的用户名密码以备使用。 使用 GitLab 新建项目登录Gitlab 使用nodeport方式打开Gitlab，然后使用root账户登录，在“Admin Area”的“Settings”下配置“Network”，使其允许从hooks和 services向local network发出请求。 新建Gitlab账户 新建项目 写一个简单的dockerfile 使用 Jenkins 新建任务新建pipeline任务 写一个pipeline1234567891011121314151617181920212223242526272829303132333435363738394041def label = "docker-$&#123;UUID.randomUUID().toString()&#125;"podTemplate(label: label, yaml: """apiVersion: v1kind: Podspec: containers: - name: docker image: 192.168.100.100/docker/docker:18 command: ['cat'] tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock volumes: - name: dockersock hostPath: path: /var/run/docker.sock""" ) &#123; def image = "192.168.100.100/library/alpine:test" node(label) &#123; stage('Git') &#123; git credentialsId: 'gitlab',url:'http://gitlab/zhi/alpine.git' &#125; stage('Build Docker image') withCredentials([ usernamePassword( credentialsId: 'harbor', passwordVariable: 'PASSWORD', usernameVariable: 'USERNAME')]) &#123; container('docker') &#123; sh "docker build -t $&#123;image&#125; ." sh "docker login -u '$USERNAME' -p '$PASSWORD' 192.168.100.100" sh "docker push $&#123;image&#125;" sh "docker rmi $&#123;image&#125;" &#125; &#125; &#125;&#125; 注意:该流水线是参照jenkinsci-kubernetes-plugin而来。 关于该流水线的简单解释: podTemplate: 用于创建代理的pod的模板 label: pod的标签。设置唯一值以避免跨构建的冲突 containers: 用于创建pod的容器模板 containerTemplate: 将添加到pod中的容器模板 name: 容器名 image: 容器镜像 command: 容器将执行的命令 tty: true,启用TTY volumes: 为pod挂载相应卷 node: 选择相应标签 stage: 构建阶段，这将显性的显示在“Stage View”下 withCredentials: 将凭据绑定到变量 usernamePassword: 将一个变量设置为用户名，将另一个变量设置为凭据中给出的密码 credentialsId: 设置凭据ID passwordVariable: 在构建期间要设置为密码的环境变量的名称 usernameVariable: 构建期间要设置为用户名的环境变量的名称 进行构建 点击“立即构建”进行构建 定时构建 构建语法 第一个*表示分钟，取值0~59 第二个*表示小时，取值0~23 第三个*表示一个月的第几天，取值1~31 第四个*表示第几月，取值1~12 第五个*表示一周中的第几天，取值0~7，其中0和7代表的都是周日 触发构建在该任务下启用触发器 勾选将更改推送到GitLab时构建，点击高级生成“Secret token” 在Gitlab下添加Webhooks 在该项目下设置“Integrations”，将Jenkins的配置配置在“URL”和“Secret Token”项下，根据情况勾选相应触发项 在Gitlab下更新该项目 查看Jenkins是否触发了构建 关于在Jenkins中使用kubectl命令 注: 在上面的Jenkins的部署中已经做好了相关权限的分配 重构jnlp-slave 注: docker安装包和kubectl命令请自行下载至构建目录下 123456789101112131415161718192021222324252627# mkdir jnlp &amp;&amp; cd jnlp# cp .kube/config .# lsconfig docker-18.06.1-ce.tgz Dockerfile kubectl# cat Dockerfile FROM jenkins/jnlp-slave:3.27-1-alpineMAINTAINER zhi &lt;wangzhijiansd@qq.com&gt;USER rootARG DOCKER_GID=994ENV DOCKER_VERSION=18.06.1-ceCOPY docker-$&#123;DOCKER_VERSION&#125;.tgz /var/tmp/RUN tar --strip-components=1 -xvzf /var/tmp/docker-$&#123;DOCKER_VERSION&#125;.tgz -C /usr/local/bin \ &amp;&amp; rm -rf /var/tmp/docker-$&#123;DOCKER_VERSION&#125;.tgz \ &amp;&amp; chmod -R 775 /usr/local/bin/dockerCOPY kubectl /usr/local/bin/RUN mkdir -p /root/.kube/COPY config /root/.kube/RUN addgroup -g $&#123;DOCKER_GID&#125; docker &amp;&amp; adduser jenkins docker USER jenkins:$&#123;DOCKER_GID&#125;# docker build -t 192.168.100.100/jenkins/jnlp-slave:latest .# docker push 192.168.100.100/jenkins/jnlp-slave:latest 编写一个pipeline12345678910111213141516171819podTemplate(label: 'mypod', cloud: 'kubernetes', containers: [ containerTemplate( name: 'jnlp', image: '192.168.100.100/jenkins/jnlp-slave:latest', alwaysPullImage: true, args: '$&#123;computer.jnlpmac&#125; $&#123;computer.name&#125;'), ], volumes: [ hostPathVolume( mountPath: '/var/run/docker.sock', hostPath: '/var/run/docker.sock'),],) &#123; node('mypod') &#123; stage('Run shell') &#123; sh 'kubectl get pod -n kube-system' &#125; &#125;&#125; 进行构建 附: K8S+Gitlab+Jenkins构建镜像并创建Pod: https://blog.51cto.com/wangzhijian/2285861 流水线语法参考: https://jenkins.io/zh/doc/book/pipeline/syntax/ 关于凭据绑定: https://jenkins.io/doc/pipeline/steps/credentials-binding/ 关于定时构建: https://www.cnblogs.com/panpan0301/p/7738249.html]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Harbor从1.6.2升级到1.7.4]]></title>
    <url>%2Fblog%2F501be2e%2F</url>
    <content type="text"><![CDATA[Harbor最新的V1.7版本又添加了一些新的功能: 在线GC（垃圾回收）- 现在 Harbor 可以清理从后端存储中已删除的镜像且在执行GC操作之前不再要求中断 Harbor 的运行。支持按需垃圾收集，使管理员能够手动配置运行docker注册表垃圾收集或使用cron计划自动配置 镜像构建历史 - 可查看容器镜像的构建历史和内容 镜像复制（Image Retag）- 提供了在镜像上传至Harbor后重新创建镜像tag的能力。此功能在CI流水线中提升镜像到生产状态或者通过编程方式重新tag镜像，亦或将特定镜像重新tag或者移动到其它仓库或者项目等场景中特别有用 支持使用Helm Chart部署Harbor，使用户能够获得Harbor服务的高可用性 支持Logger自定义，使用户能够自定义正在运行的作业的STDOUT / STDERR / FILE / DB记录器。 升级须知 必须在任何数据迁移之前备份数据 从v1.6.0开始，Harbor会在启动时自动尝试迁移数据库模式，因此如果从v1.6.0或更高版本升级，则无需调用迁移器工具来进行迁移 从v1.6.0起，Harbor将数据库从MariaDB迁移到PostgreSQL，并将Harbor，Notary和Clair DB合并为一个 停止并删除现有的Harbor实例12# cd harbor# docker-compose -f ./docker-compose.yml -f ./docker-compose.clair.yml down 备份Harbor的当前文件1# mv harbor harbor-bak 备份数据库 默认情况下目录为/data/database 1# cp -r /data/database /root/databak/ 下载迁移工具1# docker pull goharbor/harbor-migrator:v1.7.4 升级harbor.cfg 注意: harbor.cfg将被覆盖，您必须在迁移后将其移动到安装目录 1234567# docker run -it --rm -v /root/harbor-bak/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:v1.7.4 --cfg upPlease backup before upgrade, Enter y to continue updating or n to abort: yThe path of the migrated harbor.cfg is not set, the input file will be overwritten.input version: 1.6.0, migrator chain: ['1.7.0']migrating to version 1.7.0Written new values to /harbor-migration/harbor-cfg/harbor.cfg 解压harbor离线包1# tar -zxvf harbor-offline-installer-v1.7.4.tgz 覆盖harbor.cfg123# cd harbor# mv harbor.cfg harbor.bak# cp /root/harbor-bak/harbor.cfg . 载入镜像12# docker load -i harbor.v1.7.4.tar.gz# docker images|grep 1.7.4 安装Notary，Clair和Helm Chart服务123456# ./install.sh --with-notary --with-clair --with-chartmuseum......✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at https://192.168.100.100. For more details, please visit https://github.com/goharbor/harbor . 进行查看1234567891011121314# docker-compose -f ./docker-compose.yml -f ./docker-compose.clair.yml ps Name Command State Ports -------------------------------------------------------------------------------------------------------------------------------------clair /docker-entrypoint.sh Up (healthy) 6060/tcp, 6061/tcp harbor-adminserver /harbor/start.sh Up (healthy) harbor-core /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-portal nginx -g daemon off; Up (healthy) 80/tcp nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpredis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp registryctl /harbor/start.sh Up (healthy) 清除旧版本镜像1# docker images|grep 1.6.2| awk '&#123;print $3&#125;'|xargs docker rmi 注: harbor升级和数据库迁移指南]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用Kaniko进行构建]]></title>
    <url>%2Fblog%2Fkaniko%2F</url>
    <content type="text"><![CDATA[kaniko是一个从Dockerfile，容器或Kubernetes集群内构建容器映像的工具。 kaniko不依赖于Docker守护程序，并且在用户空间中完全执行Dockerfile中的每个命令。这样可以在无法轻松或安全地运行Docker守护程序的环境中构建容器映像，例如标准Kubernetes集群。 kaniko执行程序映像负责从Dockerfile构建映像并将其推送到注册表。在执行程序映像中，提取基本映像的文件系统（Dockerfile中的FROM映像）。然后，在Dockerfile中执行命令，在每个文件系统之后对用户空间中的文件系统进行快照。在每个命令之后，将一层已更改的文件附加到基本图像（如果有的话）并更新图像元数据。 在kubernetes上构建还有另外一种选择，就是docker in docker，将宿主机的/var/run/docker.dock挂载到pod，并使用宿主机Docker守护程序执行构建。但是docker in docker必须运行在特权模式下，这会产生安全风险。 在Docker中运行kaniko123456789101112131415161718192021# mkdir app &amp;&amp; cd app# vim Dockerfile FROM 192.168.100.100/library/alpine:3.9WORKDIR /appRUN echo "hello" &gt; world.txt# docker run --env DOCKER_CONFIG=/kaniko -v /root/app:/workspace -v /etc/pki/ca-trust/source/anchors/harbor-ca.pem:/kaniko/ssl/certs/ca.pem -v /root/.docker/config.json:/kaniko/config.json 192.168.100.100/k8s.gcr.io/executor:debug -d 192.168.100.100/library/test:testINFO[0000] Downloading base image 192.168.100.100/library/alpine:3.9 INFO[0000] Error while retrieving image from cache: getting file info: stat /cache/sha256:25b4d910f4b76a63a3b45d0f69a57c34157500faf6087236581eca221c62d214: no such file or directory INFO[0000] Downloading base image 192.168.100.100/library/alpine:3.9 INFO[0000] Unpacking rootfs as cmd RUN echo "hello" &gt; world.txt requires it. INFO[0002] Taking snapshot of full filesystem... INFO[0002] RUN echo "hello" &gt; world.txt INFO[0002] cmd: /bin/sh INFO[0002] args: [-c echo "hello" &gt; world.txt] INFO[0002] Taking snapshot of full filesystem... 2019/03/08 15:35:14 existing blob: sha256:6c40cc604d8e4c121adcb6b0bfe8bb038815c350980090e74aa5a6423f8f82c02019/03/08 15:35:14 pushed blob sha256:4c1586bb248bd4662909b9c520aec7d405ba28f72be717bda7f906328ba93ed22019/03/08 15:35:14 pushed blob sha256:acf567dd059c0c462de4ef165221491959c8195b5777a7f87bdd9de7fc939bea2019/03/08 15:35:14 192.168.100.100/library/test:test: digest: sha256:c430406eda9f80eed55c12625585bb7da6947edd0332bf559a9514d7d8328601 size: 588# docker run -it 192.168.100.100/library/test:test cat /app/world.txthello 注1: –env DOCKER_CONFIG=/kaniko:设置环境变量 -v /root/app:/workspace:将/app目录挂载到/workspace构建上下文 -v /etc/pki/ca-trust/source/anchors/harbor-ca.pem:/kaniko/ssl/certs/ca.pem:将私有镜像仓库的证书挂载到/kaniko/ssl/certs/下 -v /root/.docker/config.json:/kaniko/config.json:将docker配置文件挂载到/kaniko下 192.168.100.100/k8s.gcr.io/executor:debug:运行kaniko容器(gcr.io/kaniko-project/executor) -d 192.168.100.100/library/test:test:推送的镜像 注2:对于开源镜像仓库harbor，push到镜像仓库后无法显示在UI上。对于公有云来说，由于各自的认证方式有别，push镜像可能会无法上传，hub.docker.com、阿里云容器镜像服务都可以push，网易云镜像中心无法push，提示“no token in bearer response:{“errors”:[{“code”:”DENIED”,”message”:”Real name authentication required”}]}”。 在kubernetes中运行kaniko示例dockerfile1234# vim Dockerfile FROM 192.168.100.100/library/alpine:3.9WORKDIR /appRUN echo "hello" &gt; world.txt 挂载harbor镜像仓库CA证书12# kubectl create configmap ca-certificates --from-file=/etc/pki/ca-trust/source/anchors/harbor-ca.pemconfigmap/ca-certificates created 挂载docker配置文件12# kubectl create configmap docker-config --from-file=/root/.docker/config.json configmap/docker-config created 配置yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243# vim kaniko.yaml apiVersion: v1kind: Podmetadata: name: kanikospec: restartPolicy: Never initContainers: - name: git-clone image: alpine/git args: - clone - --single-branch - -- - https://github.com/zhijiansd/gityun.git - /context volumeMounts: - name: context mountPath: /context containers: - name: kaniko image: registry.cn-hangzhou.aliyuncs.com/gityun/executor:debug args: ["--dockerfile=/context/Dockerfile", "--context=/context", "--destination=192.168.100.100/library/test:test"] volumeMounts: - name: ca-certificates mountPath: /kaniko/ssl/certs/ - name: docker-config mountPath: /kaniko/.docker/ - name: context mountPath: /context restartPolicy: Never volumes: - name: ca-certificates configMap: name: ca-certificates - name: docker-config configMap: name: docker-config - name: context emptyDir: &#123;&#125; 注: 配置Init容器使用镜像 “alpine/git” clone相关项目并挂载为context，将相关代码传递到kaniko容器进行构建 使用configmap将harbor的CA证书和docker配置文件config.json挂载到kaniko 运行并查看123456789101112131415161718192021222324# kubectl create -f kaniko.yaml pod/kaniko created# kubectl get pod|grep kanikokaniko 0/1 Completed 0 29s# kubectl logs kanikoINFO[0000] Downloading base image 192.168.100.100/library/alpine:3.9 INFO[0000] Error while retrieving image from cache: getting file info: stat /cache/sha256:25b4d910f4b76a63a3b45d0f69a57c34157500faf6087236581eca221c62d214: no such file or directory INFO[0000] Downloading base image 192.168.100.100/library/alpine:3.9 INFO[0001] Unpacking rootfs as cmd RUN echo "hello" &gt; world.txt requires it. INFO[0002] Taking snapshot of full filesystem... INFO[0002] WORKDIR /app INFO[0002] cmd: workdir INFO[0002] Changed working directory to /app INFO[0002] Creating directory /app INFO[0002] Taking snapshot of files... INFO[0002] RUN echo "hello" &gt; world.txt INFO[0002] cmd: /bin/sh INFO[0002] args: [-c echo "hello" &gt; world.txt] INFO[0002] Taking snapshot of full filesystem... 2019/03/10 08:06:41 existing blob: sha256:6c40cc604d8e4c121adcb6b0bfe8bb038815c350980090e74aa5a6423f8f82c02019/03/10 08:06:41 pushed blob sha256:a45d1670e71fec26e7147195023edc9fd26f93bbd4412fea4998528122ea43a02019/03/10 08:06:41 pushed blob sha256:a04e0385010a9eedddbdd6ddca8e90725d0b981fb7954948af67b538c781727f2019/03/10 08:06:41 pushed blob sha256:e67312291a1a10d69480c1dd22c9af51a659e48ae07ba8221877397fd74e51fe2019/03/10 08:06:41 192.168.100.100/library/test:test: digest: sha256:5c3d933724e54a605f84a233eccb95e00c1870f02f213de2157b7d143686edba size: 748 注: 日志上关于 cache 的 error 提示我根据官方提供的 flags 添加了相关命令，依然会出现该错误，但是这不影响，镜像依然 push 到了harbor 使用 kubernetes 构建到 harbor ，镜像依然无法显示在UI上，但是可以pull 对于公有云，全都无法 push，提示“connect: connection refused” 所以，最后这是一个半成品，没啥用，但是尝试了，还是写下来记录一下 附: kaniko的GitHub: https://github.com/GoogleContainerTools/kaniko 本文参考文章:https://harthoover.com/using-kaniko-for-container-builds-on-kubernetes/ harbor UI无法显示push的镜像的issue:https://github.com/goharbor/harbor/issues/6811]]></content>
      <categories>
        <category>镜像仓库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>registry</tag>
        <tag>kaniko</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm部署kubernetes]]></title>
    <url>%2Fblog%2Fkubeadm%2F</url>
    <content type="text"><![CDATA[这里我使用kubeadm部署的是1.13.3，在写该篇文章之时，恰好1.13.4发布了，所以索性将版本升级了。 配置互信1234567# vim /etc/hosts192.168.100.128 Master192.168.100.129 Node01192.168.100.130 Node02# ssh-keygen -t rsa -P ''# ssh-copy-id -i .ssh/id_rsa.pub root@node01# ssh-copy-id -i .ssh/id_rsa.pub root@node02 安装 Ansible123456# yum -y install ansible# cat /etc/ansible/hosts | grep -v ^# | grep -v ^$[node]node01node02# ansible node -m copy -a 'src=/etc/hosts dest=/etc/' 关闭 SELinux 和 Firewall123456# sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config# systemctl disable firewalld &amp;&amp; systemctl stop firewalld# ansible node -m copy -a 'src=/etc/selinux/config dest=/etc/selinux/'# ansible node -a 'systemctl stop firewalld'# ansible node -a 'systemctl disable firewalld' 安装 docker123456789101112# yum install -y yum-utils device-mapper-persistent-data lvm2# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# yum makecache fast# yum list docker-ce --showduplicates | sort -r# yum install -y docker-ce-18.06.1.ce-3.el7# systemctl enable docker &amp;&amp; systemctl start docker# ansible node -m yum -a "state=present name=yum-utils"# ansible node -m copy -a 'src=/etc/yum.repos.d/docker-ce.repo dest=/etc/yum.repos.d/'# ansible node -m yum -a "state=present name=docker-ce-18.06.1.ce-3.el7"# ansible node -a 'systemctl start docker'# ansible node -a 'systemctl enable docker' 解压 kubernetes123456789# tar -zxvf kubernetes-server-linux-amd64.tar.gz # cd kubernetes/server/bin/# docker load -i kube-apiserver.tar# docker load -i kube-controller-manager.tar # docker load -i kube-scheduler.tar# docker load -i kube-proxy.tar# ansible node -m copy -a 'src=kube-proxy.tar dest=/root'# ansible node -m command -a "docker load -i kube-proxy.tar" 配置 kubernetes 源123456789101112131415161718# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# yum install -y kubelet kubeadm kubectl# systemctl enable kubelet &amp;&amp; systemctl start kubelet# ansible node -m copy -a 'src=/etc/yum.repos.d/kubernetes.repo dest=/etc/yum.repos.d/'# ansible node -m yum -a "state=present name=kubelet"# ansible node -m yum -a "state=present name=kubeadm"# ansible node -m yum -a "state=present name=kubectl"# ansible node -a 'systemctl start kubelet'# ansible node -a 'systemctl enable kubelet' 配置 kube-proxy 代理模式1234567891011# grep -v ^# /etc/sysctl.conf net.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# sysctl -pnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1# ansible node -m copy -a 'src=/etc/sysctl.conf dest=/etc/'# ansible node -a 'sysctl -p' 查看需要使用的镜像12345678# kubeadm config images listk8s.gcr.io/kube-apiserver:v1.13.3k8s.gcr.io/kube-controller-manager:v1.13.3k8s.gcr.io/kube-scheduler:v1.13.3k8s.gcr.io/kube-proxy:v1.13.3k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.2.24k8s.gcr.io/coredns:1.2.6 下载余下的镜像 请自行更改tag 123# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.6# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.2.24 启用 swap123# vim /etc/sysconfig/kubeletKUBELET_EXTRA_ARGS=--fail-swap-on=false# ansible node -m copy -a 'src=/etc/sysconfig/kubelet dest=/etc/sysconfig/' 初始化集群123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# kubeadm init \ --kubernetes-version=v1.13.3 \ --pod-network-cidr=10.244.0.0/16 \ --apiserver-advertise-address=192.168.100.128 \ --ignore-preflight-errors=Swap [init] Using Kubernetes version: v1.13.3[preflight] Running pre-flight checks [WARNING Swap]: running with swap on is not supported. Please disable swap[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder "/etc/kubernetes/pki"[certs] Generating "etcd/ca" certificate and key[certs] Generating "etcd/healthcheck-client" certificate and key[certs] Generating "apiserver-etcd-client" certificate and key[certs] Generating "etcd/server" certificate and key[certs] etcd/server serving cert is signed for DNS names [master localhost] and IPs [192.168.100.128 127.0.0.1 ::1][certs] Generating "etcd/peer" certificate and key[certs] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.100.128 127.0.0.1 ::1][certs] Generating "ca" certificate and key[certs] Generating "apiserver" certificate and key[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.100.128][certs] Generating "apiserver-kubelet-client" certificate and key[certs] Generating "front-proxy-ca" certificate and key[certs] Generating "front-proxy-client" certificate and key[certs] Generating "sa" key and public key[kubeconfig] Using kubeconfig folder "/etc/kubernetes"[kubeconfig] Writing "admin.conf" kubeconfig file[kubeconfig] Writing "kubelet.conf" kubeconfig file[kubeconfig] Writing "controller-manager.conf" kubeconfig file[kubeconfig] Writing "scheduler.conf" kubeconfig file[control-plane] Using manifest folder "/etc/kubernetes/manifests"[control-plane] Creating static Pod manifest for "kube-apiserver"[control-plane] Creating static Pod manifest for "kube-controller-manager"[control-plane] Creating static Pod manifest for "kube-scheduler"[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s[kubelet-check] Initial timeout of 40s passed.[apiclient] All control plane components are healthy after 47.012976 seconds[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "master" as an annotation[mark-control-plane] Marking the node master as control-plane by adding the label "node-role.kubernetes.io/master=''"[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: mrtv9n.fdvmt32f3kkbyyjx[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.100.128:6443 --token mrtv9n.fdvmt32f3kkbyyjx --discovery-token-ca-cert-hash sha256:0b5fefef7ca78df72d8d35d3b0e05511d24be0365b0b403f55c8438167606654 配置访问集群12345678# mkdir -p $HOME/.kube# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config# chown $(id -u):$(id -g) $HOME/.kube/config# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy &#123;"health": "true"&#125; 安装 flannel(删减多余的配置)1234567891011121314151617# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# kubectl apply -f kube-flannel.yml clusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.extensions/kube-flannel-ds-amd64 created# kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-86c58d9df4-cm8gv 1/1 Running 0 41mcoredns-86c58d9df4-xpccv 1/1 Running 0 41metcd-master 1/1 Running 0 40mkube-apiserver-master 1/1 Running 0 40mkube-controller-manager-master 1/1 Running 0 41mkube-flannel-ds-amd64-xz6bf 1/1 Running 0 32skube-proxy-29pzf 1/1 Running 0 41mkube-scheduler-master 1/1 Running 0 41m 添加 Node 节点1234567891011# vim node.sh#!/bin/bashkubeadm join 192.168.100.128:6443 --token mrtv9n.fdvmt32f3kkbyyjx --discovery-token-ca-cert-hash sha256:0b5fefef7ca78df72d8d35d3b0e05511d24be0365b0b403f55c8438167606654 --ignore-preflight-errors=Swap# ansible node -m copy -a 'src=/root/node.sh dest=/root/ mode=755'# ansible node -m copy -a 'src=/root/node.sh dest=/root/ mode=755'# ansible node -m shell -a '/root/node.sh' # kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 68m v1.13.3node01 Ready &lt;none&gt; 64s v1.13.3node02 Ready &lt;none&gt; 64s v1.13.3 kubeadm升级计划 检查可用于升级的版本，并验证当前群集是否可升级。要跳过互联网检查，请传入可选的[version]参数。 12345678910111213141516171819202122232425262728293031# kubeadm upgrade plan 1.13.4[preflight] Running pre-flight checks.[upgrade] Making sure the cluster is healthy:[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.13.3[upgrade/versions] kubeadm version: v1.13.3Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT CURRENT AVAILABLEKubelet 3 x v1.13.3 1.13.4Upgrade to the latest version in the v1.13 series:COMPONENT CURRENT AVAILABLEAPI Server v1.13.3 1.13.4Controller Manager v1.13.3 1.13.4Scheduler v1.13.3 1.13.4Kube Proxy v1.13.3 1.13.4CoreDNS 1.2.6 1.2.6Etcd 3.2.24 3.2.24You can now apply the upgrade by executing the following command: kubeadm upgrade apply 1.13.4Note: Before you can perform this upgrade, you have to update kubeadm to 1.13.4._____________________________________________________________________ 在master节点解压新版本kubernetes1234567# tar -zxvf kubernetes-server-linux-amd64.tar.gz# cd kubernetes/server/bin/# cp kubeadm /usr/bin/# docker load -i kube-apiserver.tar# docker load -i kube-controller-manager.tar# docker load -i kube-scheduler.tar# docker load -i kube-proxy.tar 将Kubernetes群集升级到指定版本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# kubeadm upgrade apply 1.13.4[preflight] Running pre-flight checks.[upgrade] Making sure the cluster is healthy:[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file.[upgrade/version] You have chosen to change the cluster version to "v1.13.4"[upgrade/versions] Cluster version: v1.13.3[upgrade/versions] kubeadm version: v1.13.4[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd][upgrade/prepull] Prepulling image for component etcd.[upgrade/prepull] Prepulling image for component kube-apiserver.[upgrade/prepull] Prepulling image for component kube-controller-manager.[upgrade/prepull] Prepulling image for component kube-scheduler.[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler[upgrade/prepull] Prepulled image for component kube-controller-manager.[upgrade/prepull] Prepulled image for component kube-apiserver.[upgrade/prepull] Prepulled image for component etcd.[upgrade/prepull] Prepulled image for component kube-scheduler.[upgrade/prepull] Successfully prepulled the images for all the control plane components[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.13.4"...Static pod: kube-apiserver-master hash: b9152d72f9c05c3d3f7b4ac7268324c6Static pod: kube-controller-manager-master hash: 8288866dd95d24b3f0eb40747d951fbaStatic pod: kube-scheduler-master hash: b734fcc86501dde5579ce80285c0bf0c[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests466472581"[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-02-14-27-45/kube-apiserver.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-apiserver-master hash: b9152d72f9c05c3d3f7b4ac7268324c6Static pod: kube-apiserver-master hash: 2c4b7dbda2d0962b4cc2b6c98516bf14[apiclient] Found 1 Pods for label selector component=kube-apiserver[upgrade/staticpods] Component "kube-apiserver" upgraded successfully![upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-02-14-27-45/kube-controller-manager.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-controller-manager-master hash: 8288866dd95d24b3f0eb40747d951fbaStatic pod: kube-controller-manager-master hash: b6ca67226d47ac720e105375a9846904[apiclient] Found 1 Pods for label selector component=kube-controller-manager[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully![upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-02-14-27-45/kube-scheduler.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-scheduler-master hash: b734fcc86501dde5579ce80285c0bf0cStatic pod: kube-scheduler-master hash: 4b52d75cab61380f07c0c5a69fb371d4[apiclient] Found 1 Pods for label selector component=kube-scheduler[upgrade/staticpods] Component "kube-scheduler" upgraded successfully![uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "master" as an annotation[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxy[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.13.4". Enjoy![upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. 升级node升级节点配置12345# kubeadm upgrade node config --kubelet-version v1.13.4[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[upgrade] The configuration for this node was successfully updated![upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. 将相关节点标记为不可调度123456# kubectl drain master --ignore-daemonsetsnode/master cordonedWARNING: Ignoring DaemonSet-managed pods: kube-flannel-ds-amd64-xz6bf, kube-proxy-dlck4node/master drained# kubectl get nodes |grep mastermaster Ready,SchedulingDisabled master 16d v1.13.3 更新相应软件包并解锁该节点12345678# cd kubernetes/server/bin/# systemctl stop kubelet# cp kubeadm kubectl kubelet /usr/bin/# systemctl start kubelet# kubectl uncordon masternode/master uncordoned# kubectl get nodes|grep mastermaster Ready master 16d v1.13.4 更新其他节点123456789101112131415161718192021222324252627282930313233343536# kubectl drain node01 --ignore-daemonsetsnode/node01 cordonedWARNING: Ignoring DaemonSet-managed pods: kube-flannel-ds-amd64-8q4kq, kube-proxy-jvz4zpod/coredns-86c58d9df4-97fw8 evictedpod/rbd-provisioner-6447467945-jjz7j evictednode/node01 evicted# ansible node01 -a "systemctl stop kubelet"# ansible node01 -m copy -a 'src=kubeadm dest=/usr/bin/'# ansible node01 -m copy -a 'src=kubectl dest=/usr/bin/'# ansible node01 -m copy -a 'src=kubelet dest=/usr/bin/'# ansible node01 -a "systemctl start kubelet"# kubectl uncordon node01node/node01 uncordoned# kubectl get nodes|grep node01node01 Ready &lt;none&gt; 16d v1.13.4# kubectl drain node02 --ignore-daemonsetsnode/node02 cordonedWARNING: Ignoring DaemonSet-managed pods: kube-flannel-ds-amd64-8zw8z, kube-proxy-2hlp7pod/rbd-provisioner-6447467945-p2dgr evictedpod/coredns-86c58d9df4-6fp9p evictednode/node02 evicted# ansible node02 -a "systemctl stop kubelet"# ansible node02 -m copy -a 'src=kubeadm dest=/usr/bin/'# ansible node02 -m copy -a 'src=kubectl dest=/usr/bin/'# ansible node02 -m copy -a 'src=kubelet dest=/usr/bin/'# ansible node02 -a "systemctl start kubelet"# kubectl uncordon node02node/node02 uncordoned# kubectl get nodes|grep node02node02 Ready &lt;none&gt; 16d v1.13.4# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 16d v1.13.4node01 Ready &lt;none&gt; 16d v1.13.4node02 Ready &lt;none&gt; 16d v1.13.4]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过Dex和openLDAP进行Kubernetes身份验证]]></title>
    <url>%2Fblog%2Fdex%2F</url>
    <content type="text"><![CDATA[Dex是一种身份服务，它使用OpenID Connect(简称OIDC)来驱动其他应用程序的身份验证。 Dex通过“connectors.”充当其他身份提供商的门户。这使得dex可以将身份验证延迟(找不到很好的词来形容，只能硬翻了)到LDAP服务器、SAML提供程序或已建立的身份提供程序（如GitHub，Google和Active Directory）。客户端编写一次身份验证逻辑与dex通信，然后dex处理给定后端的协议。 OAuth2OAuth（开放授权）是一个开放标准，允许用户授权第三方移动应用访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方移动应用或分享他们数据的所有内容。 我相信大家都使用过类似“使用QQ登录”诸如此类的按钮来登录一些第三方的应用或者网站。在这些情况下，第三方应用程序(网站)选择让外部提供商（在这种情况下为QQ）证明您的身份，而不是让您使用应用程序本身设置用户名和密码。 服务器端应用程序的一般流程是： 新用户访问应用程序。 用户点击网站上的登录按钮(诸如“使用QQ登录”)，该应用程序将用户重定向到QQ。 用户登录QQ，然后QQ会提示该应用程序会获取的相应权限。 如果用户单击“授权并登录”，则QQ会连同获取的Access Token使用代码将用户重定向回该应用程序。 应用程序通过Access Token获取用户的OpenID；调用OpenAPI，来请求访问或修改用户授权的资源。 在这些情况下，dex充当QQ（在OpenID Connect中称为“provider”），而客户端应用程序重定向到它以获得最终用户的身份。 关于OAuth: https://oauth.net/2/ ID TokensID Tokens是OpenID Connect和dex主要功能引入的OAuth2扩展。ID Tokens是由dex签名的JSON Web令牌（JWT），作为OAuth2响应的一部分返回，用于证明最终用户的身份。 OpenID Connect的OAuth2主要扩展名是令牌响应中返回的额外令牌，称为ID Tokens。此令牌是由OpenID Connect服务器签名的JSON Web令牌，具有用户ID，名称，电子邮件等众所周知的字段。 Connectors当用户通过dex登录时，用户的身份通常存储在另一个用户管理系统中：LDAP目录，GitHub组织等。Dex充当客户端应用程序和上游身份提供者之间的中间人。客户端只需要了解OpenID Connect来查询dex，而dex实现了一组用于查询其他用户管理系统的协议。 OpenID Connect TokensOpenID Connect 1.0是OAuth 2.0协议之上的简单身份层。它允许客户端根据授权服务器执行的身份验证来验证最终用户的身份，以及以可互操作和类似REST的方式获取有关最终用户的基本配置文件信息。 OpenID Connect允许所有类型的客户端（包括基于Web，移动和JavaScript客户端）请求和接收有关经过身份验证的会话和最终用户的信息。规范套件是可扩展的，允许参与者在对它们有意义时使用可选功能，例如身份数据加密，OpenID提供程序的发现和会话管理。 协议的OAuth2的主要扩展是返回的附加字段，其中访问令牌称为ID Token。此令牌是JSON Web令牌（JWT），具有由服务器签名的众所周知的字段，例如用户的电子邮件。 为了识别用户，验证者使用OAuth2 令牌响应中的id_token（而不是access_token） 作为承载令牌。 来自OpenID Connect提供商的令牌响应包括一个称为ID令牌的签名JWT。ID令牌包含名称，电子邮件，唯一标识符，在dex的情况下，包含一组可用于标识用户的组。像dex这样的OpenID Connect提供程序发布公钥; Kubernetes API服务器了解如何使用它们来验证ID令牌。 关于OpenID Connect: https://openid.net/connect/ 身份验证流程如下所示： OAuth2客户端通过dex登录用户。 在与Kubernetes API通信时，该客户端使用返回的ID令牌作为承载令牌。 Kubernetes使用dex的公钥来验证ID令牌。 指定为用户名（以及可选的组信息）的声明将与该请求相关联。 用户名和组信息可以与Kubernetes 授权插件（例如基于角色的访问控制（RBAC））结合使用以实施策略。 dex有自己的用户概念，但它允许它们以不同的方式进行身份验证，称为connectors。目前，dex提供两种类型的连接器：local连接器和OIDC连接器。使用local连接器进行身份验证时，用户使用电子邮件和密码登录，并使用dex本身提供的可自定义UI。使用OIDC连接器，用户可以通过登录到另一个OIDC身份提供商（如Google或Salesforce）进行身份验证。 从dex请求ID令牌直接使用dex对用户进行身份验证的应用程序使用OAuth2代码流来请求令牌响应。采取的确切步骤是： 用户访问客户端应用。 客户端应用程序通过OAuth2请求将用户重定向到dex。 Dex确定用户的身份。 Dex使用代码将用户重定向到客户端。 客户端使用dex为id_token交换代码。 部署 openLDAP配置PVC12345678910111213141516171819202122232425262728293031323334# vim openldap-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-data namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi"---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-config namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "1Gi"# kubectl create -f openldap-pvc.yaml persistentvolumeclaim/ldap-data createdpersistentvolumeclaim/ldap-config created 部署openLDAP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# vim openldap.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ldap labels: app: ldapspec: replicas: 1 template: metadata: labels: app: ldap spec: containers: - name: ldap image: 192.168.100.100/library/openldap:1.2.2 volumeMounts: - name: ldap-data mountPath: /var/lib/ldap - name: ldap-config mountPath: /etc/ldap/slapd.d - name: ldap-certs mountPath: /container/service/slapd/assets/certs ports: - containerPort: 389 name: openldap env: - name: LDAP_LOG_LEVEL value: "256" - name: LDAP_ORGANISATION value: "zhi" - name: LDAP_DOMAIN value: "flywzj.com" - name: LDAP_ADMIN_PASSWORD value: "admin" - name: LDAP_CONFIG_PASSWORD value: "config" - name: LDAP_READONLY_USER value: "false" - name: LDAP_READONLY_USER_USERNAME value: "readonly" - name: LDAP_READONLY_USER_PASSWORD value: "readonly" - name: LDAP_RFC2307BIS_SCHEMA value: "false" - name: LDAP_BACKEND value: "mdb" - name: LDAP_TLS value: "true" - name: LDAP_TLS_CRT_FILENAME value: "ldap.crt" - name: LDAP_TLS_KEY_FILENAME value: "ldap.key" - name: LDAP_TLS_CA_CRT_FILENAME value: "ca.crt" - name: LDAP_TLS_ENFORCE value: "false" - name: LDAP_TLS_CIPHER_SUITE value: "SECURE256:+SECURE128:-VERS-TLS-ALL:+VERS-TLS1.2:-RSA:-DHE-DSS:-CAMELLIA-128-CBC:-CAMELLIA-256-CBC" - name: LDAP_TLS_VERIFY_CLIENT value: "demand" - name: LDAP_REPLICATION value: "false" - name: LDAP_REPLICATION_CONFIG_SYNCPROV value: "binddn=\"cn=admin,cn=config\" bindmethod=simple credentials=$LDAP_CONFIG_PASSWORD searchbase=\"cn=config\" type=refreshAndPersist retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_DB_SYNCPROV value: "binddn=\"cn=admin,$LDAP_BASE_DN\" bindmethod=simple credentials=$LDAP_ADMIN_PASSWORD searchbase=\"$LDAP_BASE_DN\" type=refreshAndPersist interval=00:00:00:10 retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_HOSTS value: "#PYTHON2BASH:['ldap://ldap-one-service', 'ldap://ldap-two-service']" - name: KEEP_EXISTING_CONFIG value: "false" - name: LDAP_REMOVE_CONFIG_AFTER_SETUP value: "true" - name: LDAP_SSL_HELPER_PREFIX value: "ldap" volumes: - name: ldap-data persistentVolumeClaim: claimName: ldap-data #hostPath: # path: "/data/ldap/db" - name: ldap-config persistentVolumeClaim: claimName: ldap-config #hostPath: # path: "/data/ldap/config" - name: ldap-certs hostPath: path: "/data/ldap/certs"---apiVersion: v1kind: Servicemetadata: labels: app: ldap name: ldapspec: type: NodePort ports: - port: 389 nodePort: 38989 selector: app: ldap# kubectl create -f openldap.yaml deployment.extensions/ldap createdservice/ldap created# kubectl get pods -l app=ldapNAME READY STATUS RESTARTS AGEldap-65f5786ff8-xrtkk 1/1 Running 0 28d 部署openldapadmin12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# vim phpldapadmin.yaml apiVersion: v1kind: ReplicationControllermetadata: name: phpldapadmin-controller labels: app: phpldapadminspec: replicas: 1 selector: app: phpldapadmin template: metadata: labels: app: phpldapadmin spec: containers: - name: phpldapadmin image: 192.168.100.100/library/phpldapadmin:0.7.2 volumeMounts: - name: phpldapadmin-certs mountPath: /container/service/phpldapadmin/assets/apache2/certs - name: ldap-client-certs mountPath: /container/service/ldap-client/assets/certs ports: - containerPort: 443 env: - name: PHPLDAPADMIN_LDAP_HOSTS #value: "#PYTHON2BASH:[&#123;'ldap-service': [&#123;'server': [&#123;'tls': 'true'&#125;]&#125;]&#125;]" value: "ldap" - name: PHPLDAPADMIN_SERVER_ADMIN value: "wangzhijiansd@qq.com" - name: PHPLDAPADMIN_SERVER_PATH value: "/phpldapadmin" - name: PHPLDAPADMIN_HTTPS value: "true" - name: PHPLDAPADMIN_HTTPS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_HTTPS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_HTTPS_CA_CRT_FILENAME value: "ca.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS value: "true" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_REQCERT value: "demand" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CA_CRT_FILENAME value: "ca.crt" volumes: - name: phpldapadmin-certs hostPath: path: "/data/phpldapadmin/ssl/" - name: ldap-client-certs hostPath: path: "/data/phpldapadmin/ldap-client-certs/"---apiVersion: v1kind: Servicemetadata: labels: app: phpldapadmin name: phpldapadminspec: type: NodePort ports: - port: 443 nodePort: 32001 selector: app: phpldapadmin# kubectl create -f phpldapadmin.yaml replicationcontroller/phpldapadmin-controller createdservice/phpldapadmin created# kubectl get pods -l app=phpldapadminNAME READY STATUS RESTARTS AGEphpldapadmin-controller-4cwlb 1/1 Running 0 28d 关于 openLDAP 的相关镜像详见: https://github.com/osixia/docker-openldap https://github.com/osixia/docker-openldap-backup https://github.com/osixia/docker-phpLDAPadmin 配置 openLDAP使用 phpldapadmin 进行配置管理 输入 https://nodeip:nodeport 登录 login: Login DN: cn=admin,dc=flywzj,dc=com Password: admin 点击 Authenticate 登录 登录LDAP容器进行配置管理 注1:Dex目前允许不安全的连接，但是Dex官方强烈建议使用TLS，通过使用端口636而不是389来实现。这里使用的是不安全的389端口来实现，请知悉。 注2:这里配置两个组，组k8s关联用户wang，组test关联用户zhi 查看当前LDAP配置1234567891011121314151617181920212223242526272829303132# kubectl exec -it ldap-65f5786ff8-xrtkk /bin/bashroot@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NFpVZU5IaGhDSzZ4OWF2KzBCSjlZOUY4SzRhWTdpWUk=# search resultsearch: 2result: 0 Success# numResponses: 3# numEntries: 2 配置新建OU和组123456789101112131415161718192021222324252627282930313233root@ldap-65f5786ff8-xrtkk:/# cat &lt;&lt;EOF &gt; container/service/slapd/assets/groups.ldifdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: topdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: topdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberuid: wangdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberuid: zhiEOFroot@ldap-65f5786ff8-xrtkk:/# ldapadd -x -D "cn=admin,dc=flywzj,dc=com" -w admin -f /container/service/slapd/assets/groups.ldif -H ldap:/// adding new entry "ou=Groups,dc=flywzj,dc=com"adding new entry "ou=People,dc=flywzj,dc=com"adding new entry "cn=k8s,ou=Groups,dc=flywzj,dc=com"adding new entry "cn=test,ou=Groups,dc=flywzj,dc=com" 如上可以看出，这里新建了两个OU: ou=Groups,dc=flywzj,dc=com ou=People,dc=flywzj,dc=com 同时新建了两个组: cn=k8s,ou=Groups,dc=flywzj,dc=com cn=test,ou=Groups,dc=flywzj,dc=com 特别说明，所有配置是都可以放在一个 ldif 文件中来进行配置的，这里分成两个 ldif 文件来配置就是为了方便理解和排版。 查看配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859root@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NHNid2tMZCtnSS9LazJieGRsdWdhZFN1OGR0ZE4wVjA=# Groups, flywzj.comdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, flywzj.comdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, flywzj.comdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberUid: wang# test, Groups, flywzj.comdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberUid: zhi# search resultsearch: 2result: 0 Success# numResponses: 9# numEntries: 8 配置用户123456789101112131415161718192021222324252627282930313233343536373839# cat &lt;&lt;EOF &gt; /container/service/slapd/assets/users.ldif dn: uid=wang,ou=People,dc=flywzj,dc=comcn: wanggidnumber: 500givenname: wanghomedirectory: /home/users/wangloginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: wangzhijiansd@qq.comsn: wanguid: wanguidnumber: 1000userpassword: wangzhijiandn: uid=zhi,ou=People,dc=flywzj,dc=comhomedirectory: /home/users/zhiloginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: zhijiansd@163.comcn: zhigivenname: zhisn: zhiuid: zhiuidnumber: 1001gidnumber: 501userpassword: zhijianEOFroot@ldap-65f5786ff8-xrtkk:/# ldapadd -x -D "cn=admin,dc=flywzj,dc=com" -w admin -f /container/service/slapd/assets/users.ldif -H ldap:/// adding new entry "uid=wang,ou=People,dc=flywzj,dc=com"adding new entry "uid=zhi,ou=People,dc=flywzj,dc=com" 查看配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495root@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NHNid2tMZCtnSS9LazJieGRsdWdhZFN1OGR0ZE4wVjA=# Groups, flywzj.comdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, flywzj.comdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, flywzj.comdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberUid: wang# test, Groups, flywzj.comdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberUid: zhi# wang, People, flywzj.comdn: uid=wang,ou=People,dc=flywzj,dc=comcn: wanggidNumber: 500givenName: wanghomeDirectory: /home/users/wangloginShell: /bin/shobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: wangzhijiansd@qq.comsn: wanguid: wanguidNumber: 1000userPassword:: d2FuZ3poaWppYW4=# zhi, People, flywzj.comdn: uid=zhi,ou=People,dc=flywzj,dc=comhomeDirectory: /home/users/zhiloginShell: /bin/shobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: zhijiansd@163.comcn: zhigivenName: zhisn: zhiuid: zhiuidNumber: 1001gidNumber: 501userPassword:: emhpamlhbg==# search resultsearch: 2result: 0 Success# numResponses: 9# numEntries: 8 这时候你也可以使用 phpldapadmin 登录查看，特别说明，如果登录进去出现了一些”?”提示，那是因为某些模板没有导入导致的，可忽略。 部署 Dex Dex详见:https://github.com/dexidp/dex 生成证书并配置secret 生成dex server和login application相关证书和secret 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# vim gencert.sh#!/bin/bashmkdir -p sslcat &lt;&lt; EOF &gt; ssl/req.cnf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = dexDNS.2 = dex.svc.cluster.localDNS.3 = loginappDNS.4 = loginapp.svc.cluster.localDNS.5 = login.flywzj.comIP.1 = 192.168.100.181IP.2 = 192.168.100.182IP.3 = 192.168.100.183EOFopenssl genrsa -out ssl/dex-ca-key.pem 2048openssl req -x509 -new -nodes -key ssl/dex-ca-key.pem -days 1000 -out ssl/dex-ca.pem -subj "/CN=kube-ca"openssl genrsa -out ssl/dex-app-key.pem 2048openssl req -new -key ssl/dex-app-key.pem -out ssl/dex-app-csr.pem -subj "/CN=kube-ca" -config ssl/req.cnfopenssl x509 -req -in ssl/dex-app-csr.pem -CA ssl/dex-ca.pem -CAkey ssl/dex-ca-key.pem -CAcreateserial -out ssl/dex-app.pem -days 1000 -extensions v3_req -extfile ssl/req.cnfkubectl create secret tls dex --cert=ssl/dex-app.pem --key=ssl/dex-app-key.pem kubectl create secret tls loginapp --cert=ssl/dex-app.pem --key=ssl/dex-app-key.pem # ./gencert.sh Generating RSA private key, 2048 bit long modulus........................+++.......................................+++e is 65537 (0x10001)Generating RSA private key, 2048 bit long modulus...........................................+++........................+++e is 65537 (0x10001)Signature oksubject=/CN=kube-caGetting CA Private Keysecret/dex createdsecret/loginapp created# kubectl get secret dexNAME TYPE DATA AGEdex kubernetes.io/tls 2 8h# kubectl get secret loginappNAME TYPE DATA AGEloginapp kubernetes.io/tls 2 8h 复制证书 用于为Dex签署SSL证书的CA文件需要复制到apiserver可以读取的位置 1# cp ssl/dex-ca.pem /etc/kubernetes/ssl/ 部署 Dex123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163# wget https://raw.githubusercontent.com/dexidp/dex/master/examples/k8s/dex.yaml# vim dex.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: dex name: dex namespace: default spec: replicas: 1 template: metadata: labels: app: dex spec: serviceAccountName: dex # This is created below containers: - image: 192.168.100.100/coreos/dex:v2.10.0 name: dex command: ["/usr/local/bin/dex", "serve", "/etc/dex/cfg/config.yaml"] ports: - name: https containerPort: 5556 volumeMounts: - name: config mountPath: /etc/dex/cfg - name: tls mountPath: /etc/dex/tls volumes: - name: config configMap: name: dex items: - key: config.yaml path: config.yaml - name: tls secret: secretName: dex---kind: ConfigMapapiVersion: v1metadata: name: dex namespace: default data: config.yaml: | issuer: https://192.168.100.185:32000 storage: type: kubernetes config: inCluster: true web: https: 0.0.0.0:5556 tlsCert: /etc/dex/tls/tls.crt tlsKey: /etc/dex/tls/tls.key logger: level: "debug" format: text connectors: - type: ldap id: ldap name: LDAP config: host: ldap:389 insecureNoSSL: true insecureSkipVerify: true bindDN: cn=admin,dc=flywzj,dc=com bindPW: admin userSearch: baseDN: ou=People,dc=flywzj,dc=com filter: "(objectClass=posixAccount)" username: mail idAttr: uid emailAttr: mail nameAttr: uid groupSearch: baseDN: ou=Groups,dc=flywzj,dc=com filter: "(objectClass=posixGroup)" userAttr: uid groupAttr: memberUid nameAttr: cn oauth2: skipApprovalScreen: true staticClients: - id: login redirectURIs: - 'https://192.168.100.185:32002/callback' name: 'Login App' secret: 4TORGiNV9M54BTk1v7dNuFSaI6hUjfjr enablePasswordDB: true staticPasswords: - email: "wangzhijiansd@qq.com" # bcrypt hash of the string "password" hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W" username: "admin" userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"---apiVersion: v1kind: Servicemetadata: name: dex namespace: default spec: type: NodePort ports: - name: dex port: 5556 protocol: TCP targetPort: 5556 nodePort: 32000 selector: app: dex---apiVersion: v1kind: ServiceAccountmetadata: labels: app: dex name: dex namespace: default ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: dexrules:- apiGroups: ["dex.coreos.com"] # API group created by dex resources: ["*"] verbs: ["*"]- apiGroups: ["apiextensions.k8s.io"] resources: ["customresourcedefinitions"] verbs: ["create"] # To manage its own resources, dex must be able to create customresourcedefinitions---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: dexroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: dexsubjects:- kind: ServiceAccount name: dex # Service account assigned to the dex pod, created above namespace: default # The namespace dex is running in# kubectl create -f dex.yaml deployment.extensions/dex createdconfigmap/dex createdservice/dex createdserviceaccount/dex createdclusterrole.rbac.authorization.k8s.io/dex createdclusterrolebinding.rbac.authorization.k8s.io/dex created# kubectl get pod --show-labels -l app=dex# kubectl get services dex 注意1: yaml文件虽然是从官方的，但是做了一些改动，特别是这里将官方的3个副本更改为了1个副本，因为使用3个副本事会产生错误，我在issue中翻阅到有说是NTP时钟不同步造成的。 注意2: 这里的 connectors 是LDAP，详细文档见:https://github.com/dexidp/dex/blob/master/Documentation/connectors/ldap.md 查看 OpenID Connect 发现12345678910111213141516171819202122232425262728293031323334353637# curl -k https://192.168.100.185:32000/.well-known/openid-configuration&#123; "issuer": "https://192.168.100.185:32000", "authorization_endpoint": "https://192.168.100.185:32000/auth", "token_endpoint": "https://192.168.100.185:32000/token", "jwks_uri": "https://192.168.100.185:32000/keys", "response_types_supported": [ "code" ], "subject_types_supported": [ "public" ], "id_token_signing_alg_values_supported": [ "RS256" ], "scopes_supported": [ "openid", "email", "groups", "profile", "offline_access" ], "token_endpoint_auth_methods_supported": [ "client_secret_basic" ], "claims_supported": [ "aud", "email", "email_verified", "exp", "iat", "iss", "locale", "name", "sub" ]&#125; 查看 JSON Web Key123456789101112# curl -k https://192.168.100.185:32000/keys&#123; "keys": [ &#123; "use": "sig", "kty": "RSA", "kid": "a813de5c6100949abc59317714e3b09abecf8641", "alg": "RS256", "n": "u7G_RoZEuDwiW7kLBCMjjJMm1NgnHIXiTznxABe3uW8GsdASqRhUsDH2zFceZZObKchHWrKpkPZS4SjvcThF785xoJ4-FlAcrsUd4agyN9uwrAeL_luOrXvl-i0QAUKIHlqbTfZmzBIaFhHnG0yXKgqkXzTarQxDeynWVrVTdWsm7P_BYjQ5dnIlZu1xeRzw-NWf5UAi9Csh1x82XMtlAbMgWlJoWI36yVCCGUdJYintSp-tOfjkPBUghIO7ju8fb22X5uOgRFMq_RkIpXs2asf5FapVQMpcX_WAK3vUhmfH5F0lQZ9Cv9U__k3rHKRS7XwkcSQ4OKf7Vxrx4LQEcQ", "e": "AQAB" &#125;&#125; 配置 Login App 一旦启动并运行dex，下一步就是编写使用dex驱动身份验证的应用程序。具体详见:https://github.com/dexidp/dex/blob/master/Documentation/using-dex.md。 这里我们使用 loginapp 来配置:https://github.com/fydrah/loginapp。 配置 Configmap12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# cat ssl/dex-ca.pem -----BEGIN CERTIFICATE-----MIIC9zCCAd+gAwIBAgIJAJPzlo1fzzxqMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNVBAMMB2t1YmUtY2EwHhcNMTkwMTA0MDYyNzM5WhcNMjEwOTMwMDYyNzM5WjASMRAwDgYDVQQDDAdrdWJlLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxnFM6sykmKdmr1Z4DADujeIUZXvz5ajy+xdjpfjrNhnr3002GsthAlN6lWjJP6dnuVN20M1W/oozF7OzMuSRNO6zLepAG2TrRl/1DZG9EFSN7m65HtxK0DA3RyZc4CuDj3ADT899yziyaVTvUjR8MxLWHeibgAanZ2Bdc8icd4zt7QCGmy3hYbZZMw6UhSlgKUT24m6hw+W167knbjG/U64x1Qzik0DCx0fqY26jdJVZN6AnOpAFoIJ6RNIS1X/fbPa5kzUeTNSbgCW64LS0uOtXBh9uICxaxhHthgKtcOXFn7UrLWVnMfmd0fVi3nV3Tu5j8swrXlDVX+vaTJvDawIDAQABo1AwTjAdBgNVHQ4EFgQUv7Ta12WZE11NQhjpFOWxHG+wcZUwHwYDVR0jBBgwFoAUv7Ta12WZE11NQhjpFOWxHG+wcZUwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAsixLWA/28uVQMPfsWAv2cfdJMMCCvI6Y7uXsLUrNiONG6Ay/pU+6Qc/2kmZJlo0bHLx7P8ncKNHyTPzO1IzMDvvu65TUFdNAnKkqK90IRfjwY+RJv2J2NEKXmWRCxaJG522Uc0aGaYo6BPfrrYInr1fTh6i+/ovF4smyCo2bMX1dI5i7TlhAD9qsOA36XVYg+w5w7jXoJTYh14oKsE23pXpTiuSah6KdnvrjilwuBpV/SG40TsnITwyeaQFWMwbzJkhbvMgRK2crRruUhcIumIE5TTfLVgKzJ5oisjgjrZ3oFw871L2HTPFI3C6Xpp6yV6LNpFGOizT8HTLp63CwQw==-----END CERTIFICATE-----# vim ca-cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: ca namespace: defaultdata: ca.pem: | -----BEGIN CERTIFICATE----- MIIC9zCCAd+gAwIBAgIJAJPzlo1fzzxqMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV BAMMB2t1YmUtY2EwHhcNMTkwMTA0MDYyNzM5WhcNMjEwOTMwMDYyNzM5WjASMRAw DgYDVQQDDAdrdWJlLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA xnFM6sykmKdmr1Z4DADujeIUZXvz5ajy+xdjpfjrNhnr3002GsthAlN6lWjJP6dn uVN20M1W/oozF7OzMuSRNO6zLepAG2TrRl/1DZG9EFSN7m65HtxK0DA3RyZc4CuD j3ADT899yziyaVTvUjR8MxLWHeibgAanZ2Bdc8icd4zt7QCGmy3hYbZZMw6UhSlg KUT24m6hw+W167knbjG/U64x1Qzik0DCx0fqY26jdJVZN6AnOpAFoIJ6RNIS1X/f bPa5kzUeTNSbgCW64LS0uOtXBh9uICxaxhHthgKtcOXFn7UrLWVnMfmd0fVi3nV3 Tu5j8swrXlDVX+vaTJvDawIDAQABo1AwTjAdBgNVHQ4EFgQUv7Ta12WZE11NQhjp FOWxHG+wcZUwHwYDVR0jBBgwFoAUv7Ta12WZE11NQhjpFOWxHG+wcZUwDAYDVR0T BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAsixLWA/28uVQMPfsWAv2cfdJMMCC vI6Y7uXsLUrNiONG6Ay/pU+6Qc/2kmZJlo0bHLx7P8ncKNHyTPzO1IzMDvvu65TU FdNAnKkqK90IRfjwY+RJv2J2NEKXmWRCxaJG522Uc0aGaYo6BPfrrYInr1fTh6i+ /ovF4smyCo2bMX1dI5i7TlhAD9qsOA36XVYg+w5w7jXoJTYh14oKsE23pXpTiuSa h6KdnvrjilwuBpV/SG40TsnITwyeaQFWMwbzJkhbvMgRK2crRruUhcIumIE5TTfL VgKzJ5oisjgjrZ3oFw871L2HTPFI3C6Xpp6yV6LNpFGOizT8HTLp63CwQw== -----END CERTIFICATE-----# kubectl create -f ca-cm.yaml configmap/ca created# kubectl get configmap caNAME DATA AGEca 1 27d 配置 Login App Configmap123456789101112131415161718192021222324252627# vim loginapp-cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: loginapp namespace: defaultdata: config.yaml: | debug: false client_id: "login" client_secret: 4TORGiNV9M54BTk1v7dNuFSaI6hUjfjr issuer_url: "https://192.168.100.185:32000" issuer_root_ca: "/etc/ssl/ca.pem" redirect_url: "https://192.168.100.185:32002/callback" tls_enabled: true tls_cert: "/etc/loginapp/tls/tls.crt" tls_key: "/etc/loginapp/tls/tls.key" listen: "https://0.0.0.0:5555" disable_choices: false extra_scopes: "groups" name: "Kubernetes Auth"# kubectl create -f loginapp-cm.yaml configmap/loginapp created# kubectl get configmap loginappNAME DATA AGEloginapp 1 24d 部署 Login App1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# vim loginapp-deploy.yml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: loginapp namespace: defaultspec: replicas: 3 template: metadata: labels: app: loginapp spec: containers: - image: 192.168.100.100/library/login-app:latest name: loginapp ports: - name: https containerPort: 5555 volumeMounts: - name: ca mountPath: /etc/ssl/ - name: config mountPath: /app/ - name: tls mountPath: /etc/loginapp/tls volumes: - name: ca configMap: name: ca items: - key: ca.pem path: ca.pem - name: config configMap: name: loginapp items: - key: config.yaml path: config.yaml - name: tls secret: secretName: loginapp---apiVersion: v1kind: Servicemetadata: name: loginapp namespace: defaultspec: type: NodePort ports: - name: loginapp port: 5555 protocol: TCP targetPort: 5555 nodePort: 32002 selector: app: loginapp# kubectl create -f loginapp-deploy.yml# kubectl get pod --show-labels -l app=loginapp# kubectl get service loginapp 配置 kubernetes 配置K8s Apiserver以使用OpenID Connect 身份验证插件，详见:https://github.com/dexidp/dex/blob/master/Documentation/kubernetes.md 配置 kube-apiserver12345--oidc-issuer-url=https://192.168.100.185:32000--oidc-client-id=loginapp--oidc-ca-file=/etc/kubernetes/ssl/dex-ca.pem--oidc-username-claim=email--oidc-groups-claim=groups 配置 RBAC赋予 k8s 组 cluster-admin 角色123456789101112131415# vim k8s.yml apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: ldap-cluster-admin namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: Group name: k8s# kubectl create -f k8s.yml clusterrolebinding.rbac.authorization.k8s.io/ldap-cluster-admin created 赋予 test 组相应权限123456789101112131415161718192021222324252627# vim test.yaml kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: ldap-test namespace: testrules:- apiGroups: [""] resources: ["pods"] verbs: ["get", "watch", "list"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: ldap-test namespace: testsubjects:- kind: Group name: test apiGroup: ""roleRef: kind: Role name: ldap-test apiGroup: ""# kubectl create -f test.yaml role.rbac.authorization.k8s.io/ldap-test createdrolebinding.rbac.authorization.k8s.io/ldap-test created 登录 Dex 获取 ID Tokens 浏览器输入 https://192.168.100.185:32002/ (loginapp的nodeport)进行登录 Authentication for clients : login(login 对应之前 dex 和 Login App Configmap 的配置) 点击 “Request Token” 进行登录 之后，会跳转至 Dex 的网址 https://192.168.100.185:32000 点击 “Log in with LDAP” 输入 Username 和 Password ，点击 “Login” 登录生成 id-token 根据提示将文件复制至~/.kube/config ID Tokens 简单解释 id-token 实际上有三个部分，每个部分都是Base64编码的JSON，以”.”来分割。第一部分提供令牌的元数据。第二部分提供身份信息，这称为有效负载。第三部分是签名，用于验证令牌是否由可信方发出。 安装包以使用 jq 命令1# yum -y install jq 解码第一部分12345# echo eyJhbGciOiJSUzI1NiIsImtpZCI6Ijg1MzQ0ZTZlYjk4N2Y5ODA2MjRhODY2MTM2ZWFmOTFmNjFkNDNlYWEifQ | base64 -d | jq&#123; "alg": "RS256", "kid": "85344e6eb987f980624a866136eaf91f61d43eaa"&#125; 解码第二部分12345678910111213141516# echo eyJpc3MiOiJodHRwczovLzE5Mi4xNjguMTAwLjE4NTozMjAwMCIsInN1YiI6IkNnUjNZVzVuRWdSc1pHRnciLCJhdWQiOiJsb2dpbiIsImV4cCI6MTU0Nzc5MTAzMCwiaWF0IjoxNTQ3NzA0NjMwLCJhenAiOiJsb2dpbiIsImF0X2hhc2giOiJvTzBLcTBTYy1qdE9ybVpUbTJpbG5RIiwiZW1haWwiOiJ3YW5nemhpamlhbnNkQHFxLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJncm91cHMiOlsiazhzIl0sIm5hbWUiOiJ3YW5nIn0 | base64 -d | jq&#123; "iss": "https://192.168.100.185:32000", "sub": "CgR3YW5nEgRsZGFw", "aud": "login", "exp": 1547791030, "iat": 1547704630, "azp": "login", "at_hash": "oO0Kq0Sc-jtOrmZTm2ilnQ", "email": "wangzhijiansd@qq.com", "email_verified": true, "groups": [ "k8s" ], "name": "wang"&#125; 检查用户权限 根据之前配置的 RBAC 来测试用户拥有的权限 测试用户 wang12345# kubectl get nodes --user=wangNAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 174d v1.13.0node02 Ready &lt;none&gt; 174d v1.13.0node03 Ready &lt;none&gt; 174d v1.13.0 测试用户 zhi123456# kubectl get nodes --user=zhiError from server (Forbidden): nodes is forbidden: User &quot;zhijiansd@163.com&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope# kubectl get pod -n test --user=zhiNAME READY STATUS RESTARTS AGEtest-nginx-75677f8b58-p8w6d 1/1 Running 0 35h 最后要感谢如下文章及其作者: https://kairen.github.io/2018/04/15/kubernetes/k8s-integration-ldap/ https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/ https://github.com/ObjectifLibre/k8s-ldap https://thenewstack.io/author/joel-speed/]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>dex</tag>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署openLDAP]]></title>
    <url>%2Fblog%2Fopenldap%2F</url>
    <content type="text"><![CDATA[LDAP 代表 轻量级目录访问协议。顾名思义，它是一种用于访问目录服务的基于X.500协议的轻量级目录服务。 LDAP信息模型基于条目。条目是具有全局唯一性的属性集合专有名称（DN）。DN用于明确指代条目。每个条目的属性都有一个类型和一个或多个值。这些类型通常是助记符字符串，例如“ cn ”表示公用名，或“ mail ”表示电子邮件地址。值的语法取决于属性类型。 在LDAP中，目录条目以分层树状结构排列。此外，LDAP允许您通过使用名为objectClass的特殊属性来控制条目中所需和允许的属性。objectClass属性的值确定条目必须遵守的模式规则。 更多简介请访问:http://www.openldap.org/doc/admin24/intro.html 部署 openLDAP安装 openLDAP12# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo# yum -y install openldap-servers openldap-clients openldap-devel OpenLDAP 2.3及更高版本已转换为使用动态运行时配置引擎slapd-config:http://www.openldap.org/doc/admin24/slapdconf2.html 生成LDAP密码12# slappasswd -s zhijian&#123;SSHA&#125;pQdy+1y8IfIw9ZgIExIdOsjC/tqsmb86 复制相关文件12# cp /usr/share/openldap-servers/slapd.ldif /etc/openldap/# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG 配置 slapd.ldif12345678910111213141516171819202122232425# vim /etc/openldap/slapd.ldif ## Server status monitoring#dn: olcDatabase=monitor,cn=configobjectClass: olcDatabaseConfigolcDatabase: monitorolcAccess: to * by dn.base="gidNumber=0+uidNumber=0,cn=peercred,cn=external,c n=auth" read by dn.base="cn=admin,dc=openldap,dc=flywzj,dc=com" read by * none## Backend database definitions#dn: olcDatabase=hdb,cn=configobjectClass: olcDatabaseConfigobjectClass: olcHdbConfigolcDatabase: hdbolcSuffix: dc=openldap,dc=flywzj,dc=comolcRootDN: cn=admin,dc=openldap,dc=flywzj,dc=comolcRootPW: &#123;SSHA&#125;OLvPaV6PzzgRSCDivSzY8xVwk4fEWfZ9olcDbDirectory: /var/lib/ldapolcDbIndex: objectClass eq,presolcDbIndex: ou,cn,mail,surname,givenname eq,pres,sub 测试配置12# slaptest -u config file testing succeeded 删除原始配置文件并重置配置123# rm -rf /etc/openldap/slapd.d/*# slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif# chown -R ldap:ldap /etc/openldap/slapd.d/ 启用 openLDAP12# systemctl start slapd# systemctl status slapd 检查服务器是否正在运行并正确配置12345678910111213141516171819# ldapsearch -x -b '' -s base '(objectclass=*)' namingContexts# extended LDIF## LDAPv3# base &lt;&gt; with scope baseObject# filter: (objectclass=*)# requesting: namingContexts ##dn:namingContexts: dc=openldap,dc=flywzj,dc=com# search resultsearch: 2result: 0 Success# numResponses: 2# numEntries: 1 部署 phpldapadmin安装 phpldapadmin1# yum -y install httpd php-ldap phpldapadmin 配置访问控制123456# vim /etc/httpd/conf.d/phpldapadmin.conf &lt;IfModule mod_authz_core.c&gt; # Apache 2.4 Require local Require ip 192.168.100 &lt;/IfModule&gt; 注:如上为允许本地和192.168.100.0网段访问phpldapadmin，其他Apache Require访问控制指令请自行搜索。 更改登录方式1234# vim /etc/phpldapadmin/config.php// line 397$servers-&gt;setValue('login','attr','dn');// $servers-&gt;setValue('login','attr','uid'); 导入默认 schema 模块12345678910111213141516171819202122232425262728# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=cosine,cn=schema,cn=config"# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=nis,cn=schema,cn=config"# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=inetorgperson,cn=schema,cn=config"# ldapsearch -LLLQY EXTERNAL -H ldapi:/// -b cn=schema,cn=config "(objectClass=olcSchemaConfig)" dndn: cn=schema,cn=configdn: cn=&#123;0&#125;core,cn=schema,cn=configdn: cn=&#123;1&#125;cosine,cn=schema,cn=configdn: cn=&#123;2&#125;nis,cn=schema,cn=configdn: cn=&#123;3&#125;inetorgperson,cn=schema,cn=config 注1:这里的模版对应在 phpldapadmin 里 “创建一个子条目” 下的 “Select a template for the creation process” 模版。 注2: 如出现 “Automatically removed objectClass from templateCourier Mail: Account: courierMailAccount removed from template as it is not defined in the schema” 问题，那既是没有导入相关 objectClass 模版的原因。我曾试着将余下的 schema 也导入进去，但是没有成功，余下的几个模版依然是disable的。 配置 DN12345678910# vim /etc/openldap/base.ldif dn: dc=openldap,dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: flywzj # ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/base.ldif Enter LDAP Password: adding new entry "dc=openldap,dc=flywzj,dc=com" 注:如未配置默认DC即登录 phpldapadmin 会提示 “This base cannot be created with PLA.”问题。 启用 phpldapadmin1# systemctl start httpd 浏览器使用 http://yourip/ldapadmin 进入 phpldapadmin,点击登录，在“登录DN”中输入”cn=admin,dc=openldap,dc=flywzj,dc=com”并输入相应密码即可登录创建条目。 使用命令创建 OU1234567891011121314151617181920212223242526272829303132# vim /etc/openldap/group.ldif dn: ou=Groups,dc=openldap,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: topdn: ou=People,dc=openldap,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: topdn: cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topdn: cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: top# ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/group.ldif Enter LDAP Password: adding new entry "ou=Groups,dc=openldap,dc=flywzj,dc=com"adding new entry "ou=People,dc=openldap,dc=flywzj,dc=com"adding new entry "cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=com"adding new entry "cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=com" 注:可使用 phpldapadmin 来创建。 创建用户12345678910111213141516171819202122232425262728293031323334353637383940# vim /etc/openldap/user.ldif dn: uid=wang,ou=People,dc=openldap,dc=flywzj,dc=comcn: wanggidnumber: 500givenname: wanghomedirectory: /home/users/wangloginshell: /bin/shmail: wangzhijiansd@qq.comobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: wanguid: wanguidnumber: 1000userpassword: wangzhijiandn: uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=comcn: zhigidnumber: 501givenname: zhihomedirectory: /home/users/zhiloginshell: /bin/shmail: wangzhijiansd@qq.comobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: zhiuid: zhiuidnumber: 1001userpassword: zhijian# ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/user.ldif Enter LDAP Password: adding new entry "uid=wang,ou=People,dc=openldap,dc=flywzj,dc=com"adding new entry "uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=com" 查看具体用户123456789101112131415# ldapsearch -x -W -D "uid=wang,ou=People,dc=openldap,dc=flywzj,dc=com" -b "uid=wang,dc=openldap,ou=People,dc=flywzj,dc=com"Enter LDAP Password: # extended LDIF## LDAPv3# base &lt;uid=wang,dc=openldap,ou=People,dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## search resultsearch: 2result: 32 No such object# numResponses: 1 查看该CN下的所有条目1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# ldapsearch -x -H ldap:/// -b dc=openldap,dc=flywzj,dc=com -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -WEnter LDAP Password: # extended LDIF## LDAPv3# base &lt;dc=openldap,dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## openldap.flywzj.comdn: dc=openldap,dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: flywzjdc: openldap# Groups, openldap.flywzj.comdn: ou=Groups,dc=openldap,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, openldap.flywzj.comdn: ou=People,dc=openldap,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, openldap.flywzj.comdn: cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: top# test, Groups, openldap.flywzj.comdn: cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: top# wang, People, openldap.flywzj.comdn: uid=wang,ou=People,dc=openldap,dc=flywzj,dc=comcn: wanggidNumber: 500givenName: wanghomeDirectory: /home/users/wangloginShell: /bin/shmail: wangzhijiansd@qq.comobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: wanguid: wanguidNumber: 1000userPassword:: d2FuZ3poaWppYW4=# zhi, People, openldap.flywzj.comdn: uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=comcn: zhigidNumber: 501givenName: zhihomeDirectory: /home/users/zhiloginShell: /bin/shmail: wangzhijiansd@qq.comobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: zhiuid: zhiuidNumber: 1001userPassword:: emhpamlhbg==# search resultsearch: 2result: 0 Success# numResponses: 8# numEntries: 7 注: 亦可使用 phpldapadmin 创建。 配置 zabbix 使用 LDAP 这里以配置 zabbix 使用 LDAP 来进行测试。 配置 zabbix 的 “管理” ===&gt; “认证” 下的 “LDAP settings”项: Enable LDAP authentication: √ LDAP主机: yourip 端口: 389(默认端口,如使用使用 ldaps 协议则为636) 基于 DN: dc=openldap,dc=flywzj,dc=com 搜索属性: uid 绑定 DN: cn=admin,dc=openldap,dc=flywzj,dc=com 绑定密码: 输入该DN的密码 测试认证[必需为一个正确的LDAP用户]-登录: 如如上配置的wang 测试认证[必需为一个正确的LDAP用户]-用户密码:如如上配置的wangzhijian 点击 “测试”，成功则会提示“LDAP登录成功”。 注: 如要使用 LDAP 登录 zabbix，还需在 zabbix 的 “用户群组” 下创建同名的用户并赋予其权限，之后配置”更新”默认认证方式为“LDAP”即可使用LDAP下的该用户进行登录。 使用 kubernetes 部署 openLDAP 详询: docker-openldap，在部署时请查看对比构建方的环境变量。如需进行定制构建，构建方也给了构建方法。 配置 PVC123456789101112131415161718192021222324252627282930313233# vim openldap-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-data namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi"---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-config namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "1Gi"# kubectl create -f openldap-pvc.yaml persistentvolumeclaim/ldap-data createdpersistentvolumeclaim/ldap-config created 配置openLDAP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# vim openldap.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ldap labels: app: ldapspec: replicas: 1 template: metadata: labels: app: ldap spec: containers: - name: ldap image: 192.168.100.100/library/openldap:1.2.2 volumeMounts: - name: ldap-data mountPath: /var/lib/ldap - name: ldap-config mountPath: /etc/ldap/slapd.d - name: ldap-certs mountPath: /container/service/slapd/assets/certs ports: - containerPort: 389 name: openldap env: - name: LDAP_LOG_LEVEL value: "256" - name: LDAP_ORGANISATION value: "zhi" - name: LDAP_DOMAIN value: "flywzj.com" - name: LDAP_ADMIN_PASSWORD value: "admin" - name: LDAP_CONFIG_PASSWORD value: "config" - name: LDAP_READONLY_USER value: "false" - name: LDAP_READONLY_USER_USERNAME value: "readonly" - name: LDAP_READONLY_USER_PASSWORD value: "readonly" - name: LDAP_RFC2307BIS_SCHEMA value: "false" - name: LDAP_BACKEND value: "mdb" - name: LDAP_TLS value: "true" - name: LDAP_TLS_CRT_FILENAME value: "ldap.crt" - name: LDAP_TLS_KEY_FILENAME value: "ldap.key" - name: LDAP_TLS_CA_CRT_FILENAME value: "ca.crt" - name: LDAP_TLS_ENFORCE value: "false" - name: LDAP_TLS_CIPHER_SUITE value: "SECURE256:+SECURE128:-VERS-TLS-ALL:+VERS-TLS1.2:-RSA:-DHE-DSS:-CAMELLIA-128-CBC:-CAMELLIA-256-CBC" - name: LDAP_TLS_VERIFY_CLIENT value: "demand" - name: LDAP_REPLICATION value: "false" - name: LDAP_REPLICATION_CONFIG_SYNCPROV value: "binddn=\"cn=admin,cn=config\" bindmethod=simple credentials=$LDAP_CONFIG_PASSWORD searchbase=\"cn=config\" type=refreshAndPersist retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_DB_SYNCPROV value: "binddn=\"cn=admin,$LDAP_BASE_DN\" bindmethod=simple credentials=$LDAP_ADMIN_PASSWORD searchbase=\"$LDAP_BASE_DN\" type=refreshAndPersist interval=00:00:00:10 retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_HOSTS value: "#PYTHON2BASH:['ldap://ldap-one-service', 'ldap://ldap-two-service']" - name: KEEP_EXISTING_CONFIG value: "false" - name: LDAP_REMOVE_CONFIG_AFTER_SETUP value: "true" - name: LDAP_SSL_HELPER_PREFIX value: "ldap" volumes: - name: ldap-data persistentVolumeClaim: claimName: ldap-data #hostPath: # path: "/data/ldap/db" - name: ldap-config persistentVolumeClaim: claimName: ldap-config #hostPath: # path: "/data/ldap/config" - name: ldap-certs hostPath: path: "/data/ldap/certs"---apiVersion: v1kind: Servicemetadata: labels: app: ldap name: ldap-servicespec: type: NodePort ports: - port: 389 nodePort: 38989 selector: app: ldap# kubectl create -f openldap.yaml deployment.extensions/ldap createdservice/ldap-service created# kubectl get pods -l app=ldapNAME READY STATUS RESTARTS AGEldap-65f5786ff8-b9dnx 1/1 Running 1 11d 使用 kubernetes 部署 openldapadmin 详询: docker-phpLDAPadmin，部署时请详细查看对比构建方给定的环境变量。如需进行定制构建，构建方也给出了构建方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# vim phpldapadmin.yaml apiVersion: v1kind: ReplicationControllermetadata: name: phpldapadmin-controller labels: app: phpldapadminspec: replicas: 1 selector: app: phpldapadmin template: metadata: labels: app: phpldapadmin spec: containers: - name: phpldapadmin image: 192.168.100.100/library/phpldapadmin:0.7.2 volumeMounts: - name: phpldapadmin-certs mountPath: /container/service/phpldapadmin/assets/apache2/certs - name: ldap-client-certs mountPath: /container/service/ldap-client/assets/certs ports: - containerPort: 443 env: - name: PHPLDAPADMIN_LDAP_HOSTS #value: "#PYTHON2BASH:[&#123;'ldap-service': [&#123;'server': [&#123;'tls': 'true'&#125;]&#125;]&#125;]" value: "ldap-service" - name: PHPLDAPADMIN_SERVER_ADMIN value: "wangzhijiansd@qq.com" - name: PHPLDAPADMIN_SERVER_PATH value: "/phpldapadmin" - name: PHPLDAPADMIN_HTTPS value: "true" - name: PHPLDAPADMIN_HTTPS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_HTTPS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_HTTPS_CA_CRT_FILENAME value: "ca.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS value: "true" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_REQCERT value: "demand" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CA_CRT_FILENAME value: "ca.crt" volumes: - name: phpldapadmin-certs hostPath: path: "/data/phpldapadmin/ssl/" - name: ldap-client-certs hostPath: path: "/data/phpldapadmin/ldap-client-certs/"---apiVersion: v1kind: Servicemetadata: labels: app: phpldapadmin name: phpldapadmin-servicespec: type: NodePort ports: - port: 443 nodePort: 32001 selector: app: phpldapadmin# kubectl create -f phpldapadmin.yaml replicationcontroller/phpldapadmin-controller createdservice/phpldapadmin-service created# kubectl get pods -l app=phpldapadminNAME READY STATUS RESTARTS AGEphpldapadmin-controller-mprjw 1/1 Running 2 14d 输入 https://nodeip:nodeport 点击”login”进行登陆: Login DN: cn=admin,dc=flywzj,dc=com Password: admin 点击 Authenticate 登陆 不在演示如何在 kubernetes 的 openLDAP 上创建用户，既然安装了 phpldapadmin ,用web来创建比较简单(phpldapadmin界面粗糙)。]]></content>
      <categories>
        <category>openldap</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[切换 zabbix 认证模式]]></title>
    <url>%2Fblog%2Fb5448413%2F</url>
    <content type="text"><![CDATA[以 zabbix 配置了 openLDAP 认证，现在想切换为默认的认证方式为例。 登陆数据库查看当前认证方式1234567891011121314151617# mysql -u root -pMariaDB [(none)]&gt; use zabbix;Database changedMariaDB [zabbix]&gt; show tables like 'config';+---------------------------+| Tables_in_zabbix (config) |+---------------------------+| config |+---------------------------+1 row in set (0.01 sec)MariaDB [zabbix]&gt; select authentication_type from config ;+---------------------+| authentication_type |+---------------------+| 1 |+---------------------+1 row in set (0.00 sec) 0 代表Internal,1 代表LDAP，2 代表HTTP。 更改认证方式为默认认证方式123456MariaDB [zabbix]&gt; update config set authentication_type=0;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0MariaDB [zabbix]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 更新 Admin 密码(如需要)12345678910# 查询Admin用户的IDMariaDB [zabbix]&gt; select * from users;# 更新Admin密码MariaDB [zabbix]&gt; update users set passwd=md5("zabbix") where userid='1';Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0MariaDB [zabbix]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)]]></content>
      <categories>
        <category>openldap</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署基于 Web 的 Kubernetes 多集群管理平台 -- 360 Wayne]]></title>
    <url>%2Fblog%2F360wayne%2F</url>
    <content type="text"><![CDATA[Wayne 是一个通用的、基于 Web 的 Kubernetes 多集群管理平台。通过可视化 Kubernetes 对象模板编辑的方式，降低业务接入成本， 拥有完整的权限管理系统，适应多租户场景，是一款适合企业级集群使用的发布平台。 特性 基于 RBAC（Role based access control）的权限管理：用户通过角色与部门和项目关联，拥有部门角色允许操作部门资源，拥有项目角色允许操作项目资源，更加适合多租户场景。 简化 Kubernetes 对象创建：提供基础 Kubernetes 对象配置文件添加方式，同时支持高级模式直接编辑 Json/Yaml文件创建 Kubernetes 对象。 LDAP/OAuth 2.0/DB 多种登录模式支持：集成企业级 LDAP 登录及 DB 登录模式，同时还可以实现 OAuth2 登录。 支持多集群、多租户：可以同时管理多个 Kubernetes 集群，并针对性添加特定配置，更方便的多集群、多租户管理。 提供完整审计模块：每次操作都会有完整的审计功能，追踪用于操作历史，同时支持用户自定义 webhook。 提供基于 APIKey 的开放接口调用：用户可自主申请相关 APIKey 并管理自己的部门和项目，运维人员也可以申请全局 APIKey 进行特定资源的全局管理。 保留完整的发布历史：用户可以便捷的找到任何一次历史发布，并可轻松进行回滚，以及基于特定历史版本更新 Kubernetes 资源。 具备完善的资源报表：用户可以轻松获取各项目的资源使用占比和历史上线频次（天级）以及其他基础数据的报表和图表。 提供基于严密权限校验的 Web shell：用户可以通过 Web shell 的形式进入发布的 Pod 进行操作，自带完整的权限校验。 提供站内通知系统：方便管理员推送集群、业务通知和故障处理报告等。 架构 部署 Wayne下载相关文件并部署 Wayne 依赖 MySQL 和 RabbitMQ，其中 MySQL 是必须的服务，用户存储系统的各种数据，RabbitMQ 是可选的，主要用户扩展审计功能使用。 这里使用了 ceph 进行数据持久化 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# git clone https://github.com/Qihoo360/wayne.git# cd wayne/hack/kubernetes/# vim dependency/mysql-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-wayne-pvc namespace: default labels: app: mysql-waynespec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "10Gi"# vim dependency/rabbitmq-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: rabbitmq-wayne-pvc namespace: default labels: app: rabbitmq-waynespec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi" # kubectl apply -f dependency/persistentvolumeclaim/mysql-wayne-pvc createddeployment.extensions/mysql-wayne createdservice/mysql-wayne createdpersistentvolumeclaim/rabbitmq-wayne-pvc createddeployment.extensions/rabbitmq-wayne createdservice/rabbitmq-wayne created# kubectl get podNAME READY STATUS RESTARTS AGEmysql-wayne-75947575d-mc972 1/1 Running 0 107srabbitmq-wayne-7c6dd8f475-l4pqj 1/1 Running 0 106s# kubectl apply -f wayne/configmap/infra-wayne createddeployment.extensions/infra-wayne createddeployment.extensions/infra-wayne-woker createddeployment.extensions/infra-wayne-webhook createdservice/infra-wayne created# kubectl get podNAME READY STATUS RESTARTS AGEinfra-wayne-5d84cf49b4-lggzs 1/1 Running 0 7m44sinfra-wayne-webhook-85dcf87c48-w4tcj 1/1 Running 0 7m44sinfra-wayne-woker-84bff6f8c9-mt7h5 1/1 Running 0 7m44s 现在可以通过 http://yourip:NodePort 访问 Wayne 平台，默认管理员账号 密码admin:admin。 注: 项目启动后还需要配置集群和 Namespace 等信息才可正常使用。 配置集群和 Namespace配置集群 进入后台创建集群并将 .kube/config 复制并粘贴至该集群下 1# cat .kube/config 配置 namespace 在 wayne 后台创建命名空间(需在 kubernetes 集群中进行创建,然后与 wayne 进行绑定) 12# kubectl create namespace testnamespace/test created 查看资源状况 配置完成,在左侧边栏的 kubernetes 菜单栏可以查看当前集群的相关 node 信息、deployment 信息以及 PV 信息。 应用 Wayne创建项目 返回前台，切换至当前集群的选项卡”创建项目” 创建部署 进入该项目部署页“创建部署”进行部署，之后“创建部署模板”，之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 部署成功后，可以选择“重启”、“下线”: 点击上线机房，通过弹出的选择框可以“进入容器”、“查看日志”: 创建负载均衡 点击该项目下左侧边栏的“负载均衡”项，之后“创建负载均衡”，配置“名称”和“机房”并提交。之后“创建负载均衡模板”,之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 创建ingress 点击该项目下左侧边栏的“ingress”项，之后“创建ingress”，配置“名称”和“机房”并提交。之后“创建ingress模板”,之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 确认部署情况 注: 这里使用了 https 是因为我之前部署了 TLS 认证的 traefik。 更多详情请访问 Wayne 官方 wiki: wayne]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>wayne</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对etcd集群及kubernetes集群进行升级]]></title>
    <url>%2Fblog%2F819d304e%2F</url>
    <content type="text"><![CDATA[我的etcd集群和kubernetes集群都是二进制安装的，所以升级主要就是替换二进制文件。 这里我将原版本为3.3.8的etcd集群升级到3.3.10版本，将原版本为v1.11.1的kubernetes集群升级到v1.13.0版本，而我这里的kubernetes集群使用keepalived+haproxy做了双master的高可用、负载均衡，所以并无集群下线之忧。 升级 Etcd 集群升级检查查看集群健康状况12345678# ETCDCTL_API=3 # etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy 查看版本12# curl https://192.168.100.181:2379/version&#123;"etcdserver":"3.3.8","etcdcluster":"3.3.0"&#125; 使用快照备份 Etcd 集群 etcd leader拥有最新的应用程序数据，从leader获取快照etcd_server_is_leader 是1即为leader，否则为0。 123456789# curl -sL https://192.168.100.181:2379/metrics | grep etcd_server_is_leader# HELP etcd_server_is_leader Whether or not this member is a leader. 1 if is, 0 otherwise.# TYPE etcd_server_is_leader gaugeetcd_server_is_leader 0# curl -sL https://192.168.100.182:2379/metrics | grep etcd_server_is_leader# HELP etcd_server_is_leader Whether or not this member is a leader. 1 if is, 0 otherwise.# TYPE etcd_server_is_leader gaugeetcd_server_is_leader 1 当然，也可以使用该命令查看谁是leader 123456# etcdctl --endpoints=https://192.168.100.181:2379 member list3a406a85e3de7ef5: name=etcd-184 peerURLs=https://192.168.100.184:2380 clientURLs=https://192.168.100.184:2379 isLeader=false695714eeb38cebbe: name=etcd-181 peerURLs=https://192.168.100.181:2380 clientURLs=https://192.168.100.181:2379 isLeader=falseab8f0f710ce0bf85: name=etcd-183 peerURLs=https://192.168.100.183:2380 clientURLs=https://192.168.100.183:2379 isLeader=falsec5cb8024e23348b6: name=etcd-182 peerURLs=https://192.168.100.182:2380 clientURLs=https://192.168.100.182:2379 isLeader=trueceb2db537a9ec20d: name=etcd-185 peerURLs=https://192.168.100.185:2380 clientURLs=https://192.168.100.185:2379 isLeader=false 使用快照备份集群12345678# ETCDCTL_API=3 etcdctl --endpoints https://192.168.100.182:2379 snapshot save snapshotdbSnapshot saved at snapshotdb# ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb+----------+----------+------------+------------+| HASH | REVISION | TOTAL KEYS | TOTAL SIZE |+----------+----------+------------+------------+| c09e95e0 | 11794749 | 1226 | 19 MB |+----------+----------+------------+------------+ 下载并解压 Etcd1# tar -zxvf etcd-v3.3.10-linux-amd64.tar.gz 停止一个现有 Etcd 服务器1# systemctl stop etcd 替换 Etcd 二进制文件，使用相同配置重启 Etcd 服务器123456789101112# cp etcd-v3.3.10-linux-amd64/etcd /usr/bin/# cp etcd-v3.3.10-linux-amd64/etcdctl /usr/bin/# systemctl start etcd# systemctl status etcd# etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy 对其余成员重复如上步骤 在未升级的成员将记录以下警告，直到升级整个集群 123# systemctl status etcdthe local etcd version 3.3.8 is not up-to-datemember 695714eeb38cebbe has a higher version 3.3.10 查看集群成员健康状况和版本123456789101112131415161718# etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy# curl https://192.168.100.181:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.182:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.183:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.184:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.185:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125; 升级 Kubernetes 集群查看当前集群版本12345# kubectl get node NAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 131d v1.11.1node02 Ready &lt;none&gt; 131d v1.11.1node03 Ready &lt;none&gt; 131d v1.11.1 下载并解压文件12# tar -zxvf kubernetes-server-linux-amd64.tar.gz# cd kubernetes/server/bin 升级 Master 节点停止 Master 节点相关组件123# systemctl stop kube-apiserver# systemctl stop kube-controller-manager# systemctl stop kube-scheduler 替换 Master 节点二进制组件1# cp kube-apiserver kube-controller-manager kube-scheduler kubeadm /usr/bin/ 重新启用 Master 节点12345678# systemctl start kube-apiserver# systemctl status kube-apiserver# systemctl start kube-controller-manager# systemctl status kube-controller-manager# systemctl start kube-scheduler# systemctl status kube-scheduler 在其他 Master 节点重复如上步骤进行升级 升级 Node 节点标记节点为不可调度 设置为不可调度后，新的 pod 不会迁移或者部署在该节点 12345# kubectl cordon node01node/node01 cordoned# kubectl get node | grep node01node01 Ready,SchedulingDisabled &lt;none&gt; 131d v1.11.1 迁移该节点的 Pod 迁移时注意系统瓶颈，当其他节点的CPU、内存或者本地存储资源不足，kubernetes都不会调用pod，pod会处于pending状态，直到重新上线该节点(或者扩容节点资源)，pod才会重新上线。 1234567# kubectl drain --ignore-daemonsets --delete-local-data node01 kubectl drain node01 --ignore-daemonsets --delete-local-datanode/node01 already cordonedWARNING: Ignoring DaemonSet-managed pods: ......; Deleting pods with local storage: ......pod/my-nginx-7ff9b54467-vk572 evicted......node/node01 evicted 注:对于DaemonSet-managed pods需要使用参数–ignore-daemonsets;迁移使用本地存储的pods需要使用参数–delete-local-data(移动到其他节点将清空数据)。 查看节点上是否还存在 Pods(DaemonSet pods忽略)1# kubectl get pod -o wide --all-namespaces | grep node01 查看 Pods 是否已移动到其他节点1# kubectl get pod -o wide --all-namespaces 停用该节点 Kubelet 和 Kube-proxy12# systemctl stop kubelet# systemctl stop kube-proxy 复制并替换相应二进制文件12# scp root@master1:/root/kubernetes/server/bin/kubelet /usr/bin/# scp root@master1:/root/kubernetes/server/bin/kube-proxy /usr/bin/ 启用该 Node 节点1234# systemctl start kubelet# systemctl status kubelet# systemctl start kube-proxy# systemctl status kube-proxy 在 Master 节点上解锁(重新上线)该 Node 节点12345# kubectl uncordon node01node/node01 uncordoned# kubectl get node | grep node01node01 Ready &lt;none&gt; 131d v1.13.0 在其他 Node 节点重复如上步骤以升级 Node 节点 查看系统是否升级成功12345# kubectl get node NAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 131d v1.13.0node02 Ready &lt;none&gt; 131d v1.13.0node03 Ready &lt;none&gt; 131d v1.13.0]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Harbor从1.5.1升级和迁移到1.6.2]]></title>
    <url>%2Fblog%2Fharbor%2F</url>
    <content type="text"><![CDATA[这次升级还算比较顺利，以前我从1.2版本升级到1.5版本没有升级成功，镜像全洗白了，所以这次升级我及其谨慎，官方文档看了又看(主要是文档排版太糟糕了)，生怕又给洗白了，当然结果是好的，成功升级。 官方改了三次数据库，从最早使用的MySQL迁移到MariaDB，从1.6.0开始又迁移到了Postgresql 在1.5.1版中我并没有安装运行Notary和Clair这两个组件 升级到1.6.2版后我新部署了Notary，Clair和Helm Chart这3个组件 备份Harbor停止Harbor12# cd harbor# docker-compose down 备份Harbor的当前文件,以便在必要时回滚到当前版本12# cd ..# mv harbor harbor-backup 下载迁移工具12# docker pull goharbor/harbor-migrator:v1.6.0goharbor/harbor-migrator v1.6.0 22775c4e4066 2 months ago 803MB 备份数据1234567# mkdir backup# docker run -it --rm -e DB_USR=root -e DB_PWD=root123 -v /data/database:/var/lib/mysql -v /root/harbor-backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg -v /root/backup:/harbor-migration/backup goharbor/harbor-migrator:v1.6.0 backup......Backup performed.Success to backup harbor.cfg.# ls backupharbor.cfg registry.sql 命令参考: docker run -it –rm -e DB_USR=root -e DB_PWD={db_pwd} -v ${harbor_db_path}:/var/lib/mysql -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg -v ${backup_path}:/harbor-migration/backup goharbor/harbor-migrator:[tag] backup 升级数据库架构、harbor.cfg并迁移数据 注意：您必须在启动Harbor之前运行Notary和Clair的DB的迁移。注意：在v1.6.0中，您需要执行三个连续步骤才能完全迁移Harbor，Notary和Clair的DB。 1234567891011# docker run -it --rm -e DB_USR=root -e DB_PWD=root123 -v /data/database:/var/lib/mysql -v /root/backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:v1.6.0 upPlease backup before upgrade, Enter y to continue updating or n to abort: yTrying to start mysql server...Waiting for MySQL start.........server stoppedThe path of the migrated harbor.cfg is not set, the input file will be overwritten.input version: 1.5.0, migrator chain: ['1.6.0']migrating to version 1.6.0Written new values to /harbor-migration/harbor-cfg/harbor.cfg 命令参考: docker run -it –rm -e DB_USR=root -e DB_PWD={db_pwd} -v ${harbor_db_path}:/var/lib/mysql -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg -v ${backup_path}:/harbor-migration/backup goharbor/harbor-migrator:[tag] backup 将harbor.cfg迁移至新版本的安装目录123456# docker run -it --rm -v /root/backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:v1.6.0 --cfg up# grep ^[a-z] backup/harbor.cfg# tar -zxvf harbor-offline-installer-v1.6.2.tgz # cd harbor# mv harbor.cfg harbor.cfg.bak# cp /root/backup/harbor.cfg /root/harbor 命令参考: docker run -it –rm -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:[tag] –cfg up 安装Harbor载入镜像12# docker load -i harbor.v1.6.2.tar.gz# docker images|grep 1.6.2 安装Notary，Clair和Helm Chart服务1234567# ./install.sh --with-notary --with-clair --with-chartmuseum......✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at https://192.168.100.100. For more details, please visit https://github.com/goharbor/harbor . 在安装升级过程中我又重新使用docker-compose命令安装了一次，供参考123456789101112131415161718192021# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml down -v# vim harbor.cfg# ./prepare --with-notary --with-clair --with-chartmuseum......The configuration files are ready, please use docker-compose to start the service.# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml up -d# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml psName Command State Ports -----------------------------------------------------------------------------chartmuseum /docker-entrypoint.sh Up (healthy) 9999/tcp clair /docker-entrypoint.sh Up (healthy) 6060/tcp, 6061/tcp harbor-adminserver /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-ui /harbor/start.sh Up (healthy) nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpnotary-server /bin/server-start.sh Up notary-signer /bin/signer-start.sh Up redis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp 如果要同时安装Notary，Clair和Helm Chart服务，则应在docker-compose和prepare命令中包含所有组件. 如上，harbor已经完成升级，可使用浏览器登陆harbor查看是否成功升级. Notary 使用 如果要启用内容信任以确保图像已签名，请在推送或拉取任何图像之前在命令行中设置两个环境变量： 12# export DOCKER_CONTENT_TRUST=1# export DOCKER_CONTENT_TRUST_SERVER=https://192.168.100.100:4443 这里以上传kubernetes-dashboard为例子说明notary的使用. 1234567891011121314151617181920212223# docker push 192.168.100.100/google_containers/kubernetes-dashboard-amd64:v1.10.0The push refers to repository [192.168.100.100/google_containers/kubernetes-dashboard-amd64]5f222ffea122: Pushed v1.10.0: digest: sha256:1d2e1229a918f4bc38b5a3f9f5f11302b3e71f8397b492afac7f273a0008776a size: 529Signing and pushing trust metadataYou are about to create a new root signing key passphrase. This passphrasewill be used to protect the most sensitive key in your signing system. Pleasechoose a long, complex passphrase and be careful to keep the password and thekey file itself secure and backed up. It is highly recommended that you use apassword manager to generate the passphrase and keep it safe. There will be noway to recover this key. You can find the key in your config directory.## 第一次push镜像，系统将要求您输入根密钥密码Enter passphrase for new root key with ID 7ffe68f: Repeat passphrase for new root key with ID 7ffe68f: ## 密码设置弱系统会进行提示Enter passphrase for new repository key with ID e8c208d: Passphrase is too short. Please use a password manager to generate and store a good random passphrase.Enter passphrase for new repository key with ID e8c208d: Repeat passphrase for new repository key with ID e8c208d: Finished initializing "192.168.100.100/google_containers/kubernetes-dashboard-amd64"Successfully signed 192.168.100.100/google_containers/kubernetes-dashboard-amd64:v1.10.0 注1: 根密钥生成于: /root/.docker/trust/private/镜像密码生成于: /root/.docker/trust/tuf/[registry name]/[imagepath]注2: 要使用notary，必须在Harbor中启用HTTPS.注3: 当镜像被签名时，它在UI中显示勾号; 否则，显示交叉符号（X）。注4:如果您省略标签，则跳过内容信任。提示”No tag specified, skipping trust metadata push”，所以即便是 latest 也需要提供镜像 tag 值。 通过Clair进行漏洞扫描Clair依靠漏洞元数据来完成分析过程。第一次初始安装后，Clair将自动开始从不同的漏洞存储库更新元数据数据库。更新过程可能需要一段时间，具体取决于数据大小和网络连接。如果数据库尚未完全填充，则存储库数据网格视图的页脚会显示警告消息。 数据库准备就绪后，整个数据库更新的时间戳将显示在“管理”下“ 配置”部分的“漏洞”选项卡中。这时候就可以进行漏洞扫描了。 注意：只有具有“项目管理员”角色的用户才有权启动分析过程。 分析过程可能显示如下状态： 未扫描：标签从未被扫描过。 排队：扫描任务已安排但尚未执行。 扫描：扫描过程正在进行中。 错误：扫描过程未能完成。 完成：扫描过程已成功完成。 关于漏洞的严重级别: 红色： 高安全漏洞的级别 橙色： 中等级别的漏洞 黄色： 漏洞程度低 灰色： 未知级别的漏洞 绿色： 没有漏洞 由于Harbor是由VMware中国的团队研发并开源的，对中文支持友好，对于使用问题无需过多担心。 附:有关Notary和Docker Content Trust的更多信息，请参阅Docker的文档：https://docs.docker.com/engine/security/trust/content_trust/关于Clair:https://github.com/coreos/clairHarbor用户指南: https://github.com/goharbor/harbor/blob/master/docs/user_guide.md]]></content>
      <categories>
        <category>镜像仓库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>harbor</tag>
        <tag>registry</tag>
        <tag>notary</tag>
        <tag>clair</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github构建博客]]></title>
    <url>%2Fblog%2Fhexo%2F</url>
    <content type="text"><![CDATA[在开始构建博客前，你需要在Github拥有一个账号,之后新建一个存储库(比如:zhijiansd.github.io),这里我就不再赘述了.接下来,我们需要为GitHub添加SSH key. 配置SSH key在本地创建秘钥,并将该秘钥复制下来12# ssh-keygen -t rsa -C "wangzhijiansd@qq.com"# cat /root/.ssh/id_rsa.pub 登录GitHub进行配置打开Github主页，依次点击Settings -&gt; SSH and GPG keys -&gt; New SSH key设置,自定义好Title,然后将上面复制的秘钥粘贴在Key下.进行测试1# ssh -T git@github.com 如果提示Are you sure you want to continue connecting (yes/no)?,输入yes.看到如下信息说明SSH已配置成功: Hi zhijiansd! You’’ve successfully authenticated, but GitHub does not provide shell access. 配置用户信息12# git config --global user.name "zhijiansd"# git config --global user.email "wangzhijiansd@qq.com" 安装Hexo并下载Next主题123456# yum -y install git nodejs # npm install hexo-cli -g# hexo init blog# cd blog# npm install# git clone https://github.com/theme-next/hexo-theme-next themes/next 更多主题详见Hexo. 配置Hexo更改默认主题为Next1# sed -i "s/landscape/next/g" _config.yml ###更改默认语言为汉语 12# grep language _config.yml language: zh-CN 配置Next更改Next主题外观123# grep scheme themes/next/_config.yml|grep Pisces scheme: Pisces# Only fit scheme Pisces 设置菜单1234567# vim themes/next/_config.ymlmenu: home: / || home //首页 #about: /about/ || user //关于 tags: /tags/ || tags //标签 categories: /categories/ || th //分类 archives: /archives/ || archive //归档 创建标签文件夹并添加type1234# hexo new page "tags"# vim source/tags/index.mdtype: "tags"comments: false 创建分类文件夹并添加type1234# hexo new page "categories"# vim source/categories/index.mdtype: "categories"comments: false 创建归档文件夹并添加type1234# hexo new page "archives"# vim source/archives/index.mdtype: archivescomments: false 设置头像123456# mkdir source/images# ls source/images/avatar.jpg# vim themes/next/_config.ymlavatar: url: /images/avatar.jpg 请自行将头像图片上传至source/images/文件夹下 修改文章内链接文本样式1234567891011# vim themes/next/source/css/_common/components/post/post.styl.post-body p a&#123; color: #0593d3; //原始链接颜色 border-bottom: none; border-bottom: 1px solid #0593d3; //底部分割线颜色 &amp;:hover &#123; color: #fc6423; //鼠标经过颜色 border-bottom: none; border-bottom: 1px solid #fc6423; //底部分割线颜色 &#125;&#125; 在文章末尾添加结束语 新建并配置passage-end-tag.swig文件 123456# vim themes/next/layout/_macro/passage-end-tag.swig&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt;-------------本文结束,感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 在post.swig文件的post-body之后，post-footer之前添加以下代码 123456# vim themes/next/layout/_macro/post.swig &lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125; &lt;/div&gt; 修改主题配置文件_config.yml，在末尾添加:如下: 123# vim themes/next/_config.ymlpassage_end_tag: enabled: true 实现文章统计功能 安装插件 1# npm install hexo-symbols-count-time --save 配置启用hexo配置文件的symbols项 1234567# vim _config.yml# Writingsymbols_count_time: symbols: true time: true total_symbols: true total_time: true 配置启用next主题配置文件的symbols项 1234567# vim themes/next/_config.ymlsymbols_count_time: separated_meta: true item_text_post: true item_text_total: true awl: 2 ##平均单词长度（单词的计数）。默认值:4。CN≈2 EN≈5 wpm: 300 ##每分钟的单词。默认值:275。缓慢≈200 正常≈275 快≈350 设置页面文章的篇数1234567891011# vim themes/next/_config.ymlindex_generator: per_page: 5archive_generator: per_page: 20 yearly: true monthly: truetag_generator: per_page: 10 启用访客量以及文章阅读量统计123456789101112131415161718192021# vim themes/next/_config.yml busuanzi_count: enable: true...... site_uv: true site_uv_header: 本站访客数 site_uv_footer: 人次 site_pv: true site_pv_header: 本站总访问量 site_pv_footer: 次 page_pv: true page_pv_header: 本文总阅读量 page_pv_footer: 次# vim themes/next/layout/_third-party/analytics/busuanzi-counter.swig ...... 本站总访问量&lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次...... 本站总访问量&lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次 给文章增加阴影效果12345678# vim themes/next/source/css/_custom/custom.styl.post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);&#125; 添加站内搜索12345# npm install hexo-generator-search --save# npm install hexo-generator-searchdb --save# vim themes/next/_config.yml local_search: enable: true 设置动态背景 配置启用next主题配置文件的canvas_nest项 123# vim themes/next/_config.yml canvas_nest: enable: true 在如下文件的行尾之前添加代码 1234# vim themes/next/layout/_layout.swig &#123;% if theme.canvas_nest %&#125; &lt;script type="text/javascript" color="255,0,255" opacity='0.7' zIndex="-2" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"&gt;&lt;/script&gt; &#123;% endif %&#125; 修改标签样式12# vim themes/next/layout/_macro/post.swig&lt;a href="&#123;&#123; url_for(tag.path) &#125;&#125;" rel="tag"&gt;&lt;i class="fa fa-tag"&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt; 在文件中搜索 rel=”tag”&gt;#,将 # 换成 ,不过这里的注释会直接显示改后的样式，上面就是更改后的样式，请参考. 在文章中插入图片123# npm install hexo-asset-image --save# vim _config.ymlpost_asset_folder: true 新建文章,/source/_posts文件夹内除了abcd.md文件还有一个同名的文件夹，在文章中按照默认格式即可在文章中插入图片(图片地址使用相对地址即可) 绘制流程图1# npm install --save hexo-filter-mermaid-diagrams 实验很多遍都没绘制出来,在这里只是想告诉大家 hexo 可以绘制流程图.详询:mermaid 修改永久链接的默认格式12345678# npm install hexo-abbrlink --save# vim _config.yml#permalink: :year/:month/:day/:title/#permalink_defaults:permalink: blog/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex 使用了该插件的话，同时使用了本地图片插件，注意图片路径的变化. 配置使用评论系统这里使用 valine 来部署，文档页详见:https://valine.js.org/quickstart.html 登录LeanCloud进行注册以获取APP ID 和 APP Key 进入控制台后点击 “应用” 下拉菜单 “创建新应用” 输入新应用名称,选择 “开发版” 并创建 应用创建好后，进入刚刚创建的应用 “设置” 页 进入 “设置” 项下的 “应用 Key” 项就能看到该应用的 APP ID 和 APP Key 最后配置启用 valine 并添加 APP ID 和 APP Key 即可启用评论系统 123456789101112# vim themes/next/_config.ymlvaline: enable: true appid: # your leancloud application appid appkey: # your leancloud application appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 欢迎评论 # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size visitor: false 分类页、标签页去评论: 在对应的md文件的顶部，加上 comments: false。如: 1234567# cat tags/index.md ---title: tagsdate: 2018-11-18 22:15:29type: "tags"comments: false--- 生成Hexo1# hexo g 开启预览访问端口123# hexo server -i 192.168.100.122INFO Start processingINFO Hexo is running at http://192.168.100.122:4000 . Press Ctrl+C to stop. 浏览器输入如上IP和端口即可在本地访问该博客 配置部署Hexo博客到GitHub 配置Hexo配置文件 12345# vim _config.ymldeploy: type: git repository: https://github.com/zhijiansd/zhijiansd.github.io branch: master 安装插件 1# npm install hexo-deployer-git --save 将Hexo部署到GitHub，之后浏览器输入zhijiansd.github.io查看 1# hexo d 绑定自己的域名 当部署后 hexo 会将相关内容进行复制并 push 到远程 master 分支的根目录下，这里我的是 source 文件夹 新建 CNAME 文件1234# cd source# touch CNAME# vim CNAME ###不要http以及www等前缀flywzj.com 添加两条类型为 “CNAME” 的记录，大致如下: 主机记录 记录类型 线路类型 记录值 @ CNAME 默认 zhijiansd.github.io. www CNAME 默认 zhijiansd.github.io. 注意: 我这里域名后的 “.” 是解析商自动加上去的 hexo的常用命令如下: 命令 解释 hexo init 初始化 hexo g 生成静态网页 hexo s 启动服务预览 hexo d 部署hexo hexo clean 清除缓存 hexo n 新建文章 hexo publish 草稿 注: NexT中文文档 Hexo中文文档]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
