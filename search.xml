<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL之联结和组合查询篇]]></title>
    <url>%2Fblog%2Fmysql4%2F</url>
    <content type="text"><![CDATA[本篇主要关于联结表、高级联结、组合查询以及全文本搜索。 联结表联结 联结是一种机制，用来在一条SELECT语句中关联表，因此称之为联结。使用特殊的语法，可以联结多个表返回一组输出，联结在运行时关联表中正确的行。 关系表的设计就是要保证把信息分解成多个表，一类数据一个表。各表通过某些常用的值[即关系设计中的关系(relational)]互相关联。 外键为某个表中的一列，它包含另一个表的主键值，定义了两个表之间的关系。 能够适应不断增加的工作量而不失败。设计良好的数据库或应用程序称之为可伸缩性好(scale well)。 创建联结 指定的两个列(prod_name和prod_price)在vendors表中，而另一个列(vend_name)在products表中。SELECT语句联结两个表，这两个表用WHERE子句正确联结， WHERE子句指示匹配vendors表中的vend_id和products表中的vend_id。使用完全限定列名在于两个表都有vend_id列。 1234567891011121314151617181920MariaDB [test]&gt; SELECT vend_name,prod_name,prod_price FROM vendors,products WHERE vendors.vend_id = products.vend_id ORDER BY vend_name,prod_name;+-------------+----------------+------------+| vend_name | prod_name | prod_price |+-------------+----------------+------------+| ACME | Bird seed | 10.00 || ACME | Carrots | 2.50 || ACME | Detonator | 13.00 || ACME | Safe | 50.00 || ACME | Sling | 4.49 || ACME | TNT (1 stick) | 2.50 || ACME | TNT (5 sticks) | 10.00 || Anvils R Us | .5 ton anvil | 5.99 || Anvils R Us | 1 ton anvil | 9.99 || Anvils R Us | 2 ton anvil | 14.99 || Jet Set | JetPack 1000 | 35.00 || Jet Set | JetPack 2000 | 55.00 || LT Supplies | Fuses | 3.42 || LT Supplies | Oil can | 8.99 |+-------------+----------------+------------+14 rows in set (0.02 sec) 完全限定列名 在引用的列可能出现二义性时，必须使用完全限定列名（用一个点分隔的表名和列名）。如果引用一个没有用表名限制的具有二义性的列名， MySQL将返回错误。 WHERE子句的重要性 WHERE子句作为过滤条件，它只包含那些匹配给定条件（这里是联结条件）的行。没有WHERE子句，第一个表中的每个行将与第二个表中的每个行配对，而不管它们逻辑上是否可以配在一起。 应该保证所有联结都有WHERE子句，否则将返回比想要的数据多得多的数据。同理，应该保证WHERE子句的正确性。不正确的过滤条件将导致返回不正确的数据。 笛卡儿积(cartesian product) 由没有联结条件的表关系返回的结果为笛卡儿积。检索出的行的数目将是第一个表中的行数乘以第二个表中的行数。 内部联结 也称为等值联结(equijoin)，它基于两个表之间的相等测试。 1234567891011121314151617181920MariaDB [test]&gt; SELECT vend_name,prod_name,prod_price FROM vendors INNER JOIN products ON vendors.vend_id = products.vend_id;+-------------+----------------+------------+| vend_name | prod_name | prod_price |+-------------+----------------+------------+| Anvils R Us | .5 ton anvil | 5.99 || Anvils R Us | 1 ton anvil | 9.99 || Anvils R Us | 2 ton anvil | 14.99 || LT Supplies | Fuses | 3.42 || LT Supplies | Oil can | 8.99 || ACME | Detonator | 13.00 || ACME | Bird seed | 10.00 || ACME | Carrots | 2.50 || ACME | Safe | 50.00 || ACME | Sling | 4.49 || ACME | TNT (1 stick) | 2.50 || ACME | TNT (5 sticks) | 10.00 || Jet Set | JetPack 1000 | 35.00 || Jet Set | JetPack 2000 | 55.00 |+-------------+----------------+------------+14 rows in set (0.00 sec) 该SELECT语句返回与前面例子完全相同的数据。这里，两个表之间的关系是FROM子句的组成部分，以INNER JOIN指定。在使用这种语法时，联结条件用特定的ON子句而不是WHERE子句给出。传递给ON的实际条件与传递给WHERE的相同 联结多个表 SQL对一条SELECT语句中可以联结的表的数目没有限制。创建联结的基本规则也相同。首先列出所有表，然后定义表之间的关系。 注意:联结的表越多，性能下降越厉害。 如下显示编号为20005的订单中的物品。订单物品存储在orderitems表中。每个产品按其产品ID存储，它引用products表中的产品。这些产品通过供应商ID联结到vendors表中。 12345678910MariaDB [test]&gt; SELECT vend_name,prod_name,prod_price,quantity FROM orderitems,products,vendors WHERE vendors.vend_id = products.vend_id AND orderitems.prod_id = products.prod_id AND order_num = 20005;+-------------+----------------+------------+----------+| vend_name | prod_name | prod_price | quantity |+-------------+----------------+------------+----------+| Anvils R Us | .5 ton anvil | 5.99 | 10 || Anvils R Us | 1 ton anvil | 9.99 | 3 || ACME | TNT (5 sticks) | 10.00 | 5 || ACME | Bird seed | 10.00 | 1 |+-------------+----------------+------------+----------+4 rows in set (0.00 sec) 该处为使用子查询返回关于FC的客户信息 1234567MariaDB [test]&gt; SELECT cust_name,cust_contact FROM customers WHERE cust_id IN (SELECT cust_id FROM orders WHERE order_num IN (SELECT order_num FROM orderitems WHERE prod_id = 'FC'));+-----------+--------------+| cust_name | cust_contact |+-----------+--------------+| E Fudd | E Fudd |+-----------+--------------+1 row in set (0.00 sec) 使用联结返回关于FC的客户信息。这里使用了两个联结，同时有3个WHERE子句条件。前两个关联联结中的表，后一个过滤产品FC的数据。 1234567MariaDB [test]&gt; SELECT cust_name,cust_contact FROM customers,orders,orderitems WHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num AND prod_id = 'FC';+-----------+--------------+| cust_name | cust_contact |+-----------+--------------+| E Fudd | E Fudd |+-----------+--------------+1 row in set (0.00 sec) 创建高级联结使用表别名 别名除了用于列名和计算字段外，SQL还允许给表名起别名。这样做能缩短SQL语句；同时还允许在单条SELECT语句中多次使用相同的表。 使用表别名应该注意，表别名只在查询执行中使用。与列别名不一样，表别名不返回到客户机。 1234567MariaDB [test]&gt; SELECT cust_name,cust_contact FROM customers AS c,orders AS o,orderitems AS oi WHERE c.cust_id = o.cust_id AND oi.order_num =o.order_num AND prod_id = 'FC';+-----------+--------------+| cust_name | cust_contact |+-----------+--------------+| E Fudd | E Fudd |+-----------+--------------+1 row in set (0.01 sec) 使用不同类型的联结自联结 自联结通常作为外部语句用来替代从相同表中检索数据时使用的子查询语句。 假如某物品（其ID为DTNTR）存在问题，如果要知道生产该物品的供应商生产的其他物品是否也存在这些问题。此时此查询要求首先找到生产ID为DTNTR的物品的供应商，然后找出这个供应商生产的其他物品。 方法1 : 使用子查询 12345678910111213MariaDB [test]&gt; SELECT prod_id,prod_name FROM products WHERE vend_id = (SELECT vend_id FROM products WHERE prod_id = 'DTNTR') ;+---------+----------------+| prod_id | prod_name |+---------+----------------+| DTNTR | Detonator || FB | Bird seed || FC | Carrots || SAFE | Safe || SLING | Sling || TNT1 | TNT (1 stick) || TNT2 | TNT (5 sticks) |+---------+----------------+7 rows in set (0.00 sec) 注 : 内部的SELECT语句返回生产ID为DTNTR的物品供应商的vend_id。该ID用于外部查询的WHERE子句中，以便检索出这个供应商生产的所有物品。 方法2 : 使用联结 12345678910111213MariaDB [test]&gt; SELECT p1.prod_id,p1.prod_name FROM products AS p1,products AS p2 WHERE p1.vend_id = p2.vend_id AND p2.prod_id = 'DTNTR';+---------+----------------+| prod_id | prod_name |+---------+----------------+| DTNTR | Detonator || FB | Bird seed || FC | Carrots || SAFE | Safe || SLING | Sling || TNT1 | TNT (1 stick) || TNT2 | TNT (5 sticks) |+---------+----------------+7 rows in set (0.00 sec) 注 : 此查询中需要的两个表实际上是相同的表，因此products表在FROM子句中出现了两次。虽然这是完全合法的，但对products的引用具有二义性，因为SQL不知道你引用的是products表中的哪个实例。为解决此问题，使用了表别名。 products的第一次出现为别名p1，第二次出现为别名p2。现在可以将这些别名用作表名。 WHERE（通过匹配p1中的vend_id和p2中的vend_id）首先联结两个表，然后按第二个表中的prod_id过滤数据，返回所需的数据。 自然联结 无论何时对表进行联结，应该至少有一个列出现在不止一个表中（被联结的列）。标准的联结返回所有数据，甚至相同的列多次出现。自然联结排除多次出现，使每个列只返回一次。这一般是通过对表使用通配符(SELECT *)，对所有其他表的列使用明确的子集来完成的。 1234567MariaDB [test]&gt; SELECT c.*,o.order_num,o.order_date,oi.prod_id,oi.quantity,oi.item_price FROM customers AS c,orders AS o,orderitems AS oi WHERE c.cust_id = o.cust_id AND oi.order_num = o.order_num AND prod_id = 'FC'; +---------+-----------+------------------+-----------+------------+----------+--------------+--------------+------------+-----------+---------------------+---------+----------+------------+| cust_id | cust_name | cust_address | cust_city | cust_state | cust_zip | cust_country | cust_contact | cust_email | order_num | order_date | prod_id | quantity | item_price |+---------+-----------+------------------+-----------+------------+----------+--------------+--------------+------------+-----------+---------------------+---------+----------+------------+| 10005 | E Fudd | 4545 53rd Street | Chicago | IL | 54545 | USA | E Fudd | NULL | 20008 | 2005-10-03 00:00:00 | FC | 50 | 2.50 |+---------+-----------+------------------+-----------+------------+----------+--------------+--------------+------------+-----------+---------------------+---------+----------+------------+1 row in set (0.01 sec) 这里通配符只对第一个表使用。所有其他列明确列出，所以没有重复的列被检索出来。 外部联结 许多联结将一个表中的行与另一个表中的行相关联。但有时候会需要包含没有关联行的那些行。联结包含了那些在相关表中没有关联行的行，这种类型的联结称为外部联结。 检索所有客户及其订单 1234567891011MariaDB [test]&gt; SELECT customers.cust_id,orders.order_num FROM customers INNER JOIN orders ON customers.cust_id = orders.cust_id;+---------+-----------+| cust_id | order_num |+---------+-----------+| 10001 | 20005 || 10001 | 20009 || 10003 | 20006 || 10004 | 20007 || 10005 | 20008 |+---------+-----------+5 rows in set (0.00 sec) 检索所有客户，包括那些没有订单的客户。使用LEFT OUTER JOIN从FROM子句的左边表(customers表)中选择所有行 123456789101112MariaDB [test]&gt; SELECT customers.cust_id,orders.order_num FROM customers LEFT OUTER JOIN orders ON customers.cust_id = orders.cust_id;+---------+-----------+| cust_id | order_num |+---------+-----------+| 10001 | 20005 || 10001 | 20009 || 10002 | NULL || 10003 | 20006 || 10004 | 20007 || 10005 | 20008 |+---------+-----------+6 rows in set (0.00 sec) 用关键字OUTER JOIN来指定联结的类型。在使用OUTER JOIN语法时，必须使用RIGHT或LEFT关键字指定包括其所有行的表（ RIGHT指出的是OUTER JOIN右边的表，而LEFT指出的是OUTER JOIN左边的表）。 两种基本的外部联结形式:左外部联结和右外部联结。它们之间的唯一差别是所关联的表的顺序不同。换句话说，左外部联结可通过颠倒FROM或WHERE子句中表的顺序转换为右外部联结。 使用带聚集函数的联结 检索所有客户及每个客户所下的订单数 12345678910MariaDB [test]&gt; SELECT customers.cust_name,customers.cust_id,COUNT(orders.order_num) AS num_ord FROM customers INNER JOIN orders ON customers.cust_id = orders.cust_id GROUP BY customers.cust_id;+----------------+---------+---------+| cust_name | cust_id | num_ord |+----------------+---------+---------+| Coyote Inc. | 10001 | 2 || Wascals | 10003 | 1 || Yosemite Place | 10004 | 1 || E Fudd | 10005 | 1 |+----------------+---------+---------+4 rows in set (0.00 sec) 注 : 该SELECT语句使用INNER JOIN将customers和orders表互相关联。GROUP BY子句按客户分组数据，函数调用COUNT(orders.order_num)对每个客户的订单计数，将它作为num_ord返回。 检索包含没有下任何订单的客户 1234567891011MariaDB [test]&gt; SELECT customers.cust_name,customers.cust_id,COUNT(orders.order_num) AS num_ord FROM customers LEFT OUTER JOIN orders ON customers.cust_id = orders.cust_id GROUP BY customers.cust_id;+----------------+---------+---------+| cust_name | cust_id | num_ord |+----------------+---------+---------+| Coyote Inc. | 10001 | 2 || Mouse House | 10002 | 0 || Wascals | 10003 | 1 || Yosemite Place | 10004 | 1 || E Fudd | 10005 | 1 |+----------------+---------+---------+5 rows in set (0.00 sec) 组合查询 MySQL允许执行多个查询(多条SELECT语句)，并将结果作为单个查询结果集返回。这些组合查询通常称为并(union)或复合查询(compound query)。 需要使用组合查询的两种基本情况: 1.在单个查询中从不同的表返回类似结构的数据； 2.对单个表执行多个查询，按单个查询返回数据。 创建组合查询 用UNION操作符来组合数条SQL查询 使用UNION 检索检索价格不高于5的所有物品 12345678910MariaDB [test]&gt; SELECT vend_id,prod_id,prod_price FROM products WHERE prod_price &lt;= 5;+---------+---------+------------+| vend_id | prod_id | prod_price |+---------+---------+------------+| 1003 | FC | 2.50 || 1002 | FU1 | 3.42 || 1003 | SLING | 4.49 || 1003 | TNT1 | 2.50 |+---------+---------+------------+4 rows in set (0.00 sec) 使用IN找出供应商1001和1002生产的所有物品 1234567891011MariaDB [test]&gt; SELECT vend_id,prod_id,prod_price FROM products WHERE vend_id IN (1001,1002);+---------+---------+------------+| vend_id | prod_id | prod_price |+---------+---------+------------+| 1001 | ANV01 | 5.99 || 1001 | ANV02 | 9.99 || 1001 | ANV03 | 14.99 || 1002 | FU1 | 3.42 || 1002 | OL1 | 8.99 |+---------+---------+------------+5 rows in set (0.01 sec) 用UNION关键字分隔 1234567891011121314MariaDB [test]&gt; SELECT vend_id,prod_id,prod_price FROM products WHERE prod_price &lt;= 5 UNION SELECT vend_id,prod_id,prod_price FROM products WHERE vend_id IN (1001,1002);+---------+---------+------------+| vend_id | prod_id | prod_price |+---------+---------+------------+| 1003 | FC | 2.50 || 1002 | FU1 | 3.42 || 1003 | SLING | 4.49 || 1003 | TNT1 | 2.50 || 1001 | ANV01 | 5.99 || 1001 | ANV02 | 9.99 || 1001 | ANV03 | 14.99 || 1002 | OL1 | 8.99 |+---------+---------+------------+8 rows in set (0.01 sec) UNION规则 UNION必须由两条或两条以上的SELECT语句组成，语句之间用关键字UNION分隔（如果组合4条SELECT语句，将要使用3个UNION关键字）。 UNION中的每个查询必须包含相同的列、表达式或聚集函数（不过各个列不需要以相同的次序列出）。 列数据类型必须兼容：类型不必完全相同，但必须是DBMS可以隐含地转换的类型。（例如，不同的数值类型或不同的日期类型） 包含或取消重复的行 UNION从查询结果集中自动去除了重复的行。在使用UNION时，重复的行被自动取消(默认行为)。如果想返回所有匹配行，可使用UNION ALL而不是UNION。 使用UNION ALL不取消重复的行 123456789101112131415MariaDB [test]&gt; SELECT vend_id,prod_id,prod_price FROM products WHERE prod_price &lt;= 5 UNION ALL SELECT vend_id,prod_id,prod_price FROM products WHERE vend_id IN (1001,1002);+---------+---------+------------+| vend_id | prod_id | prod_price |+---------+---------+------------+| 1003 | FC | 2.50 || 1002 | FU1 | 3.42 || 1003 | SLING | 4.49 || 1003 | TNT1 | 2.50 || 1001 | ANV01 | 5.99 || 1001 | ANV02 | 9.99 || 1001 | ANV03 | 14.99 || 1002 | FU1 | 3.42 || 1002 | OL1 | 8.99 |+---------+---------+------------+9 rows in set (0.00 sec) 如果确实需要每个条件的匹配行全部出现（包括重复行），则必须使用UNION ALL而不是WHERE。 对组合查询结果排序 在用UNION组合查询时，只能使用一条ORDER BY子句排序，它必须出现在最后一条SELECT语句之后。对于结果集，不存在用一种方式排序一部分，而又用另一种方式排序另一部分的情况，因此不允许使用多条ORDER BY子句进行排序。 1234567891011121314MariaDB [test]&gt; SELECT vend_id,prod_id,prod_price FROM products WHERE prod_price &lt;= 5 UNION SELECT vend_id,prod_id,prod_price FROM products WHERE vend_id IN (1001,1002) ORDER BY vend_id,prod_price;+---------+---------+------------+| vend_id | prod_id | prod_price |+---------+---------+------------+| 1001 | ANV01 | 5.99 || 1001 | ANV02 | 9.99 || 1001 | ANV03 | 14.99 || 1002 | FU1 | 3.42 || 1002 | OL1 | 8.99 || 1003 | FC | 2.50 || 1003 | TNT1 | 2.50 || 1003 | SLING | 4.49 |+---------+---------+------------+8 rows in set (0.00 sec) 全文本搜索 两个最常使用的引擎为MyISAM和InnoDB，前者支持全文本搜索，而后者不支持。 为了进行全文本搜索，必须索引被搜索的列，而且要随着数据的改变不断地重新索引。在对表列进行适当设计后， MySQL会自动进行所有的索引和重新索引。 启用全文本搜索支持 一般在创建表时启用全文本搜索。MySQL根据子句FULLTEXT(note_text)的指示对它进行索引。 123456789MariaDB [test]&gt; CREATE TABLE notes ( note_id int NOT NULL AUTO_INCREMENT,prod_id char(10) NOT NULL,note_data datetime NOT NULL,note_text text NULL,primary KEY(note_id),FULLTEXT(note_text)) ENGINE=MyISAM;Query OK, 0 rows affected (0.02 sec) 可以在创建表时指定FULLTEXT，或者在稍后指定（在这种情况下所有已有数据必须立即索引）。 进行全文本搜索 在索引之后，使用两个函数Match()和Against()执行全文本搜索，其中Match()指定被搜索的列，Against()指定要使用的搜索表达式。 123456789MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('rabbit');+----------------------------------------------------------------------------------------------------------------------+| note_text |+----------------------------------------------------------------------------------------------------------------------+| Customer complaint: rabbit has been able to detect trap, food apparently less effective now. || Quantity varies, sold by the sack load.All guaranteed to be bright and orange, and suitable for use as rabbit bait. |+----------------------------------------------------------------------------------------------------------------------+2 rows in set (0.01 sec) Match(note_text)对指定列进行搜索。 Against(‘rabbit’)指定词rabbit作为搜索文本。 传递给Match()的值必须与FULLTEXT()定义中的相同。如果指定多个列，则必须列出它们（而且次序正确）。 除非使用BINARY方式，否则全文本搜索不区分大小写。 对结果排序 在SELECT中使用Match()和Against()。这使所有行都被返回 （因为没有WHERE子句）。Match()和Against()用来建立一个计算列（别名为rank），此列包含全文本搜索计算出的等级值。等级根据行中词的数目、唯一词的数目、整个索引中词的总数以及包含该词的行的数目计算出来。 1234567891011121314151617181920212223242526272829MariaDB [test]&gt; SELECT note_text,Match(note_text) Against('rabbit') AS rank FROM productnotes;+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+| note_text | rank |+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+| Customer complaint:Sticks not individually wrapped, too easy to mistakenly detonate all at once.Recommend individual wrapping. | 0 || Can shipped full, refills not available.Need to order new can if refill needed. | 0 || Safe is combination locked, combination not provided with safe.This is rarely a problem as safes are typically blown up or dropped by customers. | 0 || Quantity varies, sold by the sack load.All guaranteed to be bright and orange, and suitable for use as rabbit bait. | 1.5905543565750122 || Included fuses are short and have been known to detonate too quickly for some customers.Longer fuses are available (item FU1) and should be recommended. | 0 || Matches not included, recommend purchase of matches or detonator (item DTNTR). | 0 || Please note that no returns will be accepted if safe opened using explosives. | 0 || Multiple customer returns, anvils failing to drop fast enough or falling backwards on purchaser. Recommend that customer considers using heavier anvils. | 0 || Item is extremely heavy. Designed for dropping, not recommended for use with slings, ropes, pulleys, or tightropes. | 0 || Customer complaint: rabbit has been able to detect trap, food apparently less effective now. | 1.6408053636550903 || Shipped unassembled, requires common tools (including oversized hammer). | 0 || Customer complaint:Circular hole in safe floor can apparently be easily cut with handsaw. | 0 || Customer complaint:Not heavy enough to generate flying stars around head of victim. If being purchased for dropping, recommend ANV02 or ANV03 instead. | 0 || Call from individual trapped in safe plummeting to the ground, suggests an escape hatch be added.Comment forwarded to vendor. | 0 |+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+14 rows in set (0.00 sec) 如果指定多个搜索项，则包含多数匹配词的那些行将具有比包含较少词（或仅有一个匹配）的那些行高的等级值。 使用查询扩展 查询扩展用来设法放宽所返回的全文本搜索结果的范围。利用查询扩展，能找出可能相关的结果，即使它们并不精确包含所查找的词。 在使用查询扩展时， MySQL对数据和索引进行两遍扫描来完成搜索: 首先，进行一个基本的全文本搜索，找出与搜索条件匹配的所有行。 其次，MySQL检查这些匹配行并选择所有有用的词。 之后，MySQL再次进行全文本搜索，这次不仅使用原来的条件，而且还使用所有有用的词。 首先进行简单的全文本搜索，找出所有提到anvils的注释 1234567MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('anvils');+----------------------------------------------------------------------------------------------------------------------------------------------------------+| note_text |+----------------------------------------------------------------------------------------------------------------------------------------------------------+| Multiple customer returns, anvils failing to drop fast enough or falling backwards on purchaser. Recommend that customer considers using heavier anvils. |+----------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 使用查询扩展 1234567891011121314151617MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('anvils' WITH QUERY EXPANSION);+----------------------------------------------------------------------------------------------------------------------------------------------------------+| note_text |+----------------------------------------------------------------------------------------------------------------------------------------------------------+| Multiple customer returns, anvils failing to drop fast enough or falling backwards on purchaser. Recommend that customer considers using heavier anvils. || Customer complaint:Sticks not individually wrapped, too easy to mistakenly detonate all at once.Recommend individual wrapping. || Customer complaint:Not heavy enough to generate flying stars around head of victim. If being purchased for dropping, recommend ANV02 or ANV03 instead. || Please note that no returns will be accepted if safe opened using explosives. || Customer complaint: rabbit has been able to detect trap, food apparently less effective now. || Customer complaint:Circular hole in safe floor can apparently be easily cut with handsaw. || Matches not included, recommend purchase of matches or detonator (item DTNTR). |+----------------------------------------------------------------------------------------------------------------------------------------------------------+7 rows in set (0.00 sec) 第一行包含词anvils，因此等级最高。第二行与anvils无关，但因为它包含第一行中的两个词（customer和recommend），所以也被检索出来。第3行也包含这两个相同的词，但它们在文本中的位置更靠后且分开得更远，因此也包含这一行，但等级为第三。 布尔文本搜索 要匹配的词 要排斥的词（如果某行包含这个词，则不返回该行，即使它包含其他指定的词也是如此） 排列提示（指定某些词比其他词更重要，更重要的词等级更高） 表达式分组 另外一些内容 即使没有FULLTEXT索引也可以使用，但其性能将随着数据量的增加而降低 检索包含词heavy的所有行 123456789MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('heavy' IN BOOLEAN MODE);+---------------------------------------------------------------------------------------------------------------------------------------------------------+| note_text |+---------------------------------------------------------------------------------------------------------------------------------------------------------+| Item is extremely heavy. Designed for dropping, not recommended for use with slings, ropes, pulleys, or tightropes. || Customer complaint:Not heavy enough to generate flying stars around head of victim. If being purchased for dropping, recommend ANV02 or ANV03 instead. |+---------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows in set (0.01 sec) 匹配包含heavy但不包含任意以rope开始的词的行 12345678MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('heavy -rope*' IN BOOLEAN MODE);+---------------------------------------------------------------------------------------------------------------------------------------------------------+| note_text |+---------------------------------------------------------------------------------------------------------------------------------------------------------+| Customer complaint:Not heavy enough to generate flying stars around head of victim. If being purchased for dropping, recommend ANV02 or ANV03 instead. |+---------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 全文本布尔操作符 布尔操作符 说明 + 包含，词必须存在 - 排除，词必须不出现 &gt; 包含，而且增加等级值 &lt; 包含，且减少等级值 () 把词组成子表达式（允许这些子表达式作为一个组被包含、排除、排列等） ~ 取消一个词的排序值 * 词尾的通配符 “” 定义一个短语（与单个词的列表不一样，它匹配整个短语以便包含或排除这个短语） 搜索匹配包含词rabbit和bait的行 12345678MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('+rabbit +bait' IN BOOLEAN MODE);+----------------------------------------------------------------------------------------------------------------------+| note_text |+----------------------------------------------------------------------------------------------------------------------+| Quantity varies, sold by the sack load.All guaranteed to be bright and orange, and suitable for use as rabbit bait. |+----------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 匹配包含rabbit和bait中的至少一个词的行 123456789MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('rabbit bait' IN BOOLEAN MODE);+----------------------------------------------------------------------------------------------------------------------+| note_text |+----------------------------------------------------------------------------------------------------------------------+| Quantity varies, sold by the sack load.All guaranteed to be bright and orange, and suitable for use as rabbit bait. || Customer complaint: rabbit has been able to detect trap, food apparently less effective now. |+----------------------------------------------------------------------------------------------------------------------+2 rows in set (0.00 sec) 匹配短语rabbit bait而不是匹配两个词rabbit和bait 12345678MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('"rabbit bait"' IN BOOLEAN MODE);+----------------------------------------------------------------------------------------------------------------------+| note_text |+----------------------------------------------------------------------------------------------------------------------+| Quantity varies, sold by the sack load.All guaranteed to be bright and orange, and suitable for use as rabbit bait. |+----------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 匹配rabbit和carrot，增加前者的等级，降低后者的等级 123456789MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('&gt;rabbit &lt;carrot' IN BOOLEAN MODE);+----------------------------------------------------------------------------------------------------------------------+| note_text |+----------------------------------------------------------------------------------------------------------------------+| Quantity varies, sold by the sack load.All guaranteed to be bright and orange, and suitable for use as rabbit bait. || Customer complaint: rabbit has been able to detect trap, food apparently less effective now. |+----------------------------------------------------------------------------------------------------------------------+2 rows in set (0.00 sec) 匹配词safe和combination，降低后者的等级 12345678MariaDB [test]&gt; SELECT note_text FROM productnotes WHERE Match(note_text) Against('+safe +(&lt;combination)' IN BOOLEAN MODE);+---------------------------------------------------------------------------------------------------------------------------------------------------+| note_text |+---------------------------------------------------------------------------------------------------------------------------------------------------+| Safe is combination locked, combination not provided with safe.This is rarely a problem as safes are typically blown up or dropped by customers. |+---------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 排列而不排序:在布尔方式中，不按等级值降序排序返回的行。 全文本搜索的使用说明 在索引全文本数据时，短词被忽略且从索引中排除。短词定义为那些具有3个或3个以下字符的词 （如果需要，这个数目可以更改）。 MySQL带有一个内建的非用词（stopword）列表，这些词在索引全文本数据时总是被忽略。如果需要，可以覆盖这个列表。 许多词出现的频率很高，搜索它们没有用处（返回太多的结果）。因此， MySQL规定了一条50%规则，如果一个词出现在50%以上的行中，则将它作为一个非用词忽略。50%规则不用于IN BOOLEAN MODE。 如果表中的行数少于3行，则全文本搜索不返回结果（因为每个词或者不出现，或者至少出现在50%的行中）。 忽略词中的单引号。例如， don’t索引为dont。 不具有词分隔符（包括日语和汉语）的语言不能恰当地返回全文本搜索结果。 仅在MyISAM数据库引擎中支持全文本搜索。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL之数据处理篇]]></title>
    <url>%2Fblog%2Fmysql3%2F</url>
    <content type="text"><![CDATA[本篇主要关于计算字段、数据处理函数、分组汇总数据以及子查询。 创建计算字段计算字段 计算字段并不实际存在于数据库表中。计算字段是运行时在SELECT语句内创建的，是直接从数据库中检索出转换、计算或格式化过的数据。 拼接字段 将值联结到一起构成单个值叫拼接(concatenate)。MySQL使用Concat()函数来拼接两个列。Concat()拼接串，即把多个串连接起来形成一个较长的串。Concat()需要一个或多个指定的串，各个串之间用逗号分隔。 12345678910MariaDB [zabbix]&gt; SELECT Concat(alias,'(',passwd,')') FROM users ;+------------------------------------------+| Concat(alias,'(',passwd,')') |+------------------------------------------+| Admin(5fce1b3e34b520afeffb37ce08c7cd66) || guest(d41d8cd98f00b204e9800998ecf8427e) || flyabc(0fbc21a1f5f4cb7faef6cba8e0dd99df) || flyabd(0fbc21a1f5f4cb7faef6cba8e0dd99df) |+------------------------------------------+4 rows in set (0.01 sec) 上面的SELECT语句连接以下4个元素: 存储在users列中的帐号; 包含一个空格和一个左圆括号的串; 存储在passwd列中的密码; 包含一个右圆括号的串。 使用别名 别名(alias)是一个字段或值的替换名，用AS关键字赋予。 12345678910MariaDB [zabbix]&gt; SELECT Concat(alias,'(',passwd,')') AS new_line FROM users ;+------------------------------------------+| new_line |+------------------------------------------+| Admin(5fce1b3e34b520afeffb37ce08c7cd66) || guest(d41d8cd98f00b204e9800998ecf8427e) || flyabc(0fbc21a1f5f4cb7faef6cba8e0dd99df) || flyabd(0fbc21a1f5f4cb7faef6cba8e0dd99df) |+------------------------------------------+4 rows in set (0.00 sec) 执行算术计算 对检索出的数据进行算术计算 检索autologin = 0的所有行 123456789MariaDB [zabbix]&gt; SELECT userid,type FROM users WHERE autologin = 0;+--------+------+| userid | type |+--------+------+| 2 | 1 || 3 | 1 || 4 | 1 |+--------+------+3 rows in set (0.00 sec) 输出中显示的new_line列为一个计算字段，此计算为hostgroupid*groupid 123456789MariaDB [zabbix]&gt; SELECT userid,type,userid+type AS new_line FROM users WHERE autologin = 0;+--------+------+----------+| userid | type | new_line |+--------+------+----------+| 2 | 1 | 3 || 3 | 1 | 4 || 4 | 1 | 5 |+--------+------+----------+3 rows in set (0.00 sec) MySQL算术操作符 操作符 说明 + 加 - 减 * 乘 / 除 使用数据处理函数文本处理函数 用于处理文本串(如删除或填充值，转换值为大写或小写) 常用文本处理函数 函数 说明 Left() 返回串左边的字符 Length() 返回串的长度 Locate() 找出串的一个子串 Lower() 将串转换为小写 LTrim() 去掉串左边的空格 Right() 返回串右边的字符 RTrim() 去掉串右边的空格 Soundex() 返回串的SOUNDEX值 SubString() 返回子串的字符 Upper() 将串转换为大写 使用UPPER()函数将文本转换为大写 12345678910MariaDB [zabbix]&gt; SELECT alias,UPPER(alias) AS new_alias FROM users;+--------+-----------+| alias | new_alias |+--------+-----------+| Admin | ADMIN || flyabc | FLYABC || flyabd | FLYABD || guest | GUEST |+--------+-----------+4 rows in set (0.00 sec) 日期和时间处理函数 用于处理日期和时间值并从这些值中提取特定成分 名称 描述 ADDDATE() 将时间值(间隔)添加到日期值 ADDTIME() 加时间 CONVERT_TZ() 从一个时区转换到另一个时区 CURDATE() 返回当前日期 CURRENT_DATE()， CURRENT_DATE CURDATE()的同义词 CURRENT_TIME()， CURRENT_TIME CURTIME()的同义词 CURRENT_TIMESTAMP()， CURRENT_TIMESTAMP NOW()的同义词 CURTIME() 返回当前时间 DATE() 提取日期或日期时间表达式的日期部分 DATE_ADD() 将时间值(间隔)添加到日期值 DATE_FORMAT() 指定格式日期 DATE_SUB() 从日期中减去时间值(间隔) DATEDIFF() 减去两个日期 DAY() DAYOFMONTH()的同义词 DAYNAME() 返回工作日的名称 DAYOFMONTH() 返回月份中的一天(0-31) DAYOFWEEK() 返回参数的工作日索引 DAYOFYEAR() 返回一年中的某天(1-366) EXTRACT() 提取部分日期 FROM_DAYS() 将天数转换为日期 FROM_UNIXTIME() 将Unix时间戳记格式化为日期 GET_FORMAT() 返回日期格式字符串 HOUR() 提取时间 LAST_DAY 返回参数的月份的最后一天 LOCALTIME()， LOCALTIME NOW()的同义词 LOCALTIMESTAMP， LOCALTIMESTAMP() NOW()的同义词 MAKEDATE() 从一年中的年月日创建日期 MAKETIME() 从小时，分钟，秒创建时间 MICROSECOND() 从参数返回微秒 MINUTE() 返回参数的分钟 MONTH() 返回经过日期的月份 MONTHNAME() 返回月份名称 NOW() 返回当前日期和时间 PERIOD_ADD() 在一年的月份中添加一个期间 PERIOD_DIFF() 返回期间之间的月数 QUARTER() 从日期参数返回季度 SEC_TO_TIME() 将秒转换为”hh:mm:ss”格式 SECOND() 返回第二个(0-59) STR_TO_DATE() 将字符串转换为日期 SUBDATE() 用三个参数调用时DATE_SUB()的同义词 SUBTIME() 减去时间 SYSDATE() 返回函数执行的时间 TIME() 提取传递的表达式的时间部分 TIME_FORMAT() 格式化为时间 TIME_TO_SEC() 返回参数转换为秒 TIMEDIFF() 减去时间 TIMESTAMP() 仅使用一个参数，此函数将返回日期或日期时间表达式。有两个参数，参数的总和 TIMESTAMPADD() 向日期时间表达式添加间隔 TIMESTAMPDIFF() 从日期时间表达式中减去一个间隔 TO_DAYS() 返回日期参数转换为天 TO_SECONDS() 返回从Year 0开始转换为秒的日期或datetime参数 UNIX_TIMESTAMP() 返回Unix时间戳 UTC_DATE() 返回当前UTC日期 UTC_TIME() 返回当前UTC时间 UTC_TIMESTAMP() 返回当前UTC日期和时间 WEEK() 返回星期数 WEEKDAY() 返回工作日索引 WEEKOFYEAR() 返回日期的日历周(1-53) YEAR() 返回年份 YEARWEEK() 返回年和周 数值处理函数 用于在数值数据上进行算术操作 名称 描述 ABS() 返回绝对值 ACOS() 返回反余弦 ASIN() 返回反正弦 ATAN() 返回反正切 ATAN2()， ATAN() 返回两个参数的反正切 CEIL() 返回不小于参数的最小整数值 CEILING() 返回不小于参数的最小整数值 CONV() 在不同的基数之间转换数字 COS() 返回余弦 COT() 返回余切 CRC32() 计算循环冗余校验值 DEGREES() 将弧度转换为度 DIV 整数除法 / 部门运营商 EXP() 提升力量 FLOOR() 返回不大于参数的最大整数值 LN() 返回参数的自然对数 LOG() 返回第一个参数的自然对数 LOG10() 返回参数的以10为底的对数 LOG2() 返回参数的以2为底的对数 - 减号 MOD() 退还剩余 %， MOD 模运算符 PI() 返回pi的值 + 加法运算符 POW() 将参数提高到指定的幂 POWER() 将参数提高到指定的幂 RADIANS() 返回参数转换为弧度 RAND() 返回一个随机浮点值 ROUND() 围绕论点 SIGN() 返回参数的符号 SIN() 返回参数的正弦 SQRT() 返回参数的平方根 TAN() 返回参数的切线 * 乘法运算符 TRUNCATE() 截断为指定的小数位数 - 更改参数的符号 汇总数据聚集函数 运行在行组上，计算和返回单个值的函数 AVG()函数 AVG()通过对表中的行数计数并计算特定列值之和，求得该列的平均值。AVG()可用来返回所有列的平均值，也可用来返回特定列或行的平均值。AVG()只能用来确定特定数值列的平均值，而且列名必须作为函数参数给出。为了获得多个列的平均值，必须使用多个AVG()函数。AVG()函数忽略列值为NULL的行。 返回hostroupid列的平均值 1234567MariaDB [zabbix]&gt; SELECT AVG(hostgroupid) AS new_line FROM hosts_groups;+----------+| new_line |+----------+| 534.9024 |+----------+1 row in set (0.01 sec) 返回hostroupid列的平均值(过滤出仅groupid值等于10的值) 1234567MariaDB [zabbix]&gt; SELECT AVG(hostgroupid) AS new_line FROM hosts_groups WHERE groupid = '10';+----------+| new_line |+----------+| 210.5000 |+----------+1 row in set (0.00 sec) COUNT()函数 可利用COUNT()确定表中行的数目或符合特定条件的行的数目 COUNT()函数有两种使用方式: 1.使用COUNT(*)对表中行的数目进行计数，不管表列中包含的空值(NULL)还是非空值; 2.使用COUNT(column)对特定列中具有值的行进行计数，忽略NULL值。 对所有行进行计数 1234567MariaDB [zabbix]&gt; SELECT COUNT(*) AS new_line FROM hosts_groups;+----------+| new_line |+----------+| 246 |+----------+1 row in set (0.00 sec) 对具体行进行计数 1234567MariaDB [zabbix]&gt; SELECT COUNT(hostid) AS new_line FROM hosts_groups;+----------+| new_line |+----------+| 246 |+----------+1 row in set (0.00 sec) 注: 如指定列名，则指定列的值为空的行被COUNT()函数忽略，但如果COUNT()函数中用的是星号(*)，则不忽略。 MAX()函数 MAX()返回指定列中的最大值。MAX()要求指定列名。对非数值数据使用MAX()时，如果数据按相应的列排序，则MAX()返回最后一行。MAX()函数忽略值为NULL的行。 1234567MariaDB [zabbix]&gt; SELECT MAX(hostid) AS new_line FROM hosts_groups;+----------+| new_line |+----------+| 10511 |+----------+1 row in set (0.00 sec) MIN()函数 MIN()返回指定列中的最小值。MIN()要求指定列名。对非数值数据使用MIN()时，如果数据按相应的列排序，则MIN()返回最前面的行。MIN()函数忽略值为NULL的行。 1234567MariaDB [zabbix]&gt; SELECT MIN(hostid) AS new_line FROM hosts_groups;+----------+| new_line |+----------+| 10001 |+----------+1 row in set (0.00 sec) SUM()函数 SUM()返回指定列值的和(总计)。也可用来合计计算值。忽略列值为NULL的行。 只统计groupid等于10的hostgroupid之和 1234567MariaDB [zabbix]&gt; SELECT SUM(hostgroupid) AS new_line FROM hosts_groups WHERE groupid = '10';+----------+| new_line |+----------+| 2105 |+----------+1 row in set (0.00 sec) 合计hostgroupid*hostid。WHERE保证只统计groupid等于10的值。 1234567MariaDB [zabbix]&gt; SELECT SUM(hostgroupid*hostid) AS new_line FROM hosts_groups WHERE groupid = '10';+----------+| new_line |+----------+| 21291711 |+----------+1 row in set (0.00 sec) 组合聚集函数 SELECT语句可根据需要包含多个聚集函数 返回行数，最小值，最大值，平均值 1234567MariaDB [zabbix]&gt; SELECT COUNT(*) AS num,MIN(hostgroupid) AS min,MAX(hostgroupid) AS max,AVG(hostgroupid) AS avg FROM hosts_groups;+-----+------+------+----------+| num | min | max | avg |+-----+------+------+----------+| 246 | 92 | 791 | 534.9024 |+-----+------+------+----------+1 row in set (0.00 sec) 分组数据数据分组 分组允许把数据分为多个逻辑组，以便能对每个组进行聚集计算。 创建分组 分组是在SELECT语句的GROUP BY子句中建立的。GROUP BY子句指示MySQL分组数据，然后对每个组而不是整个结果集进行聚集。 GROUP BY子句的一些重要规定: 1.GROUP BY子句可以包含任意数目的列。 2.如果在GROUP BY子句中嵌套了分组，数据将在最后规定的分组上进行汇总。即在建立分组时，指定的所有列都在一起计算。 3.GROUP BY子句中列出的每个列都必须是检索列或有效的表达式(但不能是聚集函数)。如果在SELECT中使用表达式，则必须在GROUP BY子句中指定相同的表达式。不能使用别名。 4.大多数SQL实现不允许GROUP + BY列带有长度可变的数据类型(如文本或备注型字段)。 5.除聚集计算语句外，SELECT语句中每个列都必须在GROUP BY子句中给出。 6.如果分组列中具有NULL值，则NULL将作为一个分组返回。如果列中有多行NULL值，他们讲分为一组。 7.GROUP BY子句必须出现为WHERE子句之后，ORDER BY子句之前。 指定两列，groupid即为相关组ID，num为计算字段，GROUP BY子句指示MySQL按groupid排序并分组数据。如下groupid为4的有1个为5的有72个。 123456789101112131415161718192021MariaDB [zabbix]&gt; SELECT groupid,COUNT(*) AS num FROM hosts_groups GROUP BY groupid;+---------+-----+| groupid | num |+---------+-----+| 4 | 1 || 5 | 72 || 8 | 31 || 9 | 26 || 10 | 10 || 11 | 7 || 12 | 6 || 13 | 1 || 14 | 3 || 21 | 9 || 22 | 14 || 23 | 14 || 24 | 14 || 25 | 14 || 26 | 8 |+---------+-----+15 rows in set (0.00 sec) 过滤分组 过滤分组必须基于完整的分组而不是个别的行进行过滤。 HAVING支持所有WHERE操作符。唯一差别是WHERE过滤行，而HAVING过滤分组。WHERE在数据分组前进行过滤，HAVING在数据分组后进行过滤。 分组排序完成后，过滤出出现次数大于等于20的groupid 123456789MariaDB [zabbix]&gt; SELECT groupid,COUNT(*) AS num FROM hosts_groups GROUP BY groupid HAVING COUNT(*) &gt;= 20;+---------+-----+| groupid | num |+---------+-----+| 5 | 72 || 8 | 31 || 9 | 26 |+---------+-----+3 rows in set (0.00 sec) 同时使用WHERE子句和HAVING子句 使用WHERE子句过滤出hostgroupid大于等于600的，然后按groupid分组数据，HAVING子句过滤计数大于等于20的分组。 1234567MariaDB [zabbix]&gt; SELECT groupid,COUNT(*) AS num FROM hosts_groups WHERE hostgroupid &gt;= 600 GROUP BY groupid HAVING COUNT(*) &gt;= 20;+---------+-----+| groupid | num |+---------+-----+| 5 | 71 |+---------+-----+1 row in set (0.00 sec) 分组和排序 ORDER BY 和 GROUP BY之间的差别: ORDER BY GROUP BY 排序产生的输出 分组行。但是输出可能不是分组的顺序 任意列都可以使用(甚至非选择的列也可以使用) 只可能使用选择列或表达式列，而且必须使用每个选择列表达式 不一定需要 如果与聚集函数一起使用列(或表达式)，则必须使用 注: 一般在使用GROUP BY子句时，应该也给出ORDER BY子句。这是保证数据正确排序的唯一方法。 按groupid出现的次数进行排序 123456789MariaDB [zabbix]&gt; SELECT groupid,COUNT(*) AS num FROM hosts_groups GROUP BY groupid HAVING COUNT(*) &gt;= 20 ORDER BY num;+---------+-----+| groupid | num |+---------+-----+| 9 | 26 || 8 | 31 || 5 | 72 |+---------+-----+3 rows in set (0.00 sec) SELECT子句顺序 子句 说明 是否必须使用 SELECT 要返回的列或表达式 是 FROM 从中检索数据的表 仅在从表选择数据时使用 WHERE 行级过滤 否 GROUP BY 分组说明 仅在按组计算聚集时使用 HAVING 组级过滤 否 ORDER BY 输出排序顺序 否 LIMIT 要检索的行数 否 使用子查询子查询 子查询(subquery)，即嵌套在其他查询中的查询。 利用子查询进行过滤 作为子查询的SELECT语句只能查询单个列。在SELECT语句中，子查询总是从内向外处理。 查询关于FC的所有用户 检索orderitems表中关于FC的order_num列 1234567MariaDB [test]&gt; SELECT order_num FROM orderitems WHERE prod_id = 'FC';+-----------+| order_num |+-----------+| 20008 |+-----------+1 row in set (0.00 sec) 根据在orderitems表检索的关于FC的order_num列在orders表中找到相应id 1234567MariaDB [test]&gt; SELECT cust_id FROM orders WHERE order_num IN (20008);+---------+| cust_id |+---------+| 10005 |+---------+1 row in set (0.00 sec) 根据id检索检索相关信息 1234567MariaDB [test]&gt; SELECT cust_name,cust_contact FROM customers WHERE cust_id IN (10005);+-----------+--------------+| cust_name | cust_contact |+-----------+--------------+| E Fudd | E Fudd |+-----------+--------------+1 row in set (0.00 sec) 使用子查询把上面的第一个查询变成子查询。在SELECT语句中，子查询总是从内向外处理。 1234567MariaDB [test]&gt; SELECT cust_id FROM orders WHERE order_num IN (SELECT order_num FROM orderitems WHERE prod_id = 'FC');+---------+| cust_id |+---------+| 10005 |+---------+1 row in set (0.00 sec) 1234567MariaDB [test]&gt; SELECT cust_name,cust_contact FROM customers WHERE cust_id IN (SELECT cust_id FROM orders WHERE order_num IN (SELECT order_num FROM orderitems WHERE prod_id = 'FC'));+-----------+--------------+| cust_name | cust_contact |+-----------+--------------+| E Fudd | E Fudd |+-----------+--------------+1 row in set (0.00 sec) 作为计算字段使用子查询 使用子查询的另一方法是创建计算字段。 对customers表中每个客户返回3列:cust_name、cust_state和orders。 orders是一个计算字段，它是由圆括号中的子查询建立的。该子查询对检索出的每个客户执行一次。WHERE子句使用完全限定列名限制出现歧义性的列名。 1234567891011MariaDB [test]&gt; SELECT cust_name,cust_state,(SELECT COUNT(*) FROM orders WHERE orders.cust_id = customers.cust_id) AS orders FROM customers ORDER BY cust_name;+----------------+------------+--------+| cust_name | cust_state | orders |+----------------+------------+--------+| E Fudd | IL | 1 || Jack | NY | 0 || li | NY | 0 || Tom | CA | 0 || wang | BJ | 0 |+----------------+------------+--------+5 rows in set (0.01 sec) 涉及外部查询的子查询称为相关子查询。任何时候只要列名可能有多义性，就必须使用这种语法(表名和列名由一个句点分隔)。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL之检索过滤数据篇]]></title>
    <url>%2Fblog%2Fmysql2%2F</url>
    <content type="text"><![CDATA[该篇主要为检索数据、过滤数据以及使用正则表达式进行搜索。 检索数据 SELECT用于检索从一个或多个表中选择的行，并且可以包括UNION语句和子查询。 检索单个列 所需的列名在SELECT关键字之后给出， FROM关键字指出从其中检索数据的表名。如下即为从users表中检索passwd列。 12345678MariaDB [zabbix]&gt; SELECT passwd FROM users;+----------------------------------+| passwd |+----------------------------------+| 5fce1b3e34b520afeffb37ce08c7cd66 || d41d8cd98f00b204e9800998ecf8427e |+----------------------------------+2 rows in set (0.00 sec) 检索多个列 与检索单个列唯一的不同是必须在SELECT关键字后给出多个列名，列名之间必须以逗号分隔。 12345678MariaDB [zabbix]&gt; SELECT alias,passwd FROM users;+-------+----------------------------------+| alias | passwd |+-------+----------------------------------+| Admin | 5fce1b3e34b520afeffb37ce08c7cd66 || guest | d41d8cd98f00b204e9800998ecf8427e |+-------+----------------------------------+2 rows in set (0.00 sec) 检索所有列 使用通配符”*”即可检索所有列 1234567MariaDB [test]&gt; SELECT * FROM pet;+------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+------+-------+---------+------+------------+-------+| Tom | Diane | hamster | f | 1999-03-30 | NULL |+------+-------+---------+------+------------+-------+1 row in set (0.00 sec) 检索不同的行 使用关键字DISTINCT指示MySQL只返回不同的值。即如groupid为8的值有10个，则只返回唯一的一个。 123456789101112131415161718192021MariaDB [zabbix]&gt; SELECT DISTINCT groupid FROM hosts_groups;+---------+| groupid |+---------+| 4 || 5 || 8 || 9 || 10 || 11 || 12 || 13 || 14 || 21 || 22 || 23 || 24 || 25 || 26 |+---------+15 rows in set (0.11 sec) 限制结果 SELECT语句返回所有匹配的行，它们可能是指定表中的每个行。为了返回第一行或前几行，可使用LIMIT子句。 返回不多于5行1234567891011MariaDB [zabbix]&gt; SELECT hostid FROM hosts_groups LIMIT 5;+--------+| hostid |+--------+| 10001 || 10047 || 10048 || 10050 || 10074 |+--------+5 rows in set (0.00 sec) 返回从第5行开始的5行1234567891011MariaDB [zabbix]&gt; SELECT hostid FROM hosts_groups LIMIT 5,5;+--------+| hostid |+--------+| 10075 || 10076 || 10077 || 10078 || 10079 |+--------+5 rows in set (0.00 sec) 检索出来的第一行为行0而不是行1.因此，LIMIT 1,1 将检索出第二行而不是第一行。 如果没有足够的行，将只返回它能返回的最大行。 完全限定表名 即同时使用表名和列字。 1234567MariaDB [test]&gt; SELECT pet.name,pet.birth FROM pet;+------+------------+| name | birth |+------+------------+| Tom | 1999-03-30 |+------+------------+1 row in set (0.00 sec) 排序检索数据排序数据非排序状态 原始数据并没有特定的顺序 123456789MariaDB [zabbix]&gt; SELECT * FROM hosts_groups;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 194 | 10001 | 10 || 189 | 10047 | 12 || 188 | 10048 | 12 || 187 | 10050 | 8 |...... 排序状态 为了明确地用SELECT语句对检索出的数据进行排序，可使用ORDER BY子句。ORDER BY子句取一个或多个列的名字，以此对输出进行排序。 123456789MariaDB [zabbix]&gt; SELECT * FROM hosts_groups ORDER BY hostgroupid;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 92 | 10084 | 4 || 175 | 10093 | 8 || 176 | 10169 | 12 || 177 | 10095 | 8 |...... 按多个列进行排序 为了按多个列排序，只要指定列名，列名之间用逗号分开即可(就像选择多个列时所做的那样)。在按多个列排序时，排序完全按所规定的顺序进行。(如下即为首先按groupid,然后再按hostgroupid进行排序) 123456789MariaDB [zabbix]&gt; SELECT * FROM hosts_groups ORDER BY groupid,hostgroupid;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 92 | 10084 | 4 || 544 | 10388 | 5 || 618 | 10425 | 5 || 620 | 10426 | 5 |...... 按列位置排序 首先按第1列(hostgroupid),然后再按第3列(groupid)进行排序 123456789MariaDB [zabbix]&gt; SELECT * FROM hosts_groups ORDER BY 1,3;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 92 | 10084 | 4 || 175 | 10093 | 8 || 176 | 10169 | 12 || 177 | 10095 | 8 |...... 指定排序方向 默认的排序顺序为升序排序(从A到Z)。还可以使用ORDER BY子句以降序(从Z到A)顺序排序。为了进行降序排序，必须指定DESC关键字。 123456789MariaDB [zabbix]&gt; SELECT * FROM hosts_groups ORDER BY hostgroupid DESC;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 631 | 10431 | 21 || 630 | 10431 | 5 || 629 | 10430 | 21 || 628 | 10430 | 5 |.... 指定多列降序排序 首先按groupid进行降序排序,然后再按hostgroupid进行降序排序 123456789MariaDB [zabbix]&gt; SELECT * FROM hosts_groups ORDER BY groupid DESC,hostgroupid;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 457 | 10344 | 21 || 545 | 10388 | 21 || 619 | 10425 | 21 || 621 | 10426 | 21 |...... 过滤数据 数据库表一般包含大量的数据，很少需要检索表中所有行。通常只会根据特定操作或报告的需要提取表数据的子集。只检索所需数据需要指定搜索条件(search criteria)，搜索条件也称为过滤条件(filter condition)。 使用WHERE子句 在SELECT语句中，数据根据WHERE子句中指定的搜索条件进行过滤。WHERE子句在表名(FROM子句)之后给出。 操作符 说明 = 等于 &lt;&gt; 不等于 != 不等于 &lt; 小于 &lt;= 小于等于 !&lt; 不小于 &gt; 大于 &gt;= 大于等于 !&gt; 不大于 BETWEEN 在指定的两个值之间 IS NULL 为NULL的值 检查单个值 检索groupid值等于5的行 1234567891011121314MariaDB [zabbix]&gt; SELECT * FROM hosts_groups WHERE groupid=5;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 544 | 10388 | 5 || 618 | 10425 | 5 || 620 | 10426 | 5 || 622 | 10427 | 5 || 624 | 10428 | 5 || 626 | 10429 | 5 || 628 | 10430 | 5 || 630 | 10431 | 5 |+-------------+--------+---------+8 rows in set (0.02 sec) 列出hostgroupid值小于100的行 1234567MariaDB [zabbix]&gt; SELECT * FROM hosts_groups WHERE hostgroupid &lt; 100;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 92 | 10084 | 4 |+-------------+--------+---------+1 row in set (0.00 sec) 列出hostgroupid值小于等于100的行 1234567MariaDB [zabbix]&gt; SELECT * FROM hosts_groups WHERE hostgroupid &lt;= 100;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 92 | 10084 | 4 |+-------------+--------+---------+1 row in set (0.00 sec) 不匹配检查 列出除了guest用户外的所有用户 1234567MariaDB [zabbix]&gt; SELECT alias FROM users WHERE alias &lt;&gt; 'guest';+-------+| alias |+-------+| Admin |+-------+1 row in set (0.00 sec) 1234567MariaDB [zabbix]&gt; SELECT alias FROM users WHERE alias != 'guest';+-------+| alias |+-------+| Admin |+-------+1 row in set (0.00 sec) 单引号用来限定字符串。如果将值与串类型的列进行比较，则需要限定引号。用来与数值列进行比较的值不用引号。 范围值检查 可使用BETWEEN操作符检查某个范围的值。该操作符需要两个值，即范围的开始值和结束值。 123456789101112MariaDB [zabbix]&gt; SELECT * FROM hosts_groups WHERE hostgroupid between 170 and 180;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 175 | 10093 | 8 || 176 | 10169 | 12 || 177 | 10095 | 8 || 178 | 10094 | 8 || 179 | 10096 | 8 || 180 | 10097 | 8 |+-------------+--------+---------+6 rows in set (0.00 sec) 空值检查 在一个列不包含值时，称其为包含空值NULL。它与字段包含0、空字符串或仅仅包含空格不同。检查具有NULL值的列使用IS NULL子句。 1234567MariaDB [test]&gt; SELECT name FROM pet WHERE death IS NULL;+------+| name |+------+| Tom |+------+1 row in set (0.00 sec) 组合WHERE子句 操作符(operator)是用来联结或改变WHERE子句中的子句的关键字。也称为逻辑操作符(logical operator)。 与AND操作符组合进行过滤 AND操作符用来指示检索满足所有给定条件的行。 检索groupid等于8且hostgroupid介于170和180之间的行: 1234567891011MariaDB [zabbix]&gt; SELECT * FROM hosts_groups WHERE groupid = '8' AND hostgroupid BETWEEN 170 AND 180;+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 175 | 10093 | 8 || 177 | 10095 | 8 || 178 | 10094 | 8 || 179 | 10096 | 8 || 180 | 10097 | 8 |+-------------+--------+---------+5 rows in set (0.00 sec) 与OR操作符组合进行过滤 OR操作符用来指示检索匹配任一给定条件的行。 匹配hostid等于10429或groupid等于21的行: 12345678910111213141516MariaDB [zabbix]&gt; SELECT * FROM hosts_groups WHERE hostid = '10429' OR groupid = '21';+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 457 | 10344 | 21 || 545 | 10388 | 21 || 619 | 10425 | 21 || 621 | 10426 | 21 || 623 | 10427 | 21 || 625 | 10428 | 21 || 626 | 10429 | 5 || 627 | 10429 | 21 || 629 | 10430 | 21 || 631 | 10431 | 21 |+-------------+--------+---------+10 rows in set (0.02 sec) 计算次序 任何时候使用具有AND和OR操作符的WHERE子句，都应该使用圆括号明确地分组操作符。 检索groupid等于4或者12且hostgroupid大于200的行: 123456789MariaDB [zabbix]&gt; SELECT hostgroupid,hostid FROM hosts_groups WHERE ( groupid = '4' OR groupid = '14' ) AND hostgroupid &gt; 200;+-------------+--------+| hostgroupid | hostid |+-------------+--------+| 201 | 10173 || 202 | 10174 || 203 | 10175 |+-------------+--------+3 rows in set (0.00 sec) IN操作符 IN操作符用来指定条件范围，范围中的每个条件都可以进行匹配。 IN的取值由逗号分隔，全都括在圆括号中。 检索groupid等于4和13的所有行: 12345678MariaDB [zabbix]&gt; SELECT * FROM hosts_groups WHERE groupid IN ('4','13');+-------------+--------+---------+| hostgroupid | hostid | groupid |+-------------+--------+---------+| 92 | 10084 | 4 || 190 | 10170 | 13 |+-------------+--------+---------+2 rows in set (0.00 sec) NOT操作符 NOT操作符的功能就是否定它之后所跟的任何条件。 检索interface下type不等于1的行: 12345678MariaDB [zabbix]&gt; SELECT hostid,type FROM interface WHERE not type = '1';+--------+------+| hostid | type |+--------+------+| 10388 | 2 || 10425 | 2 |+--------+------+2 rows in set (0.00 sec) 用通配符进行过滤 通配符是用来匹配值的一部分的特殊字符，本身实际是SQL的WHERE子句中有特殊含义的字符。为在搜索子句中使用通配符，必须使用LIKE操作符。通配符搜索只能用于文本字段(串)。 百分号通配符 ％表示任何字符出现任意次数。可在搜索模式中任意位置使用，并且可以使用多个通配符。 除了一个或多个字符外， %还能匹配0个字符。 %代表搜索模式中给定位置的0个、 1个或多个字符。 12345678MariaDB [zabbix]&gt; SELECT userid,lastaccess FROM sessions WHERE lastaccess LIKE '1%9%';+--------+------------+| userid | lastaccess |+--------+------------+| 1 | 1571994757 || 2 | 1564566998 |+--------+------------+2 rows in set (0.00 sec) 12345678MariaDB [zabbix]&gt; SELECT userid,lastaccess FROM sessions WHERE lastaccess LIKE '%32%';+--------+------------+| userid | lastaccess |+--------+------------+| 2 | 1564583212 || 2 | 1564583221 |+--------+------------+2 rows in set (0.00 sec) 下划线通配符 _只匹配单个字符。 12345678910MariaDB [zabbix]&gt; SELECT hostid,type FROM interface WHERE hostid LIKE '1048_';+--------+------+| hostid | type |+--------+------+| 10480 | 1 || 10481 | 1 || 10482 | 1 || 10483 | 1 |+--------+------+4 rows in set (0.01 sec) 使用正则表达式进行搜索 正则表达式是用来匹配文本的特殊的串（字符集合）。 MySQL中的正则表达式匹配不区分大小写(即，大写和小写都匹配)。为区分大小写，可使用BINARY关键字。 LIKE与REGEXP的差别:LIKE匹配整个列。如果被匹配的文本在列值中出现， LIKE将不会找到它，相应的行也不被返回(除非使用通配符)。而REGEXP在列值内进行匹配，如果被匹配的文本在列值中出现，REGEXP将会找到它，相应的行将被返回。 基本字符匹配 匹配groupid列包含12的行: 123456789101112MariaDB [zabbix]&gt; SELECT groupid FROM hosts_groups WHERE groupid REGEXP '12' ORDER BY groupid;+---------+| groupid |+---------+| 12 || 12 || 12 || 12 || 12 || 12 |+---------+6 rows in set (0.00 sec) 匹配不区分大小写: 1234567MariaDB [test]&gt; SELECT prod_name FROM products WHERE prod_name REGEXP BINARY 'Fuses';+-----------+| prod_name |+-----------+| Fuses |+-----------+1 row in set (0.00 sec) 进行OR匹配 使用”|”匹配多个串之一(或者为这个串，或者为另一个串） 12345678MariaDB [zabbix]&gt; SELECT hostgroupid FROM hosts_groups WHERE hostgroupid REGEXP '630|660' ORDER BY hostgroupid;+-------------+| hostgroupid |+-------------+| 630 || 660 |+-------------+2 rows in set (0.00 sec) 匹配几个字符之一 通过指定一组用”[ ]”括起来的字符来匹配任何单一字符。 除非把字符”|”括在一个集合中，否则它将应用于整个串。字符集合也可以被否定，即，它匹配除指定字符外的任何东西。为否定一个字符集，在集合的开始处放置一个”^”即可。 123456789MariaDB [zabbix]&gt; SELECT alias FROM users WHERE alias REGEXP '[cd]' ORDER BY alias;+--------+| alias |+--------+| Admin || flyabc || flyabd |+--------+3 rows in set (0.00 sec) 123456789MariaDB [zabbix]&gt; SELECT alias FROM users WHERE alias REGEXP 'c|d' ORDER BY alias;+--------+| alias |+--------+| Admin || flyabc || flyabd |+--------+3 rows in set (0.00 sec) 匹配范围 集合可用来定义要匹配的一个或多个字符。[0-9]匹配数字0到9，[a-z]匹配任意字母字符。 123456789MariaDB [zabbix]&gt; SELECT alias FROM users WHERE alias REGEXP '[e-g]' ORDER BY alias;+--------+| alias |+--------+| flyabc || flyabd || guest |+--------+3 rows in set (0.00 sec) 匹配特殊字符 为了匹配特殊字符，必须用”\“为前导。”\-“表示查找”-“，”\.”表示查找”.”。为了匹配反斜杠”\”字符本身，需要使用”\\”。 匹配字符类 类 说明 [:alnum:] 任意字母和数字(同[a-zA-Z0-9]) [:alpha:] 任意字符(同[a-zA-Z]) [:blank:] 空格和制表(同[\t]) [:cntrl:] ASCII控制字符(ASCII 0到31和127) [:digit:] 任意数字(同[0-9]) [:graph:] 与[:print:]相同，但不包括空格 [:lower:] 任意小写字母(同[a-z]) [:print:] 任意可打印字符 [:punct:] 既不在[:alnum:]又不在[:cntrl:]中的任意字符 [:space:] 包括空格在内的任意空白字符(同[\f\n\r\t\v]) [:upper:] 任意大写字母(同[A-Z]) [:xdigit:] 任意十六进制数字(同[a-fA-F0-9]) 匹配多个实例 元字符 说明 * 0个或多个匹配 + 1个或多个匹配(等于{1,}) ? 0个或1个匹配(等于{0,1}) {n} 指定数目的匹配 {n,} 不少于指定数目的匹配 {n,m} 匹配数目的范围(m不超过255) [:digit:]匹配任意数字，{4}确切地要求它前面的字符（任意数字）出现4次 123456789MariaDB [zabbix]&gt; SELECT name FROM applications WHERE name REGEXP '[[:digit:]]&#123;4&#125;' ORDER BY name;+-----------+| name |+-----------+| http-8080 || http-8443 || jk-8009 |+-----------+3 rows in set (0.00 sec) 123456789MariaDB [zabbix]&gt; SELECT name FROM applications WHERE name REGEXP '[0-9][0-9][0-9][0-9]' ORDER BY name;+-----------+| name |+-----------+| http-8080 || http-8443 || jk-8009 |+-----------+3 rows in set (0.00 sec) 定位符 元字符 说明 ^ 文本的开始 $ 文本的结尾 [[:&lt;:]] 词的开始 [[:&gt;:]] 词的结尾 匹配以r结尾的行 12345678910MariaDB [zabbix]&gt; SELECT name FROM applications WHERE name REGEXP '[r]$' ORDER BY name;+-------------------+| name |+-------------------+| Garbage collector || Zabbix server || Zabbix server || Zabbix server |+-------------------+4 rows in set (0.00 sec) 注1 : ^有两种用法，在集合中，用它来否定该集合，否则，用来指串的开始处。 注2 : 可在不使用数据库表的情况下用SELET来测试正则表达式。GEGEXP检查总是返回0(没有匹配)或1(匹配)。相应语法如下:SELECT ‘hello’ REGEXP ‘[0-9]’。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL简用]]></title>
    <url>%2Fblog%2Fmysql1%2F</url>
    <content type="text"><![CDATA[该文档只是一个简单的mysql入门介绍。 登录MySQL 要连接到服务器，在调用mysql时通常需要提供一个MySQL用户名，并且很可能还需要提供一个密码。如果服务器在您登录的计算机以外的计算机上运行，则还需要指定一个主机名。 在本机登录12# mysql -u root -pEnter password: -u : 指定MySQL用户名 -P : 指定密码，在出现”Enter password”提示时输入 在其他主机登录12# mysql -h 192.168.100.200 -u root -pEnter password: -h : 指定MySQL数据库地址 -u : 指定MySQL用户名 -P : 指定密码，在出现”Enter password”提示时输入 简单的输入查询查询数据库版本以及当前日期1234567MariaDB [(none)]&gt; SELECT VERSION(), CURRENT_DATE;+----------------+--------------+| VERSION() | CURRENT_DATE |+----------------+--------------+| 5.5.60-MariaDB | 2019-11-06 |+----------------+--------------+1 row in set (0.00 sec) 以下查询等价: SELECT VERSION(), CURRENT_DATE; select version(), current_date; SeLeCt vErSiOn(), current_DATE; 执行多个语句1234567891011121314MariaDB [(none)]&gt; SELECT VERSION(); SELECT NOW();+----------------+| VERSION() |+----------------+| 5.5.60-MariaDB |+----------------+1 row in set (0.00 sec)+---------------------+| NOW() |+---------------------+| 2019-11-06 20:38:17 |+---------------------+1 row in set (0.00 sec) MySQL所处状态的含义 提示符 含义 &gt; 准备进行新查询 -&gt; 等待多行查询的下一行 ‘&gt; 等待下一行，等待以单引号(‘)开头的字符串的完成 “&gt; 等待下一行，等待以双引号(“)开头的字符串的完成 &gt; 等待下一行，等待以反引号(`)开头的标识符的完成 /*&gt; 等待下一行，等待以/*开头的注释的完成 创建和使用数据库查看数据库1234567891011MariaDB [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || test || zabbix |+--------------------+5 rows in set (0.00 sec) 创建新数据库12MariaDB [(none)]&gt; CREATE DATABASE newtest;Query OK, 1 row affected (0.00 sec) 进入相关数据库1MariaDB [(none)]&gt; use test 在登录MySQL时调用1234# mysql -u root -p testEnter password: ......MariaDB [test]&gt; 创建表查看表123456789101112MariaDB [test]&gt; show tables;+----------------+| Tables_in_test |+----------------+| customers || orderitems || orders || productnotes || products || vendors |+----------------+6 rows in set (0.00 sec) 创建表12MariaDB [test]&gt; CREATE TABLE pet (name VARCHAR(20), owner VARCHAR(20),species VARCHAR(20), sex CHAR(1), birth DATE, death DATE);Query OK, 0 rows affected (0.05 sec) 查看表结构信息123456789101112MariaDB [test]&gt; DESC pet;+---------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------+-------------+------+-----+---------+-------+| name | varchar(20) | YES | | NULL | || owner | varchar(20) | YES | | NULL | || species | varchar(20) | YES | | NULL | || sex | char(1) | YES | | NULL | || birth | date | YES | | NULL | || death | date | YES | | NULL | |+---------+-------------+------+-----+---------+-------+6 rows in set (0.00 sec) 将数据加载到表中123MariaDB [test]&gt; INSERT INTO pet VALUES ('Puffball','Diane','hamster','f','1999-03-30',NULL);Query OK, 1 row affected (0.01 sec) 查看该表数据1234567MariaDB [test]&gt; SELECT * FROM pet;+----------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+-------+| Puffball | Diane | hamster | f | 1999-03-30 | NULL |+----------+-------+---------+------+------------+-------+1 row in set (0.00 sec) 更新表项1234567891011MariaDB [test]&gt; UPDATE pet SET name = 'Tom' WHERE owner = 'Diane';Query OK, 1 row affected (0.02 sec)Rows matched: 1 Changed: 1 Warnings: 0MariaDB [test]&gt; SELECT * FROM pet;+------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+------+-------+---------+------+------------+-------+| Tom | Diane | hamster | f | 1999-03-30 | NULL |+------+-------+---------+------+------------+-------+1 row in set (0.00 sec) 一些语句查看服务器使用的默认命令选项和系统变量值1# mysqltest --verbose --help 如上命令适用于MariaDB，MySQL使用命令mysqld。 显示允许的SHOW语句1MariaDB [(none)]&gt; HELP SHOW; 显示服务器状态信息1MariaDB [(none)]&gt; SHOW STATUS; 显示服务器支持的权限1MariaDB [(none)]&gt; SHOW PRIVILEGES; 显示授予用户(所有用户或特定用户)的安全权限1MariaDB [(none)]&gt; SHOW GRANTS; 显示服务器错误消息1MariaDB [(none)]&gt; SHOW ERRORS; 显示服务器警告消息1MariaDB [(none)]&gt; SHOW WARNINGS; 显示系统中正在运行的所有进程(即当前正在执行的查询)1MariaDB [(none)]&gt; SHOW PROCESSLIST; 显示当前数据库中每个表的相关信息1MariaDB [test]&gt; show table status; 显示系统变量的名称和值1MariaDB [(none)]&gt; SHOW VARIABLEs; 显示服务器所支持的权限1MariaDB [(none)]&gt; SHOW PRIVILEGES; 显示可用的存储引擎和默认存储引擎12345678910111213141516MariaDB [(none)]&gt; SHOW ENGINES;+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+| Engine | Support | Comment | Transactions | XA | Savepoints |+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+| MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO || MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO || CSV | YES | CSV storage engine | NO | NO | NO || BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO || MyISAM | YES | MyISAM storage engine | NO | NO | NO || InnoDB | DEFAULT | Percona-XtraDB, Supports transactions, row-level locking, and foreign keys | YES | YES | YES || ARCHIVE | YES | Archive storage engine | NO | NO | NO || FEDERATED | YES | FederatedX pluggable storage engine | YES | NO | YES || PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO || Aria | YES | Crash-safe tables with MyISAM heritage | NO | NO | NO |+--------------------+---------+----------------------------------------------------------------------------+--------------+------+------------+10 rows in set (0.00 sec) 显示相关服务器插件的信息1MariaDB [(none)]&gt; SHOW PLUGINS; 附: MySQL文档页:https://dev.mysql.com/doc/]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic Stack 7集群在kubernetes上的部署实践]]></title>
    <url>%2Fblog%2Fes%2F</url>
    <content type="text"><![CDATA[默认情况下，ES的节点类型有如下几种: Master-eligible node: 有资格被选为控制群集的主节点(配置: node.master: true) Data node: 数据节点保存数据并执行与数据相关的操作(配置: node.data: true) Ingest node: 进行数据处理的节点(配置: node.ingest: true) Machine learning node: 机器学习节点，用于运行作业和处理机器学习API请求(配置: xpack.ml.enabled: true)(node.ml: true) 详细: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html 这里我先行部署3master、3data、3ingest的ES集群，再部署filebeat、logstash、kibana，同时我这里整合了kafka进行传输。 创建 elasticsearch 集群创建 ES-Master1234567891011121314151617181920212223# vim es-master-config.yaml apiVersion: v1kind: ConfigMapmetadata: name: es-master namespace: kafka labels: component: elasticsearch role: masterdata: elasticsearch.yml: | cluster.name: "es-$&#123;NAMESPACE&#125;" node.name: "$&#123;POD_NAME&#125;" network.host: 0.0.0.0 http.host: 0.0.0.0 transport.host: 0.0.0.0 bootstrap.memory_lock: false discovery.seed_hosts: "es-master" cluster.initial_master_nodes: "es-master-0,es-master-1,es-master-2" node.master: true node.data: false node.ingest: false cluster.remote.connect: false 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# vim es-master.yaml apiVersion: apps/v1kind: StatefulSetmetadata: name: es-master namespace: kafka labels: k8s-app: es-master component: elasticsearch role: masterspec: serviceName: es-master replicas: 3 selector: matchLabels: k8s-app: es-master template: metadata: labels: k8s-app: es-master component: elasticsearch role: master spec: initContainers: - name: es-init image: 192.168.100.100/library/alpine:3.9 command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"] securityContext: privileged: true containers: - name: es-master securityContext: privileged: true capabilities: add: - IPC_LOCK - SYS_RESOURCE image: 192.168.100.100/library/elasticsearch:7.1.0 env: - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ES_JAVA_OPTS value: "-Xms2g -Xmx2g" resources: limits: cpu: '1' memory: 2Gi requests: cpu: '1' memory: 2Gi ports: - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: es-master-config mountPath: /usr/share/elasticsearch/config/elasticsearch.yml subPath: elasticsearch.yml volumes: - name: es-master-config configMap: name: es-master 1234567891011121314151617181920# vim es-master-service.yaml apiVersion: v1kind: Servicemetadata: name: es-master namespace: kafka labels: k8s-app: es-master component: elasticsearch role: masterspec: clusterIP: None ports: - name: transport port: 9300 targetPort: 9300 selector: k8s-app: es-master component: elasticsearch role: master 创建 ES-Data1234567891011121314151617181920212223# vim es-data-config.yaml apiVersion: v1kind: ConfigMapmetadata: name: es-data namespace: kafka labels: component: elasticsearch role: datadata: elasticsearch.yml: | cluster.name: "es-$&#123;NAMESPACE&#125;" node.name: "$&#123;POD_NAME&#125;" network.host: 0.0.0.0 http.host: 0.0.0.0 transport.host: 0.0.0.0 bootstrap.memory_lock: false discovery.seed_hosts: "es-master" cluster.initial_master_nodes: "es-master-0,es-master-1,es-master-2" node.master: false node.data: true node.ingest: false cluster.remote.connect: false 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# vim es-data.yaml apiVersion: apps/v1kind: StatefulSetmetadata: name: es-data namespace: kafka labels: k8s-app: es-data component: elasticsearch role: dataspec: serviceName: es-data replicas: 3 selector: matchLabels: k8s-app: es-data template: metadata: labels: k8s-app: es-data component: elasticsearch role: data spec: securityContext: fsGroup: 1000 initContainers: - name: es-init image: 192.168.100.100/library/alpine:3.9 command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"] securityContext: privileged: true containers: - name: es-data securityContext: privileged: true capabilities: add: - IPC_LOCK - SYS_RESOURCE image: 192.168.100.100/library/elasticsearch:7.1.0 env: - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: ES_JAVA_OPTS value: "-Xms2g -Xmx2g" resources: limits: cpu: '1' memory: 2Gi requests: cpu: '1' memory: 2Gi ports: - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: es-data-config mountPath: /usr/share/elasticsearch/config/elasticsearch.yml subPath: elasticsearch.yml - name: es-data mountPath: /usr/share/elasticsearch/data volumes: - name: es-data-config configMap: name: es-data volumeClaimTemplates: - metadata: name: es-data annotations: volume.beta.kubernetes.io/storage-class: "kafka-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi 1234567891011121314151617181920# vim es-data-service.yaml apiVersion: v1kind: Servicemetadata: name: es-data namespace: kafka labels: k8s-app: es-data component: elasticsearch role: dataspec: clusterIP: None ports: - name: transport port: 9300 targetPort: 9300 selector: k8s-app: es-data component: elasticsearch role: data 创建 ES-Ingest1234567891011121314151617181920212223# vim es-ingest-config.yaml apiVersion: v1kind: ConfigMapmetadata: name: es-ingest namespace: kafka labels: component: elasticsearch role: ingestdata: elasticsearch.yml: | cluster.name: "es-$&#123;NAMESPACE&#125;" node.name: "$&#123;POD_NAME&#125;" network.host: 0.0.0.0 http.host: 0.0.0.0 transport.host: 0.0.0.0 bootstrap.memory_lock: false discovery.seed_hosts: "es-master" cluster.initial_master_nodes: "es-master-0,es-master-1,es-master-2" node.master: false node.data: false node.ingest: true cluster.remote.connect: false 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# vim es-ingest.yaml apiVersion: apps/v1kind: StatefulSetmetadata: name: es-ingest namespace: kafka labels: k8s-app: es-ingest component: elasticsearch role: ingestspec: serviceName: es-ingest replicas: 3 selector: matchLabels: k8s-app: es-ingest template: metadata: labels: k8s-app: es-ingest component: elasticsearch role: ingest spec: initContainers: - name: es-init image: 192.168.100.100/library/alpine:3.9 command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"] securityContext: privileged: true containers: - name: es-ingest securityContext: privileged: true capabilities: add: - IPC_LOCK - SYS_RESOURCE image: 192.168.100.100/library/elasticsearch:7.1.0 env: - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ES_JAVA_OPTS value: "-Xms2g -Xmx2g" resources: limits: cpu: '1' memory: 2Gi requests: cpu: '1' memory: 2Gi ports: - containerPort: 9200 name: http protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: es-ingest-config mountPath: /usr/share/elasticsearch/config/elasticsearch.yml subPath: elasticsearch.yml volumes: - name: es-ingest-config configMap: name: es-ingest 1234567891011121314151617181920# vim es-ingest-service.yaml apiVersion: v1kind: Servicemetadata: name: es-ingest namespace: kafka labels: k8s-app: es-ingest component: elasticsearch role: ingestspec: type: LoadBalancer ports: - name: http port: 9200 targetPort: 9200 selector: k8s-app: es-ingest component: elasticsearch role: ingest 部署 elasticsearch 集群1234567891011121314151617181920212223# kubectl create -f es-master,es-ingest,es-dataconfigmap/es-master createdservice/es-master createddeployment.apps/es-master createdconfigmap/es-ingest createdservice/es-ingest createddeployment.apps/es-ingest createdconfigmap/es-data createdstatefulset.apps/es-data created# kubectl -n kafka get pod|grep eses-data-0 1/1 Running 0 8des-data-1 1/1 Running 0 8des-data-2 1/1 Running 0 8des-ingest-0 1/1 Running 0 8des-ingest-1 1/1 Running 0 8des-ingest-2 1/1 Running 0 8des-master-0 1/1 Running 0 8des-master-1 1/1 Running 0 8des-master-2 1/1 Running 0 8d# kubectl -n kafka get services|grep eses-data ClusterIP None &lt;none&gt; 9300/TCP 8des-ingest LoadBalancer 10.108.81.201 &lt;pending&gt; 9200:32039/TCP 8des-master ClusterIP None &lt;none&gt; 9300/TCP 8d 查看相关信息查看集群健康状况123# curl http://192.168.100.128:32039/_cat/health?vepoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1562396264 06:57:44 es-kafka green 9 3 16 8 0 0 0 0 - 100.0% status: red表示集群故障；yellow表示集群可用但不可靠(单节点即为此状况)；green表示集群正常。 node.total: 集群节点数 node.data: 存储数据的节点数 shards: 分片数 pri: 主分片数 查看集群节点 带*星号表明该节点是主节点；带-表明该节点是从节点。 1234567891011# curl http://192.168.100.128:32039/_cat/nodes?vip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name10.244.2.156 21 96 11 2.29 1.17 1.22 d - es-data-110.244.2.154 22 96 11 2.29 1.17 1.22 i - es-ingest-210.244.2.155 18 96 11 2.29 1.17 1.22 m * es-master-210.244.1.93 18 98 7 0.55 0.33 0.54 m - es-master-110.244.2.152 15 96 11 2.29 1.17 1.22 m - es-master-010.244.1.92 23 98 7 0.55 0.33 0.54 i - es-ingest-110.244.1.94 59 98 7 0.55 0.33 0.54 d - es-data-010.244.2.153 21 96 11 2.29 1.17 1.22 i - es-ingest-010.244.2.157 47 96 11 2.29 1.17 1.22 d - es-data-2 查看索引状况12345# curl http://192.168.100.128:32039/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open .kibana_task_manager Tr36EqEnSDutce244ltNJA 1 1 2 0 42.8kb 21.4kbgreen open pod-2019-07-01 yq7v1rCETlKpeG4--wBGIw 1 1 173919 0 131.8mb 61mbgreen open .kibana_1 ypI9i51BSiaCN1OwMnMaUQ 1 1 5 0 54.4kb 27.2kb 部署Filebeat123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155# vim filebeat.yaml ---apiVersion: v1kind: ConfigMapmetadata: name: filebeat-config namespace: kafka labels: k8s-app: filebeatdata: filebeat.yml: |- filebeat.config: inputs: path: $&#123;path.config&#125;/inputs.d/*.yml reload.enabled: false modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false output.kafka: hosts: ["bootstrap:9092"] topic: '%&#123;[fields.log_topic]&#125;' enabled: true partition.round_robin: reachable_only: false required_acks: 1---apiVersion: v1kind: ConfigMapmetadata: name: filebeat-inputs namespace: kafka labels: k8s-app: filebeatdata: kubernetes.yml: |- - type: docker containers.ids: - "*" processors: - add_kubernetes_metadata: in_cluster: true fields: log_topic: pod---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: filebeat namespace: kafka labels: k8s-app: filebeatspec: template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 containers: - name: filebeat image: 192.168.100.100/library/filebeat:7.1.0 args: [ "-c", "/etc/filebeat.yml", "-e", ] env: - name: KAFKA_HOST value: bootstrap - name: KAFKA_PORT value: "9092" securityContext: runAsUser: 0 resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: inputs configMap: defaultMode: 0600 name: filebeat-inputs - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: filebeatsubjects:- kind: ServiceAccount name: filebeat namespace: kafkaroleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: filebeat labels: k8s-app: filebeatrules:- apiGroups: [""] resources: - namespaces - pods verbs: - get - watch - list---apiVersion: v1kind: ServiceAccountmetadata: name: filebeat namespace: kafka labels: k8s-app: filebeat# kubectl apply -f filebeat.yaml configmap/filebeat-config changedconfigmap/filebeat-inputs changeddaemonset.extensions/filebeat changedclusterrolebinding.rbac.authorization.k8s.io/filebeat changedclusterrole.rbac.authorization.k8s.io/filebeat changedserviceaccount/filebeat changed 关于kafka-output，同时要注意的是这里无法自动创建topic(我在本地进行的伪分布式kafka集群可自动创建topic)，这里可通过kafka-manager进行创建。 部署 Logstash部署 pipeline123456789101112131415161718192021222324252627# vim pipeline-config.yaml apiVersion: v1kind: ConfigMapmetadata: name: pipeline-config namespace: kafkadata: logstash.conf: | input&#123; kafka&#123; bootstrap_servers =&gt; 'bootstrap:9092' topics_pattern =&gt; "pod" consumer_threads =&gt; 3 decorate_events =&gt; true codec =&gt; "json" auto_offset_reset =&gt; "latest" group_id =&gt; "logstash" &#125; &#125; output &#123; elasticsearch &#123; hosts =&gt; ["es-ingest:9200"] index =&gt; "%&#123;[@metadata][topic]&#125;-%&#123;+YYYY-MM-dd&#125;" &#125; &#125;# kubectl apply -f pipeline-config.yaml configmap/pipeline-config configured 关于Kafka-input 关于Elasticsearch-output 部署 logstash1234567891011121314151617181920212223242526272829303132333435363738394041424344# vim logstash.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: logstash namespace: kafka labels: app: logstashspec: replicas: 1 selector: matchLabels: app: logstash template: metadata: labels: app: logstash spec: securityContext: fsGroup: 1000 containers: - name: logstash image: 192.168.100.100/library/logstash:7.1.0 env: - name: XPACK_MONITORING_ENABLED value: "true" - name: xpack.monitoring.elasticsearch.hosts value: "http://es-ingest:9200" resources: limits: cpu: '1' memory: 2Gi requests: cpu: '1' memory: 2Gi volumeMounts: - name: config mountPath: /usr/share/logstash/pipeline/ volumes: - name: config configMap: name: pipeline-config# kubectl apply -f logstash.yaml deployment.apps/logstash changed 部署 Kibana1234567891011121314# vim kibana-config.yaml apiVersion: v1kind: ConfigMapmetadata: name: kibana-config namespace: kafkadata: kibana.yml: | server.name: kibana server.port: 5601 server.host: "0.0.0.0" xpack.monitoring.ui.container.elasticsearch.enabled: true# kubectl apply -f kibana-config.yaml configmap/kibana-config changed 123456789101112131415161718192021222324252627282930313233343536373839404142434445# vim kibana.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: kibana namespace: kafka labels: k8s-app: kibanaspec: replicas: 1 selector: matchLabels: k8s-app: kibana template: metadata: labels: k8s-app: kibana spec: containers: - name: kibana image: 192.168.100.100/library/kibana:7.1.0 resources: requests: memory: 2Gi cpu: 1 limits: memory: 4Gi cpu: 1 env: - name: ELASTICSEARCH_HOSTS value: "http://es-ingest:9200" ports: - containerPort: 5601 name: http protocol: TCP volumeMounts: - name: config mountPath: /usr/share/kibana/config/kibana.yml subPath: kibana.yml volumes: - name: config configMap: name: kibana-config# kubectl apply -f kibana.yaml deployment.apps/kibana configured 123456789101112131415161718# vim kibana-service.yaml apiVersion: v1kind: Servicemetadata: name: kibana namespace: kafka labels: k8s-app: kibanaspec: type: LoadBalancer ports: - port: 5601 protocol: TCP targetPort: http selector: k8s-app: kibana# kubectl apply -f kibana-service.yaml service/kibana changed 进行查看查看pod和service12345678910111213141516171819# kubectl get pod -n kafka | egrep "filebeat|logstash|es|kibana"es-data-0 1/1 Running 0 9des-data-1 1/1 Running 0 9des-data-2 1/1 Running 0 9des-ingest-0 1/1 Running 0 9des-ingest-1 1/1 Running 0 9des-ingest-2 1/1 Running 0 9des-master-0 1/1 Running 0 9des-master-1 1/1 Running 0 9des-master-2 1/1 Running 0 9dfilebeat-skwf8 1/1 Running 0 9dfilebeat-zmjs7 1/1 Running 0 9dkibana-fc98f5b47-hml5g 1/1 Running 0 9dlogstash-5d86f89fdc-x5jgh 1/1 Running 0 9d# kubectl get service -n kafka | egrep "filebeat|logstash|es|kibana"es-data ClusterIP None &lt;none&gt; 9300/TCP 9des-ingest LoadBalancer 10.108.81.201 &lt;pending&gt; 9200:32039/TCP 9des-master ClusterIP None &lt;none&gt; 9300/TCP 9dkibana LoadBalancer 10.105.150.55 &lt;pending&gt; 5601:32431/TCP 9d 查看elasticsearch是否收录索引1234567891011# curl http://192.168.100.128:32039/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open pod-2019-07-05 eG9vVDWlRGqVJBxIqznoHg 1 1 357001 0 196.1mb 97.5mbgreen open pod-2019-07-03 RIGZT8VpQVCBAieZU0mPbw 1 1 266771 125 199.9mb 99.9mbgreen open pod-2019-07-06 sLSU4y6YSoO8rc1dJFKhmw 1 1 338327 0 229.9mb 114.9mbgreen open .kibana_task_manager Tr36EqEnSDutce244ltNJA 1 1 2 0 42.8kb 21.4kbgreen open pod-2019-07-04 vOm3X34xR6CxOVz2YpHzww 1 1 333639 0 226.3mb 106.2mbgreen open pod-2019-07-07 GaUrBGjkR_asjOl8xnv74Q 1 1 19 0 27mb 13.5mbgreen open pod-2019-07-02 menu1_B6TdGv4rHhYY2DDA 1 1 505044 0 262.2mb 131.1mbgreen open pod-2019-07-01 yq7v1rCETlKpeG4--wBGIw 1 1 173919 0 131.8mb 61mbgreen open .kibana_1 ypI9i51BSiaCN1OwMnMaUQ 1 1 5 0 54.4kb 27.2kb 登录kibana 创建index 查看]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>es</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes部署kafka集群]]></title>
    <url>%2Fblog%2Fkafka%2F</url>
    <content type="text"><![CDATA[该部署实践是从kubernetes-kafka处获得，如不想看可直接前往。 创建 Namespace1# kubectl create namespace kafka 创建 RBAC1234# cat rbac-namespace-default/kustomization.yaml resources:- node-reader.yml- pod-labler.yml 12345678910111213141516171819202122232425262728293031323334# cat rbac-namespace-default/node-reader.yml # To see if init containers need RBAC:## $ kubectl -n kafka exec kafka-0 -- cat /etc/kafka/server.properties | grep broker.rack# #init#broker.rack=# zone lookup failed, see -c init-config logs# $ kubectl -n kafka logs -c init-config kafka-0# ++ kubectl get node some-node '-o=go-template=&#123;&#123;index .metadata.labels "failure-domain.beta.kubernetes.io/zone"&#125;&#125;'# Error from server (Forbidden): User "system:serviceaccount:kafka:default" cannot get nodes at the cluster scope.: "Unknown user \"system:serviceaccount:kafka:default\""#---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: node-readerrules:- apiGroups: - "" resources: - nodes verbs: - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kafka-node-readerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: node-readersubjects:- kind: ServiceAccount name: default namespace: kafka 123456789101112131415161718192021222324252627282930313233343536# cat rbac-namespace-default/pod-labler.yml # To see if init containers need RBAC:## $ kubectl -n kafka logs kafka-2 -c init-config# ...# Error from server (Forbidden): pods "kafka-2" is forbidden: User "system:serviceaccount:kafka:default" cannot get pods in the namespace "kafka": Unknown user "system:serviceaccount:kafka:default"#---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: pod-labler namespace: kafkarules:- apiGroups: - "" resources: - pods verbs: - get - update - patch---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kafka-pod-labler namespace: kafkaroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: pod-lablersubjects:- kind: ServiceAccount name: default namespace: kafka 12345# kubectl apply -k rbac-namespace-defaultrole.rbac.authorization.k8s.io/pod-labler createdclusterrole.rbac.authorization.k8s.io/node-reader createdrolebinding.rbac.authorization.k8s.io/kafka-pod-labler createdclusterrolebinding.rbac.authorization.k8s.io/kafka-node-reader created 部署 Zookeeper12345678# cat zookeeper/kustomization.yaml resources:- 10zookeeper-config.yml- 20pzoo-service.yml- 21zoo-service.yml- 30service.yml- 50pzoo.yml- 51zoo.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# cat zookeeper/10zookeeper-config.yml apiVersion: v1kind: ConfigMapmetadata: name: zookeeper-config namespace: kafkadata: init.sh: |- #!/bin/bash set -e set -x [ -d /var/lib/zookeeper/data ] || mkdir /var/lib/zookeeper/data [ -z "$ID_OFFSET" ] &amp;&amp; ID_OFFSET=1 export ZOOKEEPER_SERVER_ID=$(($&#123;HOSTNAME##*-&#125; + $ID_OFFSET)) echo "$&#123;ZOOKEEPER_SERVER_ID:-1&#125;" | tee /var/lib/zookeeper/data/myid cp -Lur /etc/kafka-configmap/* /etc/kafka/ [ ! -z "$PZOO_REPLICAS" ] &amp;&amp; [ ! -z "$ZOO_REPLICAS" ] &amp;&amp; &#123; sed -i "s/^server\\./#server./" /etc/kafka/zookeeper.properties for N in $(seq $PZOO_REPLICAS); do echo "server.$N=pzoo-$(( $N - 1 )).pzoo:2888:3888:participant" &gt;&gt; /etc/kafka/zookeeper.properties; done for N in $(seq $ZOO_REPLICAS); do echo "server.$(( $PZOO_REPLICAS + $N ))=zoo-$(( $N - 1 )).zoo:2888:3888:participant" &gt;&gt; /etc/kafka/zookeeper.properties; done &#125; sed -i "s/server\.$ZOOKEEPER_SERVER_ID\=[a-z0-9.-]*/server.$ZOOKEEPER_SERVER_ID=0.0.0.0/" /etc/kafka/zookeeper.properties zookeeper.properties: | tickTime=2000 dataDir=/var/lib/zookeeper/data dataLogDir=/var/lib/zookeeper/log clientPort=2181 maxClientCnxns=1 initLimit=5 syncLimit=2 server.1=pzoo-0.pzoo:2888:3888:participant server.2=pzoo-1.pzoo:2888:3888:participant server.3=pzoo-2.pzoo:2888:3888:participant server.4=zoo-0.zoo:2888:3888:participant server.5=zoo-1.zoo:2888:3888:participant log4j.properties: |- log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n # Suppress connection log messages, three lines per livenessProbe execution log4j.logger.org.apache.zookeeper.server.NIOServerCnxnFactory=WARN log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN 123456789101112131415# cat zookeeper/20pzoo-service.yml apiVersion: v1kind: Servicemetadata: name: pzoo namespace: kafkaspec: ports: - port: 2888 name: peer - port: 3888 name: leader-election clusterIP: None selector: app: zookeeper 123456789101112131415# cat zookeeper/21zoo-service.yml apiVersion: v1kind: Servicemetadata: name: zoo namespace: kafkaspec: ports: - port: 2888 name: peer - port: 3888 name: leader-election clusterIP: None selector: app: zookeeper 123456789101112# cat zookeeper/30service.yml apiVersion: v1kind: Servicemetadata: name: zookeeper namespace: kafkaspec: ports: - port: 2181 name: client selector: app: zookeeper 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# cat zookeeper/50pzoo.yml apiVersion: apps/v1kind: StatefulSetmetadata: name: pzoo namespace: kafkaspec: selector: matchLabels: app: zookeeper serviceName: "pzoo" replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: Parallel template: metadata: labels: app: zookeeper spec: terminationGracePeriodSeconds: 10 initContainers: - name: init-config image: 192.168.100.100/kafka/kafka-initutils:latest command: ['/bin/bash', '/etc/kafka-configmap/init.sh'] volumeMounts: - name: configmap mountPath: /etc/kafka-configmap - name: config mountPath: /etc/kafka - name: data mountPath: /var/lib/zookeeper containers: - name: zookeeper image: 192.168.100.100/kafka/kafka:2.2.0 env: - name: KAFKA_LOG4J_OPTS value: -Dlog4j.configuration=file:/etc/kafka/log4j.properties command: - ./bin/zookeeper-server-start.sh - /etc/kafka/zookeeper.properties lifecycle: preStop: exec: command: ["sh", "-ce", "kill -s TERM 1; while $(kill -0 1 2&gt;/dev/null); do sleep 1; done"] ports: - containerPort: 2181 name: client - containerPort: 2888 name: peer - containerPort: 3888 name: leader-election resources: requests: cpu: 10m memory: 100Mi limits: memory: 120Mi readinessProbe: exec: command: - /bin/sh - -c - '[ "imok" = "$(echo ruok | nc -w 1 -q 1 127.0.0.1 2181)" ]' volumeMounts: - name: config mountPath: /etc/kafka - name: data mountPath: /var/lib/zookeeper volumes: - name: configmap configMap: name: zookeeper-config - name: config emptyDir: &#123;&#125; volumeClaimTemplates: - metadata: name: data spec: accessModes: [ "ReadWriteOnce" ] storageClassName: kafka-rbd resources: requests: storage: 1Gi 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# cat zookeeper/51zoo.yml apiVersion: apps/v1kind: StatefulSetmetadata: name: zoo namespace: kafkaspec: selector: matchLabels: app: zookeeper serviceName: "zoo" replicas: 2 updateStrategy: type: RollingUpdate podManagementPolicy: Parallel template: metadata: labels: app: zookeeper spec: terminationGracePeriodSeconds: 10 initContainers: - name: init-config image: 192.168.100.100/kafka/kafka-initutils:latest command: ['/bin/bash', '/etc/kafka-configmap/init.sh'] env: - name: ID_OFFSET value: "4" volumeMounts: - name: configmap mountPath: /etc/kafka-configmap - name: config mountPath: /etc/kafka - name: data mountPath: /var/lib/zookeeper containers: - name: zookeeper image: 192.168.100.100/kafka/kafka:2.2.0 env: - name: KAFKA_LOG4J_OPTS value: -Dlog4j.configuration=file:/etc/kafka/log4j.properties command: - ./bin/zookeeper-server-start.sh - /etc/kafka/zookeeper.properties lifecycle: preStop: exec: command: ["sh", "-ce", "kill -s TERM 1; while $(kill -0 1 2&gt;/dev/null); do sleep 1; done"] ports: - containerPort: 2181 name: client - containerPort: 2888 name: peer - containerPort: 3888 name: leader-election resources: requests: cpu: 10m memory: 100Mi limits: memory: 120Mi readinessProbe: exec: command: - /bin/sh - -c - '[ "imok" = "$(echo ruok | nc -w 1 -q 1 127.0.0.1 2181)" ]' volumeMounts: - name: config mountPath: /etc/kafka - name: data mountPath: /var/lib/zookeeper volumes: - name: configmap configMap: name: zookeeper-config - name: config emptyDir: &#123;&#125; volumeClaimTemplates: - metadata: name: data spec: accessModes: [ "ReadWriteOnce" ] storageClassName: kafka-rbd resources: requests: storage: 1Gi 1234567891011121314# kubectl apply -k zookeeperconfigmap/zookeeper-config createdservice/pzoo createdservice/zoo createdservice/zookeeper createdstatefulset.apps/pzoo createdstatefulset.apps/zoo created# kubectl get pod -n kafkaNAME READY STATUS RESTARTS AGEpzoo-0 1/1 Running 0 24mpzoo-1 1/1 Running 0 24mpzoo-2 1/1 Running 0 24mzoo-0 1/1 Running 0 24mzoo-1 1/1 Running 0 24m 部署 Kafka123456# cat kafka/kustomization.yaml resources:- 10broker-config.yml- 20dns.yml- 30bootstrap-service.yml- 50kafka.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257# cat kafka/10broker-config.yml apiVersion: v1kind: ConfigMapmetadata: name: broker-config namespace: kafkadata: init.sh: |- #!/bin/bash set -e set -x cp /etc/kafka-configmap/log4j.properties /etc/kafka/ KAFKA_BROKER_ID=$&#123;HOSTNAME##*-&#125; SEDS=("s/#init#broker.id=#init#/broker.id=$KAFKA_BROKER_ID/") LABELS="kafka-broker-id=$KAFKA_BROKER_ID" ANNOTATIONS="" hash kubectl 2&gt;/dev/null || &#123; SEDS+=("s/#init#broker.rack=#init#/#init#broker.rack=# kubectl not found in path/") &#125; &amp;&amp; &#123; ZONE=$(kubectl get node "$NODE_NAME" -o=go-template='&#123;&#123;index .metadata.labels "failure-domain.beta.kubernetes.io/zone"&#125;&#125;') if [ "x$ZONE" == "x&lt;no value&gt;" ]; then SEDS+=("s/#init#broker.rack=#init#/#init#broker.rack=# zone label not found for node $NODE_NAME/") else SEDS+=("s/#init#broker.rack=#init#/broker.rack=$ZONE/") LABELS="$LABELS kafka-broker-rack=$ZONE" fi OUTSIDE_HOST=$(kubectl get node "$NODE_NAME" -o jsonpath='&#123;.status.addresses[?(@.type=="InternalIP")].address&#125;') OUTSIDE_PORT=3240$&#123;KAFKA_BROKER_ID&#125; SEDS+=("s|#init#advertised.listeners=PLAINTEXT://#init#|advertised.listeners=PLAINTEXT://:9092,OUTSIDE://$&#123;OUTSIDE_HOST&#125;:$&#123;OUTSIDE_PORT&#125;|") ANNOTATIONS="$ANNOTATIONS kafka-listener-outside-host=$OUTSIDE_HOST kafka-listener-outside-port=$OUTSIDE_PORT" if [ ! -z "$LABELS" ]; then kubectl -n $POD_NAMESPACE label pod $POD_NAME $LABELS || echo "Failed to label $POD_NAMESPACE.$POD_NAME - RBAC issue?" fi if [ ! -z "$ANNOTATIONS" ]; then kubectl -n $POD_NAMESPACE annotate pod $POD_NAME $ANNOTATIONS || echo "Failed to annotate $POD_NAMESPACE.$POD_NAME - RBAC issue?" fi &#125; printf '%s\n' "$&#123;SEDS[@]&#125;" | sed -f - /etc/kafka-configmap/server.properties &gt; /etc/kafka/server.properties.tmp [ $? -eq 0 ] &amp;&amp; mv /etc/kafka/server.properties.tmp /etc/kafka/server.properties server.properties: |- ############################# Log Basics ############################# # A comma seperated list of directories under which to store log files # Overrides log.dir log.dirs=/var/lib/kafka/data/topics # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. num.partitions=12 default.replication.factor=3 min.insync.replicas=2 auto.create.topics.enable=false # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. #num.recovery.threads.per.data.dir=1 ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. #init#broker.id=#init# #init#broker.rack=#init# ############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 #listeners=PLAINTEXT://:9092 listeners=PLAINTEXT://:9092,OUTSIDE://:9094 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for "listeners" if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). #advertised.listeners=PLAINTEXT://your.host.name:9092 #init#advertised.listeners=PLAINTEXT://#init# # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL,OUTSIDE:PLAINTEXT inter.broker.listener.name=PLAINTEXT # The number of threads that the server uses for receiving requests from the network and sending responses to the network #num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O #num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server #socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server #socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) #socket.request.max.bytes=104857600 ############################# Internal Topic Settings ############################# # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state" # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3. #offsets.topic.replication.factor=1 #transaction.state.log.replication.factor=1 #transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 1. Durability: Unflushed data may be lost if you are not using replication. # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks. # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # The number of messages to accept before forcing a flush of data to disk #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # https://cwiki.apache.org/confluence/display/KAFKA/KIP-186%3A+Increase+offsets+retention+default+to+7+days offsets.retention.minutes=10080 # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=-1 # A size-based retention policy for logs. Segments are pruned from the log unless the remaining # segments drop below log.retention.bytes. Functions independently of log.retention.hours. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. #log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies #log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. zookeeper.connect=zookeeper:2181 # Timeout in ms for connecting to zookeeper #zookeeper.connection.timeout.ms=6000 ############################# Group Coordinator Settings ############################# # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. #group.initial.rebalance.delay.ms=0 log4j.properties: |- # Unspecified loggers and loggers with additivity=true output to server.log and stdout # Note that INFO only applies to unspecified loggers, the log level of the child logger is used otherwise log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.kafkaAppender=org.apache.log4j.DailyRollingFileAppender log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.kafkaAppender.File=$&#123;kafka.logs.dir&#125;/server.log log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout log4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.stateChangeAppender=org.apache.log4j.DailyRollingFileAppender log4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.stateChangeAppender.File=$&#123;kafka.logs.dir&#125;/state-change.log log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender log4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.requestAppender.File=$&#123;kafka.logs.dir&#125;/kafka-request.log log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout log4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender log4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.cleanerAppender.File=$&#123;kafka.logs.dir&#125;/log-cleaner.log log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender log4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.controllerAppender.File=$&#123;kafka.logs.dir&#125;/controller.log log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.authorizerAppender=org.apache.log4j.DailyRollingFileAppender log4j.appender.authorizerAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.authorizerAppender.File=$&#123;kafka.logs.dir&#125;/kafka-authorizer.log log4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n # Change the two lines below to adjust ZK client logging log4j.logger.org.I0Itec.zkclient.ZkClient=INFO log4j.logger.org.apache.zookeeper=INFO # Change the two lines below to adjust the general broker logging level (output to server.log and stdout) log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO # Change to DEBUG or TRACE to enable request logging log4j.logger.kafka.request.logger=WARN, requestAppender log4j.additivity.kafka.request.logger=false # Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE for additional output # related to the handling of requests #log4j.logger.kafka.network.Processor=TRACE, requestAppender #log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender #log4j.additivity.kafka.server.KafkaApis=false log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender log4j.additivity.kafka.network.RequestChannel$=false log4j.logger.kafka.controller=TRACE, controllerAppender log4j.additivity.kafka.controller=false log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender log4j.additivity.kafka.log.LogCleaner=false log4j.logger.state.change.logger=TRACE, stateChangeAppender log4j.additivity.state.change.logger=false # Change to DEBUG to enable audit log for the authorizer log4j.logger.kafka.authorizer.logger=WARN, authorizerAppender log4j.additivity.kafka.authorizer.logger=false 123456789101112# cat kafka/20dns.yml apiVersion: v1kind: Servicemetadata: name: broker namespace: kafkaspec: clusterIP: None ports: - port: 9092 selector: app: kafka 123456789101112# cat kafka/30bootstrap-service.yml apiVersion: v1kind: Servicemetadata: name: bootstrap namespace: kafkaspec: type: LoadBalancer ports: - port: 9092 selector: app: kafka 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# cat kafka/50kafka.yml apiVersion: apps/v1kind: StatefulSetmetadata: name: kafka namespace: kafkaspec: selector: matchLabels: app: kafka serviceName: "broker" replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: Parallel template: metadata: labels: app: kafka spec: terminationGracePeriodSeconds: 30 initContainers: - name: init-config image: 192.168.100.100/kafka/kafka-initutils:latest env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace command: ['/bin/bash', '/etc/kafka-configmap/init.sh'] volumeMounts: - name: configmap mountPath: /etc/kafka-configmap - name: config mountPath: /etc/kafka - name: extensions mountPath: /opt/kafka/libs/extensions containers: - name: broker image: 192.168.100.100/kafka/kafka:2.2.0 env: - name: CLASSPATH value: /opt/kafka/libs/extensions/* - name: KAFKA_LOG4J_OPTS value: -Dlog4j.configuration=file:/etc/kafka/log4j.properties - name: JMX_PORT value: "5555" ports: - name: inside containerPort: 9092 - name: outside containerPort: 9094 - name: jmx containerPort: 5555 command: - ./bin/kafka-server-start.sh - /etc/kafka/server.properties lifecycle: preStop: exec: command: ["sh", "-ce", "kill -s TERM 1; while $(kill -0 1 2&gt;/dev/null); do sleep 1; done"] resources: requests: cpu: 100m memory: 100Mi limits: # This limit was intentionally set low as a reminder that # the entire Yolean/kubernetes-kafka is meant to be tweaked # before you run production workloads memory: 600Mi readinessProbe: tcpSocket: port: 9092 timeoutSeconds: 1 volumeMounts: - name: config mountPath: /etc/kafka - name: data mountPath: /var/lib/kafka/data - name: extensions mountPath: /opt/kafka/libs/extensions volumes: - name: configmap configMap: name: broker-config - name: config emptyDir: &#123;&#125; - name: extensions emptyDir: &#123;&#125; volumeClaimTemplates: - metadata: name: data spec: accessModes: [ "ReadWriteOnce" ] storageClassName: kafka-rbd resources: requests: storage: 5Gi 12345678910# kubectl apply -k kafkaconfigmap/broker-config createdservice/broker createdservice/bootstrap createdstatefulset.apps/kafka created# kubectl get pod -n kafkaNAME READY STATUS RESTARTS AGEkafka-0 1/1 Running 0 10mkafka-1 1/1 Running 0 10mkafka-2 1/1 Running 0 10m 部署 yahoo-kafka-manager123456789101112131415161718192021222324252627# cat yahoo-kafka-manager/kafka-manager.yml apiVersion: apps/v1kind: Deploymentmetadata: name: kafka-manager namespace: kafkaspec: replicas: 1 selector: matchLabels: app: kafka-manager template: metadata: labels: app: kafka-manager spec: containers: - name: kafka-manager image: 192.168.100.100/kafka/kafka-manager:2.2.0 ports: - containerPort: 80 env: - name: ZK_HOSTS value: zookeeper.kafka:2181 command: - ./bin/kafka-manager - -Dhttp.port=80 1234567891011121314# cat yahoo-kafka-manager/kafka-manager-service.yml apiVersion: v1kind: Servicemetadata: name: kafka-manager namespace: kafkaspec: selector: app: kafka-manager type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 123456# kubectl apply -k yahoo-kafka-managerservice/kafka-manager createddeployment.apps/kafka-manager created# kubectl get pod -n kafkaNAME READY STATUS RESTARTS AGEkafka-manager-67574848d7-4w69r 1/1 Running 0 115s 查看相关信息123456789101112131415# kubectl -n kafka get pod | egrep "zoo|kafka"kafka-0 1/1 Running 0 1dkafka-1 1/1 Running 0 1dkafka-2 1/1 Running 0 1dkafka-manager-64fc99c564-8b89l 1/1 Running 0 1dpzoo-0 1/1 Running 0 1dpzoo-1 1/1 Running 0 1dpzoo-2 1/1 Running 0 1dzoo-0 1/1 Running 0 1dzoo-1 1/1 Running 0 1d# kubectl -n kafka get service | egrep "zoo|kafka"kafka-manager LoadBalancer 10.101.2.121 &lt;pending&gt; 80:30644/TCP 1dpzoo ClusterIP None &lt;none&gt; 2888/TCP,3888/TCP 1dzoo ClusterIP None &lt;none&gt; 2888/TCP,3888/TCP 1dzookeeper ClusterIP 10.102.1.148 &lt;none&gt; 2181/TCP 1d 登录 Kafka-Manager 如下图为已创建好的kafka集群 如下图为该集群相关信息 如下图为该集群相关topic相关信息]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Drone + Gogs 执行自动构建]]></title>
    <url>%2Fblog%2Fdrone%2F</url>
    <content type="text"><![CDATA[Drone 是一种基于容器技术的持续交付系统。Drone 使用YAML配置文件来定义和执行Docker容器中的Pipelines。 Gogs 是一个用Go编写的轻量级git服务器，它设计简单，易于设置和操作。Gogs提供存储库文件查看和编辑，项目问题跟踪以及项目文档的内置wiki。 Drone 作为一个CI工具，当然会出现与Jenkins的对比，从docker镜像来看，Jenkins的镜像达到了702M，而Drone的最新的docker镜像才62.1M。最重要的是drone使用的是yaml文件来进行构建，这对使用docker的人来说是相当友好的，而Jenkins的pipeline的编写对大多数人来说都是一种挑战。就像Jenkins官方所说的一样，“Jenkins是开源CI&amp;CD软件领导者， 提供超过1000个插件来支持构建、部署、自动化， 满足任何项目的需要。”，而drone所提供的插件相对来说很少，只有百十个的样子，所以这也是Jenkins的优势。 当然，对于Gitlab-runner，它也有同样的问题，gitlab-runner的v11.9.0的镜像达到了403M，而且Gitlab-runner是与Gitlab深度集成的，如果你使用了Gitlab，那这很好；而如果你使用的是GitHub这些源代码管理系统，那这就是问题了。 Drone适用于在Docker容器内运行的任何语言，数据库或服务。Drone使用容器将预先配置的步骤放入pipeline中。从现有插件或通过创建自己的插件来进行构建。当然，正如前面说的，drone使用yaml来定义pipeline，它的语法简洁明了，很容易上手。关于drone的更多介绍详见:https://drone.io/。 还是从外在的地方来对比，我使用的Gitlab-CE的11.7.5版本，docker镜像达到了1.59G，而最新版的gogs，它的镜像才99.1M，足够轻量级。Gogs在GitHub上的star也突破了三万，这也证明了gogs的受欢迎程度，同时gogs对中文支持很好。其他不再细讲，Gitlab官方也对Gogs做了对比，详见:https://about.gitlab.com/devops-tools/gogs-vs-gitlab.html。 这里对于Drone和Gogs的部署是使用docker-compose单机模式部署的。在使用docker-compose部署之前，我首先想的是将drone部署在kubernetes上，但是drone对此的支持不是很好，drone需要可路由的域名，无论是使用traefik部署的内部域名，还是service，Gitlab和Gogs都无法连接该域名或service，这就导致Gitlab和Gogs无法将代码推送到drone。drone的文档也还不够丰富，我也查阅了官方论坛，依然没能解决，所以很遗憾。 配置 docker-compose123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# vim docker-compose.yaml version: '3.5'services: drone-server: container_name: drone-server image: 192.168.100.100/drone/drone:1.1.0 restart: always labels: - "traefik.docker.network: traefik" - "traefik.frontend.rule=Host:drone.flywzj.com" networks: traefik: aliases: - drone.flywzj.com ports: - "18080:80" - "4433:443" volumes: - /data/drone:/data - /var/run/docker.sock:/var/run/docker.sock environment: - DRONE_GIT_ALWAYS_AUTH=false - DRONE_RUNNER_CAPACITY=2 - DRONE_RUNNER_NETWORKS=traefik - DRONE_SERVER_HOST=drone-server - DRONE_SERVER_PROTO=http - DRONE_LOGS_DEBUG=true - DRONE_TLS_AUTOCERT=false - DRONE_GOGS_SKIP_VERIFY=false - DRONE_GOGS_SERVER=http://gogs.flywzj.com:3000 - DRONE_PROVIDER=gogs - DRONE_DATABASE_DATASOURCE=/data/drone.sqlite - DRONE_DATABASE_DRIVER=sqlite3 gogs: container_name: gogs image: 192.168.100.100/gogs/gogs:latest restart: always labels: - "traefik.docker.network: traefik" - "traefik.frontend.rule=Host:gogs.flywzj.com" - "traefik.http.port=3000" - "traefik.ssh.port=22" networks: traefik: aliases: - gogs.flywzj.com ports: - "10022:22" - "3000:3000" volumes: - /data/gogs:/data depends_on: - mysql mysql: container_name: mysql image: 192.168.100.100/library/mysql:5.7 restart: always labels: - "traefik.docker.network: traefik" networks: - traefik volumes: - /data/mysql:/var/lib/mysql - /var/run/docker.sock:/var/run/docker.sock ports: - "3306:3306" command: --explicit_defaults_for_timestamp=true --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci environment: MYSQL_ROOT_PASSWORD: 'passw0rd' MYSQL_DATABASE: 'gogs' MYSQL_USER: 'gogs' MYSQL_PASSWORD: 'gogs' TZ: Asia/Shanghai traefik-proxy: container_name: traefik image: 192.168.100.100/library/traefik:1.7 restart: always labels: - "traefik.docker.network: traefik" networks: - traefik command: --api --docker ports: - "80:80" - "8080:8080" volumes: - /var/run/docker.sock:/var/run/docker.socknetworks: traefik: name: traefik external: false 部署 docker-compose12345678910111213# docker-compose up -dCreating mysql ... doneCreating gogs ... doneCreating drone-server ... Creating traefik ... Creating gogs ... # docker-compose ps Name Command State Ports -----------------------------------------------------------------------------------------------------drone-server /bin/drone-server Up 0.0.0.0:4433-&gt;443/tcp, 0.0.0.0:18080-&gt;80/tcp gogs /app/gogs/docker/start.sh ... Up 0.0.0.0:10022-&gt;22/tcp, 0.0.0.0:3000-&gt;3000/tcpmysql docker-entrypoint.sh --exp ... Up 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp traefik /traefik --api --docker Up 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:8080-&gt;8080/tcp 部署 Gogs 这里通过traefik配置的gogs.flywzj.com:3000即可登录配置界面，安装后创建一个新的仓库 注: 数据库主机 —-&gt; mysql:3306 (这里的mysql是container_name) 启用 Drone 这里通过traefik配置的drone.flywzj.com即可登录，之后输入在gogs上创建的账号和密码即可登录，登录后drone会自动进行同步 点击”ACTIVATE”激活该代码库 进入该代码库，点击”SETTINGS”之后，点击”ACTIVATE REPOSITORY”激活储存库 激活后配置项如下图所示 推送代码到 Gogs 注:新建的代码库是无法直接在gogs的web端进行编辑的，需要从远端推送一次后才能在gogs的web端进行编辑 123456789101112131415161718192021222324252627# git clone http://gogs.flywzj.com:3000/zhi/test.git# cd test/# vim DockerfileFROM alpineRUN echo hello-world# vim .drone.ymlkind: pipelinename: defaultsteps:- name: build image: plugins/docker settings: username: from_secret: docker_username password: from_secret: docker_password repo: wangzhijian/test tags: latest dry_run: true# git add --all .# git commit -m "update"[master（根提交） 7dbb251] update 2 files changed, 14 insertions(+) create mode 100644 .drone.yml create mode 100644 Dockerfile# git push -u origin master 注:”from_secret”里的密钥需在”SETTINGS” ===&gt; “Secrets” 下进行配置；使用 “dry_run: true” 将不推送到镜像仓库 查看 Drone 构建中的状态显示如下: 构建完成显示如下: 点击即可进入查看构建的详细信息: 附: docker-compose文档:https://docs.docker.com/compose/compose-file/ traefik文档页:https://docs.traefik.io/ gogs的GitHub:https://github.com/gogs/gogs drone的GitHub:https://github.com/drone/drone drone文档页:https://docs.drone.io/ drone的插件:http://plugins.drone.io/ drone官方构建示例代码:https://github.com/drone/hello-world/tree/test-docker-plugin drone官方构建示例:https://cloud.drone.io/drone/hello-world/7/1/2]]></content>
      <categories>
        <category>CI</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>gogs</tag>
        <tag>drone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在kubernetes上部署GitLab Runner]]></title>
    <url>%2Fblog%2Frunner%2F</url>
    <content type="text"><![CDATA[GitLab Runner是一个开源项目，用于运行您的jobs并将结果发送回GitLab。它与GitLab CI一起使用，GitLab CI是GitLab随附的开源持续集成服务，用于协调jobs。 注册Runner查看配置信息在Kubernetes上设置Gitlab Runner的第一步是获取身份验证令牌。此令牌非常重要，因为它会向Gitlab验证您的跑步者。要注册Runner，我们需要从Gitlab获取配置详细信息并完成Runner的注册过程。 这里我是直接使用root账户来注册的，查看位于Admin Area &gt; Runners 的“Set up a shared Runner manually”(手动设置共享Runner)，这里包含使用Gitlab注册新Runner所需的配置详细信息，记住URL和token。 注意: 使用其他普通账户可以注册Specific Runners(特定的runner)，配置信息位于相关项目的Settings &gt; CI/CD &gt; Runners 下。 注册Runner接下来，我们需要完成注册过程以连接新的Runner。注册Runner的最简单方法是使用Runner 在本地启动Docker容器来进行注册: 123456789101112131415161718192021# docker run --rm -it --entrypoint /bin/bash gitlab/gitlab-runner:latestroot@ceb637716b3c:/# gitlab-runner register ##注册runnerRuntime platform arch=amd64 os=linux pid=30 revision=692ae235 version=11.9.0Running in system-mode. Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/):http://192.168.100.128:32626/ ##配置gitlab-ci的URL，通常为域名Please enter the gitlab-ci token for this runner:5cy-91nwWtcqsuNXWZmF ##输入gitlab-ci tokenPlease enter the gitlab-ci description for this runner:[ceb637716b3c]: Kubernetes Gitlab-Runner ##输入gitlab-ci的描述Please enter the gitlab-ci tags for this runner (comma separated):kubernetes,gitlab-runner ##输入该runner的tagRegistering runner... succeeded runner=5cy-91nwPlease enter the executor: shell, virtualbox, docker+machine, docker-ssh+machine, kubernetes, parallels, docker-ssh, ssh, docker:kubernetes ##输入executorRunner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded! root@ceb637716b3c:/# grep token /etc/gitlab-runner/config.toml ##获取身份验证令牌 token = "-YGyg2YY_E4z9siNhpxj" bearer_token_overwrite_allowed = false 注意:复制此令牌并确保不会丢失它,这是验证Runner连接Gitlab的唯一身份令牌。保存令牌后，为安全计，请删除Docker实例。 在Kubernetes上部署Runner为Runner创建RBAC(角色访问控制)1234567891011121314151617181920212223242526# vim gitlab-runner-rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: gitlab-runner---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: gitlab-runnerrules: - apiGroups: [""] resources: ["*"] verbs: ["*"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: gitlab-runnersubjects: - kind: ServiceAccount name: gitlab-runnerroleRef: kind: Role name: gitlab-runner apiGroup: rbac.authorization.k8s.io 创建ConfigMap1234567891011121314151617181920# vim config.toml concurrent = 4[[runners]] name = "Kubernetes Gitlab-Runner" url = "http://192.168.100.128:32626" token = "-YGyg2YY_E4z9siNhpxj" executor = "kubernetes" [runners.kubernetes] namespace = "default" privileged = true poll_timeout = 600 cpu_request = "1" service_cpu_request = "200m" [[runners.kubernetes.volumes.host_path]] name = "docker" mount_path = "/var/run/docker.sock" host_path = "/var/run/docker.sock"# kubectl create configmap gitlab-runner-config --from-file=config.tomlconfigmap/gitlab-runner-config created 关于Kubernetes executor详询:https://docs.gitlab.com/runner/executors/kubernetes.html 部署Runner12345678910111213141516171819202122232425262728293031323334353637383940# vim gitlab-runner.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: gitlab-runnerspec: replicas: 1 selector: matchLabels: name: gitlab-runner template: metadata: labels: name: gitlab-runner spec: serviceAccountName: gitlab-runner containers: - name: gitlab-runner image: 192.168.100.100/gitlab/gitlab-runner:v11.9.0 imagePullPolicy: Always resources: requests: cpu: "100m" limits: cpu: "100m" volumeMounts: - name: config mountPath: /etc/gitlab-runner/config.toml readOnly: true subPath: config.toml volumes: - name: config configMap: name: gitlab-runner-config restartPolicy: Always# kubectl apply -f .serviceaccount/gitlab-runner createdrole.rbac.authorization.k8s.io/gitlab-runner createdrolebinding.rbac.authorization.k8s.io/gitlab-runner createddeployment.extensions/gitlab-runner created 查看连接情况 注意:再次说明，使用root账户创建的runner是共享的，其他项目、其他用户的项目都可以使用。而在某个用户下创建的项目为特定runner，只执行该项目jobs。 使用Runner新建一个项目配置环境变量 配置项位于该项目 Settings &gt; CI/CD &gt; Environment variables 下 新建一个简单的dockerfile文件123FROM 192.168.100.100/library/alpine:3.9WORKDIR /appRUN echo "hello" &gt; hh.txt 新建gitlab-ci.yml配置文件12345678910111213141516image: name: 192.168.100.100/library/alpine:3.9stages: - test test: stage: test image: 192.168.100.100/docker/docker:18 script: - docker info - docker login -u $harbor_username -p $harbor_password 192.168.100.100 - docker build -t 192.168.100.100/docker/test:latest . - docker push 192.168.100.100/docker/test:latest tags: - kubernetes 这里要注意的是，我截的图着重将clone的地址也截了下来，这里的gitlab为service。如果你是用的域名，而且域名可达，可忽略该问题。如果域名不可达，或者使用的是nodeport，又或者像我这样使用的是service，就需要更改gitlab的配置文件，不然gitlab-runner会出现无法Cloning repository的问题。这里我通过configmap挂载gitlab.rb配置文件到/etc/gitlab/目录来实现，gitlab.rb只需配置如下即可:external_url “http://gitlab&quot; 之后gitlab-runner会自动进行构建，构建成功会显示“passed”，失败则会显示“failed”。 点击“Stages”可查看相关构建阶段的详细信息 参照文章:https://adambcomer.com/blog/setup-gitlab-cicd-on-kubernetes.html]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>gitlab</tag>
        <tag>runnner</tag>
        <tag>CI</tag>
        <tag>CD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes+GitLab+Jenkins使用Pipeline构建镜像]]></title>
    <url>%2Fblog%2Fops%2F</url>
    <content type="text"><![CDATA[GitLab和Jenkins都是作为Pod部署在kubernetes上的，Jenkins通过创建的pipeline流水线任务从gitlab拉取构建文件以进行构建。 注:Pod都使用了持久化存储来部署,同时使用的是私有仓库镜像。 部署GitLab部署Redis12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# vim redis.yaml apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: redis namespace: defaultspec: serviceName: redis replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: redis spec: terminationGracePeriodSeconds: 30 containers: - name: redis image: 192.168.100.100/library/redis:4 ports: - containerPort: 6379 resources: requests: memory: "1Gi" cpu: "1000m" limits: memory: "2Gi" cpu: "2000m" livenessProbe: exec: command: - redis-cli - ping initialDelaySeconds: 30 timeoutSeconds: 5 readinessProbe: exec: command: - redis-cli - ping initialDelaySeconds: 5 timeoutSeconds: 1 volumeMounts: - name: redis mountPath: /var/lib/redis volumeClaimTemplates: - metadata: name: redis annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi---apiVersion: v1kind: Servicemetadata: name: redis namespace: defaultspec: ports: - name: redis port: 6379 targetPort: redis selector: name: redis# kubectl create -f redis.yaml statefulset.apps/redis createdservice/redis created 部署PostgreSQL1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# vim postgresql.yaml apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: postgresql namespace: defaultspec: serviceName: postgresql replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: postgresql spec: terminationGracePeriodSeconds: 30 containers: - name: postgresql image: 192.168.100.100/library/postgres:11 ports: - containerPort: 5432 resources: requests: memory: "1Gi" cpu: "1000m" limits: memory: "2Gi" cpu: "2000m" env: - name: DB_USER value: gitlab - name: DB_PASS value: passw0rd - name: DB_NAME value: gitlab_production - name: DB_EXTENSION value: pg_trgm livenessProbe: exec: command: - pg_isready - -h - localhost - -U - postgres initialDelaySeconds: 30 timeoutSeconds: 5 readinessProbe: exec: command: - pg_isready - -h - localhost - -U - postgres initialDelaySeconds: 5 timeoutSeconds: 1 volumeMounts: - name: postgresql mountPath: /var/lib/postgresql volumeClaimTemplates: - metadata: name: postgresql annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi---apiVersion: v1kind: Servicemetadata: name: postgresql namespace: defaultspec: ports: - name: postgresql port: 5432 targetPort: postgresql selector: name: postgresql# kubectl create -f postgresql.yaml statefulset.apps/postgresql createdservice/postgresql created 部署GitLab123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176# vim gitlab.yaml apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: gitlab namespace: defaultspec: serviceName: gitlab replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: gitlab labels: name: gitlab spec: terminationGracePeriodSeconds: 30 containers: - name: gitlab image: 192.168.100.100/gitlab/gitlab-ce:11.7.5-ce resources: requests: memory: "1Gi" cpu: "1000m" limits: memory: "4Gi" cpu: "2000m" env: - name: TZ value: Asia/BeiJing - name: GITLAB_TIMEZONE value: BeiJing - name: GITLAB_SECRETS_DB_KEY_BASE value: rVhTJmMMnn3RCwxXV7HCTNTkJXWLPjJhNHM3sWTgKwnqnNVTwNCVJ7zPLzKgVz7z - name: GITLAB_SECRETS_SECRET_KEY_BASE value: VkT7bxtXRsgvLpqRgbVnJKfjTMs7TsWrkXfmntPvbNWRLc4dxK9cWkLNpnNNvmsM - name: GITLAB_SECRETS_OTP_KEY_BASE value: Xgksv7WKTvWVrNm9vcvbdgMnVqXLPb3F9XvbFLXNLTTXtzcvJgp7nTrdbxN444Jt - name: GITLAB_ROOT_PASSWORD value: wangzhijian - name: GITLAB_ROOT_EMAIL value: wangzhijiansd@qq.com - name: GITLAB_HOST value: git.default - name: GITLAB_PORT value: "80" - name: GITLAB_SSH_PORT value: "22" - name: GITLAB_NOTIFY_ON_BROKEN_BUILDS value: "true" - name: GITLAB_NOTIFY_PUSHER value: "false" - name: GITLAB_BACKUP_SCHEDULE value: daily - name: GITLAB_BACKUP_TIME value: 01:00 - name: DB_TYPE value: postgresql - name: DB_HOST value: postgresql - name: DB_PORT value: "5432" - name: DB_USER value: gitlab - name: DB_PASS value: passw0rd - name: DB_NAME value: gitlab_production - name: REDIS_HOST value: redis - name: REDIS_PORT value: "6379" - name: SMTP_ENABLED value: "true" - name: SMTP_DOMAIN value: www.flywzj.com - name: SMTP_HOST value: smtp.qq.com - name: SMTP_PORT value: "465" - name: SMTP_USER value: wangzhijiansd@qq.com - name: SMTP_PASS value: "********" - name: SMTP_STARTTLS value: "true" - name: SMTP_AUTHENTICATION value: login - name: IMAP_ENABLED value: "false" - name: IMAP_HOST value: imap.gmail.com - name: IMAP_PORT value: "993" - name: IMAP_USER value: mailer@example.com - name: IMAP_PASS value: password - name: IMAP_SSL value: "true" - name: IMAP_STARTTLS value: "false" ports: - name: http containerPort: 80 - name: ssh containerPort: 22 volumeMounts: - name: gitlab-data mountPath: /var/opt/gitlab - name: gitlab-config mountPath: /etc/gitlab - name: gitlab-logs mountPath: /var/log/gitlab volumeClaimTemplates: - metadata: name: gitlab-data annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi - metadata: name: gitlab-config annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 100Mi - metadata: name: gitlab-logs annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 100Mi# vim gitlab-svc.yaml apiVersion: v1kind: Servicemetadata: name: gitlab namespace: defaultspec: type: LoadBalancer ports: - name: http port: 80 targetPort: http - name: ssh port: 22 targetPort: ssh selector: name: gitlab# kubectl create -f gitlab.yaml statefulset.apps/gitlab created# kubectl create -f gitlab-svc.yaml service/gitlab created 关于变量的赋予请参考:https://github.com/sameersbn/docker-gitlab 如需进行特殊配置，请参考gitlab相关文档将相应配置写入gitlab.rb并使用configmap将其挂载到/etc/gitlab/目录下 部署Jenkins1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# vim jenkins.yamlapiVersion: apps/v1beta1kind: StatefulSetmetadata: name: jenkinsspec: serviceName: jenkins replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: jenkins labels: name: jenkins spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins containers: - name: jenkins image: 192.168.100.100/jenkins/jenkins:lts imagePullPolicy: Always ports: - containerPort: 8080 - containerPort: 50000 resources: limits: cpu: 2 memory: 2Gi requests: cpu: 1 memory: 1Gi env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS value: -Duser.timezone=Asia/Shanghai -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 volumeMounts: - name: jenkins-home mountPath: /var/jenkins_home livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 # ~2 minutes readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 # ~2 minutes securityContext: fsGroup: 1000 volumeClaimTemplates: - metadata: name: jenkins-home annotations: volume.beta.kubernetes.io/storage-class: "ceph-rbd" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi---apiVersion: v1kind: Servicemetadata: name: jenkins namespace: defaultspec: type: LoadBalancer ports: - name: http port: 8080 targetPort: 8080 - name: agent port: 50000 targetPort: 50000 selector: name: jenkins# kubectl apply -f jenkins.yamlstatefulset.apps/jenkins configuredservice/jenkins configured 配置RBAC12345678910111213141516171819202122232425262728293031323334353637383940# vim jenkins-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: jenkins---kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: jenkinsrules:- apiGroups: [""] resources: ["pods"] verbs: ["create","delete","get","list","patch","update","watch"]- apiGroups: [""] resources: ["pods/exec"] verbs: ["create","delete","get","list","patch","update","watch"]- apiGroups: [""] resources: ["pods/log"] verbs: ["get","list","watch"]- apiGroups: [""] resources: ["secrets"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: jenkinsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkinssubjects:- kind: ServiceAccount name: jenkins# kubectl apply -f jenkins-rbac.yamlserviceaccount/jenkins configuredrole.rbac.authorization.k8s.io/jenkins configuredrolebinding.rbac.authorization.k8s.io/jenkins configured# kubectl create clusterrolebinding cluster-admin-jenkins --clusterrole=cluster-admin --serviceaccount=default:default 查看相关部署情况查看 StatefulSet123456# kubectl get statefulsetNAME READY AGEgitlab 1/1 41hjenkins 1/1 41hpostgresql 1/1 41hredis 1/1 41h 查看 PV和PVC123456789101112# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-18ec78b1-4ee5-11e9-b373-000c292a7b79 10Gi RWO Delete Bound default/redis-redis-0 ceph-rbd 41hpvc-48a7f8c4-4ee9-11e9-b373-000c292a7b79 10Gi RWO Delete Bound default/postgresql-postgresql-0 ceph-rbd 41hpvc-49035b17-4eeb-11e9-b373-000c292a7b79 10Gi RWO Delete Bound default/gitlab-gitlab-0 ceph-rbd 41hpvc-b5bbbd9b-4af6-11e9-9bd3-000c292a7b79 10Gi RWO Delete Bound default/jenkins-home-jenkins-0 ceph-rbd 41h# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEgitlab-gitlab-0 Bound pvc-49035b17-4eeb-11e9-b373-000c292a7b79 10Gi RWO ceph-rbd 41hjenkins-home-jenkins-0 Bound pvc-b5bbbd9b-4af6-11e9-9bd3-000c292a7b79 10Gi RWO ceph-rbd 41hpostgresql-postgresql-0 Bound pvc-48a7f8c4-4ee9-11e9-b373-000c292a7b79 10Gi RWO ceph-rbd 41hredis-redis-0 Bound pvc-18ec78b1-4ee5-11e9-b373-000c292a7b79 10Gi RWO ceph-rbd 41h 查看 Pod123456# kubectl get podNAME READY STATUS RESTARTS AGEgitlab-0 1/1 Running 0 41hjenkins-0 1/1 Running 0 41hpostgresql-0 1/1 Running 0 41hredis-0 1/1 Running 0 41h 查看 Service123456# kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgitlab LoadBalancer 10.96.131.243 &lt;pending&gt; 80:30284/TCP,22:32377/TCP 41hjenkins LoadBalancer 10.106.240.93 &lt;pending&gt; 8080:31906/TCP,50000:31611/TCP 41hpostgresql ClusterIP 10.106.35.107 &lt;none&gt; 5432/TCP 41hredis ClusterIP 10.99.125.252 &lt;none&gt; 6379/TCP 41h 配置Jenkins 当您使用nodeport方式打开Jenkins的时候，首先会出现“Unlocking Jenkins”，这里的“Administrator password”存放在logs中，你可以通过查看日志获取，也可通过如下命令获取: 12# kubectl exec -it jenkins-0 cat /var/jenkins_home/secrets/initialAdminPassword1f297e8179664848a3e0b672ceafce1f 在之后会提示您安装插件(这里我安装了推荐插件)和配置管理员账号，请自行安装和配置。 登录后，在“系统管理”===&gt;“插件管理”===&gt;“available”(可用插件)===&gt;安装Kubernetes和GitLab插件 关于Gitlab插件的配置 在“系统管理”===&gt;“系统设置”下进行设置 Connection name: gitlab(可自行更改) Gitlab host URL: http://gitlab(该gitlab与kubernetes的service对应) Credentials: 点击“Add”或者在首页的凭据栏配置全局凭据，如下所示: 配置完成后点击“Test Connection”进行测试，如出现“Success”即为成功 关于GitLab API token 登录相关Gitlab账户，在该用户的“Setting”===&gt;“Access Tokens”下添加个人访问令牌的名称、到期时间以及授予的权限,如下所示: 之后会生成个人访问令牌，请注意保存，之后将无法再查看该token。将该token复制到Jenkins以添加凭据 关于kubernetes插件的配置 在“系统管理”===&gt;“系统设置”===&gt;拖到最后找到“云”===&gt;“新增一个云” name: kubernetes(可名称) kubernetes 地址: https://kubernetes.default(可配置为apiserver地址) Jenkins 地址: http://jenkins:8080(配置的是service名，也可以直接配置http://ip:port) Jenkins 通道: jenkins:50000(配置的是service:port，也可以配置ip:port) 这里没有配置凭据，因为上面的部署中已经赋予了相应权限，如果是集群外安装则需要添加凭据 点击“Test Connection”，如出现“Connection test successful”则说明 Jenkins已经可以和 Kubernetes 通信了 关于添加凭据 Jenkins支持添加多种类型凭据，除了上面介绍的“Gitlab API token”还有“SSH Username with private key”、“Certificate”以及常见的用户名密码类型，这里说的就是用户名密码类型，这里添加harbor镜像仓库以及Gitlab的用户名密码以备使用。 使用 GitLab 新建项目登录Gitlab 使用nodeport方式打开Gitlab，然后使用root账户登录，在“Admin Area”的“Settings”下配置“Network”，使其允许从hooks和 services向local network发出请求。 新建Gitlab账户 新建项目 写一个简单的dockerfile 使用 Jenkins 新建任务新建pipeline任务 写一个pipeline1234567891011121314151617181920212223242526272829303132333435363738394041def label = "docker-$&#123;UUID.randomUUID().toString()&#125;"podTemplate(label: label, yaml: """apiVersion: v1kind: Podspec: containers: - name: docker image: 192.168.100.100/docker/docker:18 command: ['cat'] tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock volumes: - name: dockersock hostPath: path: /var/run/docker.sock""" ) &#123; def image = "192.168.100.100/library/alpine:test" node(label) &#123; stage('Git') &#123; git credentialsId: 'gitlab',url:'http://gitlab/zhi/alpine.git' &#125; stage('Build Docker image') withCredentials([ usernamePassword( credentialsId: 'harbor', passwordVariable: 'PASSWORD', usernameVariable: 'USERNAME')]) &#123; container('docker') &#123; sh "docker build -t $&#123;image&#125; ." sh "docker login -u '$USERNAME' -p '$PASSWORD' 192.168.100.100" sh "docker push $&#123;image&#125;" sh "docker rmi $&#123;image&#125;" &#125; &#125; &#125;&#125; 注意:该流水线是参照jenkinsci-kubernetes-plugin而来。 关于该流水线的简单解释: podTemplate: 用于创建代理的pod的模板 label: pod的标签。设置唯一值以避免跨构建的冲突 containers: 用于创建pod的容器模板 containerTemplate: 将添加到pod中的容器模板 name: 容器名 image: 容器镜像 command: 容器将执行的命令 tty: true,启用TTY volumes: 为pod挂载相应卷 node: 选择相应标签 stage: 构建阶段，这将显性的显示在“Stage View”下 withCredentials: 将凭据绑定到变量 usernamePassword: 将一个变量设置为用户名，将另一个变量设置为凭据中给出的密码 credentialsId: 设置凭据ID passwordVariable: 在构建期间要设置为密码的环境变量的名称 usernameVariable: 构建期间要设置为用户名的环境变量的名称 进行构建 点击“立即构建”进行构建 定时构建 构建语法 第一个*表示分钟，取值0~59 第二个*表示小时，取值0~23 第三个*表示一个月的第几天，取值1~31 第四个*表示第几月，取值1~12 第五个*表示一周中的第几天，取值0~7，其中0和7代表的都是周日 触发构建在该任务下启用触发器 勾选将更改推送到GitLab时构建，点击高级生成“Secret token” 在Gitlab下添加Webhooks 在该项目下设置“Integrations”，将Jenkins的配置配置在“URL”和“Secret Token”项下，根据情况勾选相应触发项 在Gitlab下更新该项目 查看Jenkins是否触发了构建 关于在Jenkins中使用kubectl命令 注: 在上面的Jenkins的部署中已经做好了相关权限的分配 重构jnlp-slave 注: docker安装包和kubectl命令请自行下载至构建目录下 123456789101112131415161718192021222324252627# mkdir jnlp &amp;&amp; cd jnlp# cp .kube/config .# lsconfig docker-18.06.1-ce.tgz Dockerfile kubectl# cat Dockerfile FROM jenkins/jnlp-slave:3.27-1-alpineMAINTAINER zhi &lt;wangzhijiansd@qq.com&gt;USER rootARG DOCKER_GID=994ENV DOCKER_VERSION=18.06.1-ceCOPY docker-$&#123;DOCKER_VERSION&#125;.tgz /var/tmp/RUN tar --strip-components=1 -xvzf /var/tmp/docker-$&#123;DOCKER_VERSION&#125;.tgz -C /usr/local/bin \ &amp;&amp; rm -rf /var/tmp/docker-$&#123;DOCKER_VERSION&#125;.tgz \ &amp;&amp; chmod -R 775 /usr/local/bin/dockerCOPY kubectl /usr/local/bin/RUN mkdir -p /root/.kube/COPY config /root/.kube/RUN addgroup -g $&#123;DOCKER_GID&#125; docker &amp;&amp; adduser jenkins docker USER jenkins:$&#123;DOCKER_GID&#125;# docker build -t 192.168.100.100/jenkins/jnlp-slave:latest .# docker push 192.168.100.100/jenkins/jnlp-slave:latest 编写一个pipeline12345678910111213141516171819podTemplate(label: 'mypod', cloud: 'kubernetes', containers: [ containerTemplate( name: 'jnlp', image: '192.168.100.100/jenkins/jnlp-slave:latest', alwaysPullImage: true, args: '$&#123;computer.jnlpmac&#125; $&#123;computer.name&#125;'), ], volumes: [ hostPathVolume( mountPath: '/var/run/docker.sock', hostPath: '/var/run/docker.sock'),],) &#123; node('mypod') &#123; stage('Run shell') &#123; sh 'kubectl get pod -n kube-system' &#125; &#125;&#125; 进行构建 附: K8S+Gitlab+Jenkins构建镜像并创建Pod: https://blog.51cto.com/wangzhijian/2285861 流水线语法参考: https://jenkins.io/zh/doc/book/pipeline/syntax/ 关于凭据绑定: https://jenkins.io/doc/pipeline/steps/credentials-binding/ 关于定时构建: https://www.cnblogs.com/panpan0301/p/7738249.html]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Harbor从1.6.2升级到1.7.4]]></title>
    <url>%2Fblog%2F501be2e%2F</url>
    <content type="text"><![CDATA[Harbor最新的V1.7版本又添加了一些新的功能: 在线GC（垃圾回收）- 现在 Harbor 可以清理从后端存储中已删除的镜像且在执行GC操作之前不再要求中断 Harbor 的运行。支持按需垃圾收集，使管理员能够手动配置运行docker注册表垃圾收集或使用cron计划自动配置 镜像构建历史 - 可查看容器镜像的构建历史和内容 镜像复制（Image Retag）- 提供了在镜像上传至Harbor后重新创建镜像tag的能力。此功能在CI流水线中提升镜像到生产状态或者通过编程方式重新tag镜像，亦或将特定镜像重新tag或者移动到其它仓库或者项目等场景中特别有用 支持使用Helm Chart部署Harbor，使用户能够获得Harbor服务的高可用性 支持Logger自定义，使用户能够自定义正在运行的作业的STDOUT / STDERR / FILE / DB记录器。 升级须知 必须在任何数据迁移之前备份数据 从v1.6.0开始，Harbor会在启动时自动尝试迁移数据库模式，因此如果从v1.6.0或更高版本升级，则无需调用迁移器工具来进行迁移 从v1.6.0起，Harbor将数据库从MariaDB迁移到PostgreSQL，并将Harbor，Notary和Clair DB合并为一个 停止并删除现有的Harbor实例12# cd harbor# docker-compose -f ./docker-compose.yml -f ./docker-compose.clair.yml down 备份Harbor的当前文件1# mv harbor harbor-bak 备份数据库 默认情况下目录为/data/database 1# cp -r /data/database /root/databak/ 下载迁移工具1# docker pull goharbor/harbor-migrator:v1.7.4 升级harbor.cfg 注意: harbor.cfg将被覆盖，您必须在迁移后将其移动到安装目录 1234567# docker run -it --rm -v /root/harbor-bak/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:v1.7.4 --cfg upPlease backup before upgrade, Enter y to continue updating or n to abort: yThe path of the migrated harbor.cfg is not set, the input file will be overwritten.input version: 1.6.0, migrator chain: ['1.7.0']migrating to version 1.7.0Written new values to /harbor-migration/harbor-cfg/harbor.cfg 解压harbor离线包1# tar -zxvf harbor-offline-installer-v1.7.4.tgz 覆盖harbor.cfg123# cd harbor# mv harbor.cfg harbor.bak# cp /root/harbor-bak/harbor.cfg . 载入镜像12# docker load -i harbor.v1.7.4.tar.gz# docker images|grep 1.7.4 安装Notary，Clair和Helm Chart服务123456# ./install.sh --with-notary --with-clair --with-chartmuseum......✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at https://192.168.100.100. For more details, please visit https://github.com/goharbor/harbor . 进行查看1234567891011121314# docker-compose -f ./docker-compose.yml -f ./docker-compose.clair.yml ps Name Command State Ports -------------------------------------------------------------------------------------------------------------------------------------clair /docker-entrypoint.sh Up (healthy) 6060/tcp, 6061/tcp harbor-adminserver /harbor/start.sh Up (healthy) harbor-core /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-portal nginx -g daemon off; Up (healthy) 80/tcp nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpredis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp registryctl /harbor/start.sh Up (healthy) 清除旧版本镜像1# docker images|grep 1.6.2| awk '&#123;print $3&#125;'|xargs docker rmi 注: harbor升级和数据库迁移指南]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用Kaniko进行构建]]></title>
    <url>%2Fblog%2Fkaniko%2F</url>
    <content type="text"><![CDATA[kaniko是一个从Dockerfile，容器或Kubernetes集群内构建容器映像的工具。 kaniko不依赖于Docker守护程序，并且在用户空间中完全执行Dockerfile中的每个命令。这样可以在无法轻松或安全地运行Docker守护程序的环境中构建容器映像，例如标准Kubernetes集群。 kaniko执行程序映像负责从Dockerfile构建映像并将其推送到注册表。在执行程序映像中，提取基本映像的文件系统（Dockerfile中的FROM映像）。然后，在Dockerfile中执行命令，在每个文件系统之后对用户空间中的文件系统进行快照。在每个命令之后，将一层已更改的文件附加到基本图像（如果有的话）并更新图像元数据。 在kubernetes上构建还有另外一种选择，就是docker in docker，将宿主机的/var/run/docker.dock挂载到pod，并使用宿主机Docker守护程序执行构建。但是docker in docker必须运行在特权模式下，这会产生安全风险。 在Docker中运行kaniko123456789101112131415161718192021# mkdir app &amp;&amp; cd app# vim Dockerfile FROM 192.168.100.100/library/alpine:3.9WORKDIR /appRUN echo "hello" &gt; world.txt# docker run --env DOCKER_CONFIG=/kaniko -v /root/app:/workspace -v /etc/pki/ca-trust/source/anchors/harbor-ca.pem:/kaniko/ssl/certs/ca.pem -v /root/.docker/config.json:/kaniko/config.json 192.168.100.100/k8s.gcr.io/executor:debug -d 192.168.100.100/library/test:testINFO[0000] Downloading base image 192.168.100.100/library/alpine:3.9 INFO[0000] Error while retrieving image from cache: getting file info: stat /cache/sha256:25b4d910f4b76a63a3b45d0f69a57c34157500faf6087236581eca221c62d214: no such file or directory INFO[0000] Downloading base image 192.168.100.100/library/alpine:3.9 INFO[0000] Unpacking rootfs as cmd RUN echo "hello" &gt; world.txt requires it. INFO[0002] Taking snapshot of full filesystem... INFO[0002] RUN echo "hello" &gt; world.txt INFO[0002] cmd: /bin/sh INFO[0002] args: [-c echo "hello" &gt; world.txt] INFO[0002] Taking snapshot of full filesystem... 2019/03/08 15:35:14 existing blob: sha256:6c40cc604d8e4c121adcb6b0bfe8bb038815c350980090e74aa5a6423f8f82c02019/03/08 15:35:14 pushed blob sha256:4c1586bb248bd4662909b9c520aec7d405ba28f72be717bda7f906328ba93ed22019/03/08 15:35:14 pushed blob sha256:acf567dd059c0c462de4ef165221491959c8195b5777a7f87bdd9de7fc939bea2019/03/08 15:35:14 192.168.100.100/library/test:test: digest: sha256:c430406eda9f80eed55c12625585bb7da6947edd0332bf559a9514d7d8328601 size: 588# docker run -it 192.168.100.100/library/test:test cat /app/world.txthello 注1: –env DOCKER_CONFIG=/kaniko:设置环境变量 -v /root/app:/workspace:将/app目录挂载到/workspace构建上下文 -v /etc/pki/ca-trust/source/anchors/harbor-ca.pem:/kaniko/ssl/certs/ca.pem:将私有镜像仓库的证书挂载到/kaniko/ssl/certs/下 -v /root/.docker/config.json:/kaniko/config.json:将docker配置文件挂载到/kaniko下 192.168.100.100/k8s.gcr.io/executor:debug:运行kaniko容器(gcr.io/kaniko-project/executor) -d 192.168.100.100/library/test:test:推送的镜像 注2:对于开源镜像仓库harbor，push到镜像仓库后无法显示在UI上。对于公有云来说，由于各自的认证方式有别，push镜像可能会无法上传，hub.docker.com、阿里云容器镜像服务都可以push，网易云镜像中心无法push，提示“no token in bearer response:{“errors”:[{“code”:”DENIED”,”message”:”Real name authentication required”}]}”。 在kubernetes中运行kaniko示例dockerfile1234# vim Dockerfile FROM 192.168.100.100/library/alpine:3.9WORKDIR /appRUN echo "hello" &gt; world.txt 挂载harbor镜像仓库CA证书12# kubectl create configmap ca-certificates --from-file=/etc/pki/ca-trust/source/anchors/harbor-ca.pemconfigmap/ca-certificates created 挂载docker配置文件12# kubectl create configmap docker-config --from-file=/root/.docker/config.json configmap/docker-config created 配置yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243# vim kaniko.yaml apiVersion: v1kind: Podmetadata: name: kanikospec: restartPolicy: Never initContainers: - name: git-clone image: alpine/git args: - clone - --single-branch - -- - https://github.com/zhijiansd/gityun.git - /context volumeMounts: - name: context mountPath: /context containers: - name: kaniko image: registry.cn-hangzhou.aliyuncs.com/gityun/executor:debug args: ["--dockerfile=/context/Dockerfile", "--context=/context", "--destination=192.168.100.100/library/test:test"] volumeMounts: - name: ca-certificates mountPath: /kaniko/ssl/certs/ - name: docker-config mountPath: /kaniko/.docker/ - name: context mountPath: /context restartPolicy: Never volumes: - name: ca-certificates configMap: name: ca-certificates - name: docker-config configMap: name: docker-config - name: context emptyDir: &#123;&#125; 注: 配置Init容器使用镜像 “alpine/git” clone相关项目并挂载为context，将相关代码传递到kaniko容器进行构建 使用configmap将harbor的CA证书和docker配置文件config.json挂载到kaniko 运行并查看123456789101112131415161718192021222324# kubectl create -f kaniko.yaml pod/kaniko created# kubectl get pod|grep kanikokaniko 0/1 Completed 0 29s# kubectl logs kanikoINFO[0000] Downloading base image 192.168.100.100/library/alpine:3.9 INFO[0000] Error while retrieving image from cache: getting file info: stat /cache/sha256:25b4d910f4b76a63a3b45d0f69a57c34157500faf6087236581eca221c62d214: no such file or directory INFO[0000] Downloading base image 192.168.100.100/library/alpine:3.9 INFO[0001] Unpacking rootfs as cmd RUN echo "hello" &gt; world.txt requires it. INFO[0002] Taking snapshot of full filesystem... INFO[0002] WORKDIR /app INFO[0002] cmd: workdir INFO[0002] Changed working directory to /app INFO[0002] Creating directory /app INFO[0002] Taking snapshot of files... INFO[0002] RUN echo "hello" &gt; world.txt INFO[0002] cmd: /bin/sh INFO[0002] args: [-c echo "hello" &gt; world.txt] INFO[0002] Taking snapshot of full filesystem... 2019/03/10 08:06:41 existing blob: sha256:6c40cc604d8e4c121adcb6b0bfe8bb038815c350980090e74aa5a6423f8f82c02019/03/10 08:06:41 pushed blob sha256:a45d1670e71fec26e7147195023edc9fd26f93bbd4412fea4998528122ea43a02019/03/10 08:06:41 pushed blob sha256:a04e0385010a9eedddbdd6ddca8e90725d0b981fb7954948af67b538c781727f2019/03/10 08:06:41 pushed blob sha256:e67312291a1a10d69480c1dd22c9af51a659e48ae07ba8221877397fd74e51fe2019/03/10 08:06:41 192.168.100.100/library/test:test: digest: sha256:5c3d933724e54a605f84a233eccb95e00c1870f02f213de2157b7d143686edba size: 748 注: 日志上关于 cache 的 error 提示我根据官方提供的 flags 添加了相关命令，依然会出现该错误，但是这不影响，镜像依然 push 到了harbor 使用 kubernetes 构建到 harbor ，镜像依然无法显示在UI上，但是可以pull 对于公有云，全都无法 push，提示“connect: connection refused” 所以，最后这是一个半成品，没啥用，但是尝试了，还是写下来记录一下 附: kaniko的GitHub: https://github.com/GoogleContainerTools/kaniko 本文参考文章:https://harthoover.com/using-kaniko-for-container-builds-on-kubernetes/ harbor UI无法显示push的镜像的issue:https://github.com/goharbor/harbor/issues/6811]]></content>
      <categories>
        <category>镜像仓库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>registry</tag>
        <tag>kaniko</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm部署kubernetes]]></title>
    <url>%2Fblog%2Fkubeadm%2F</url>
    <content type="text"><![CDATA[这里我使用kubeadm部署的是1.13.3，在写该篇文章之时，恰好1.13.4发布了，所以索性将版本升级了。 配置互信1234567# vim /etc/hosts192.168.100.128 Master192.168.100.129 Node01192.168.100.130 Node02# ssh-keygen -t rsa -P ''# ssh-copy-id -i .ssh/id_rsa.pub root@node01# ssh-copy-id -i .ssh/id_rsa.pub root@node02 安装 Ansible123456# yum -y install ansible# cat /etc/ansible/hosts | grep -v ^# | grep -v ^$[node]node01node02# ansible node -m copy -a 'src=/etc/hosts dest=/etc/' 关闭 SELinux 和 Firewall123456# sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config# systemctl disable firewalld &amp;&amp; systemctl stop firewalld# ansible node -m copy -a 'src=/etc/selinux/config dest=/etc/selinux/'# ansible node -a 'systemctl stop firewalld'# ansible node -a 'systemctl disable firewalld' 安装 docker123456789101112# yum install -y yum-utils device-mapper-persistent-data lvm2# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# yum makecache fast# yum list docker-ce --showduplicates | sort -r# yum install -y docker-ce-18.06.1.ce-3.el7# systemctl enable docker &amp;&amp; systemctl start docker# ansible node -m yum -a "state=present name=yum-utils"# ansible node -m copy -a 'src=/etc/yum.repos.d/docker-ce.repo dest=/etc/yum.repos.d/'# ansible node -m yum -a "state=present name=docker-ce-18.06.1.ce-3.el7"# ansible node -a 'systemctl start docker'# ansible node -a 'systemctl enable docker' 解压 kubernetes123456789# tar -zxvf kubernetes-server-linux-amd64.tar.gz # cd kubernetes/server/bin/# docker load -i kube-apiserver.tar# docker load -i kube-controller-manager.tar # docker load -i kube-scheduler.tar# docker load -i kube-proxy.tar# ansible node -m copy -a 'src=kube-proxy.tar dest=/root'# ansible node -m command -a "docker load -i kube-proxy.tar" 配置 kubernetes 源123456789101112131415161718# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# yum install -y kubelet kubeadm kubectl# systemctl enable kubelet &amp;&amp; systemctl start kubelet# ansible node -m copy -a 'src=/etc/yum.repos.d/kubernetes.repo dest=/etc/yum.repos.d/'# ansible node -m yum -a "state=present name=kubelet"# ansible node -m yum -a "state=present name=kubeadm"# ansible node -m yum -a "state=present name=kubectl"# ansible node -a 'systemctl start kubelet'# ansible node -a 'systemctl enable kubelet' 配置 kube-proxy 代理模式1234567891011# grep -v ^# /etc/sysctl.conf net.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# sysctl -pnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1# ansible node -m copy -a 'src=/etc/sysctl.conf dest=/etc/'# ansible node -a 'sysctl -p' 查看需要使用的镜像12345678# kubeadm config images listk8s.gcr.io/kube-apiserver:v1.13.3k8s.gcr.io/kube-controller-manager:v1.13.3k8s.gcr.io/kube-scheduler:v1.13.3k8s.gcr.io/kube-proxy:v1.13.3k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.2.24k8s.gcr.io/coredns:1.2.6 下载余下的镜像 请自行更改tag 123# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.6# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.2.24 启用 swap123# vim /etc/sysconfig/kubeletKUBELET_EXTRA_ARGS=--fail-swap-on=false# ansible node -m copy -a 'src=/etc/sysconfig/kubelet dest=/etc/sysconfig/' 初始化集群123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# kubeadm init \ --kubernetes-version=v1.13.3 \ --pod-network-cidr=10.244.0.0/16 \ --apiserver-advertise-address=192.168.100.128 \ --ignore-preflight-errors=Swap [init] Using Kubernetes version: v1.13.3[preflight] Running pre-flight checks [WARNING Swap]: running with swap on is not supported. Please disable swap[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder "/etc/kubernetes/pki"[certs] Generating "etcd/ca" certificate and key[certs] Generating "etcd/healthcheck-client" certificate and key[certs] Generating "apiserver-etcd-client" certificate and key[certs] Generating "etcd/server" certificate and key[certs] etcd/server serving cert is signed for DNS names [master localhost] and IPs [192.168.100.128 127.0.0.1 ::1][certs] Generating "etcd/peer" certificate and key[certs] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.100.128 127.0.0.1 ::1][certs] Generating "ca" certificate and key[certs] Generating "apiserver" certificate and key[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.100.128][certs] Generating "apiserver-kubelet-client" certificate and key[certs] Generating "front-proxy-ca" certificate and key[certs] Generating "front-proxy-client" certificate and key[certs] Generating "sa" key and public key[kubeconfig] Using kubeconfig folder "/etc/kubernetes"[kubeconfig] Writing "admin.conf" kubeconfig file[kubeconfig] Writing "kubelet.conf" kubeconfig file[kubeconfig] Writing "controller-manager.conf" kubeconfig file[kubeconfig] Writing "scheduler.conf" kubeconfig file[control-plane] Using manifest folder "/etc/kubernetes/manifests"[control-plane] Creating static Pod manifest for "kube-apiserver"[control-plane] Creating static Pod manifest for "kube-controller-manager"[control-plane] Creating static Pod manifest for "kube-scheduler"[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s[kubelet-check] Initial timeout of 40s passed.[apiclient] All control plane components are healthy after 47.012976 seconds[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "master" as an annotation[mark-control-plane] Marking the node master as control-plane by adding the label "node-role.kubernetes.io/master=''"[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: mrtv9n.fdvmt32f3kkbyyjx[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.100.128:6443 --token mrtv9n.fdvmt32f3kkbyyjx --discovery-token-ca-cert-hash sha256:0b5fefef7ca78df72d8d35d3b0e05511d24be0365b0b403f55c8438167606654 配置访问集群12345678# mkdir -p $HOME/.kube# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config# chown $(id -u):$(id -g) $HOME/.kube/config# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy &#123;"health": "true"&#125; 安装 flannel(删减多余的配置)1234567891011121314151617# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# kubectl apply -f kube-flannel.yml clusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.extensions/kube-flannel-ds-amd64 created# kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-86c58d9df4-cm8gv 1/1 Running 0 41mcoredns-86c58d9df4-xpccv 1/1 Running 0 41metcd-master 1/1 Running 0 40mkube-apiserver-master 1/1 Running 0 40mkube-controller-manager-master 1/1 Running 0 41mkube-flannel-ds-amd64-xz6bf 1/1 Running 0 32skube-proxy-29pzf 1/1 Running 0 41mkube-scheduler-master 1/1 Running 0 41m 添加 Node 节点1234567891011# vim node.sh#!/bin/bashkubeadm join 192.168.100.128:6443 --token mrtv9n.fdvmt32f3kkbyyjx --discovery-token-ca-cert-hash sha256:0b5fefef7ca78df72d8d35d3b0e05511d24be0365b0b403f55c8438167606654 --ignore-preflight-errors=Swap# ansible node -m copy -a 'src=/root/node.sh dest=/root/ mode=755'# ansible node -m copy -a 'src=/root/node.sh dest=/root/ mode=755'# ansible node -m shell -a '/root/node.sh' # kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 68m v1.13.3node01 Ready &lt;none&gt; 64s v1.13.3node02 Ready &lt;none&gt; 64s v1.13.3 kubeadm升级计划 检查可用于升级的版本，并验证当前群集是否可升级。要跳过互联网检查，请传入可选的[version]参数。 12345678910111213141516171819202122232425262728293031# kubeadm upgrade plan 1.13.4[preflight] Running pre-flight checks.[upgrade] Making sure the cluster is healthy:[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.13.3[upgrade/versions] kubeadm version: v1.13.3Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT CURRENT AVAILABLEKubelet 3 x v1.13.3 1.13.4Upgrade to the latest version in the v1.13 series:COMPONENT CURRENT AVAILABLEAPI Server v1.13.3 1.13.4Controller Manager v1.13.3 1.13.4Scheduler v1.13.3 1.13.4Kube Proxy v1.13.3 1.13.4CoreDNS 1.2.6 1.2.6Etcd 3.2.24 3.2.24You can now apply the upgrade by executing the following command: kubeadm upgrade apply 1.13.4Note: Before you can perform this upgrade, you have to update kubeadm to 1.13.4._____________________________________________________________________ 在master节点解压新版本kubernetes1234567# tar -zxvf kubernetes-server-linux-amd64.tar.gz# cd kubernetes/server/bin/# cp kubeadm /usr/bin/# docker load -i kube-apiserver.tar# docker load -i kube-controller-manager.tar# docker load -i kube-scheduler.tar# docker load -i kube-proxy.tar 将Kubernetes群集升级到指定版本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# kubeadm upgrade apply 1.13.4[preflight] Running pre-flight checks.[upgrade] Making sure the cluster is healthy:[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file.[upgrade/version] You have chosen to change the cluster version to "v1.13.4"[upgrade/versions] Cluster version: v1.13.3[upgrade/versions] kubeadm version: v1.13.4[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd][upgrade/prepull] Prepulling image for component etcd.[upgrade/prepull] Prepulling image for component kube-apiserver.[upgrade/prepull] Prepulling image for component kube-controller-manager.[upgrade/prepull] Prepulling image for component kube-scheduler.[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler[upgrade/prepull] Prepulled image for component kube-controller-manager.[upgrade/prepull] Prepulled image for component kube-apiserver.[upgrade/prepull] Prepulled image for component etcd.[upgrade/prepull] Prepulled image for component kube-scheduler.[upgrade/prepull] Successfully prepulled the images for all the control plane components[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.13.4"...Static pod: kube-apiserver-master hash: b9152d72f9c05c3d3f7b4ac7268324c6Static pod: kube-controller-manager-master hash: 8288866dd95d24b3f0eb40747d951fbaStatic pod: kube-scheduler-master hash: b734fcc86501dde5579ce80285c0bf0c[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests466472581"[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-02-14-27-45/kube-apiserver.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-apiserver-master hash: b9152d72f9c05c3d3f7b4ac7268324c6Static pod: kube-apiserver-master hash: 2c4b7dbda2d0962b4cc2b6c98516bf14[apiclient] Found 1 Pods for label selector component=kube-apiserver[upgrade/staticpods] Component "kube-apiserver" upgraded successfully![upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-02-14-27-45/kube-controller-manager.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-controller-manager-master hash: 8288866dd95d24b3f0eb40747d951fbaStatic pod: kube-controller-manager-master hash: b6ca67226d47ac720e105375a9846904[apiclient] Found 1 Pods for label selector component=kube-controller-manager[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully![upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-02-14-27-45/kube-scheduler.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-scheduler-master hash: b734fcc86501dde5579ce80285c0bf0cStatic pod: kube-scheduler-master hash: 4b52d75cab61380f07c0c5a69fb371d4[apiclient] Found 1 Pods for label selector component=kube-scheduler[upgrade/staticpods] Component "kube-scheduler" upgraded successfully![uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "master" as an annotation[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxy[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.13.4". Enjoy![upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. 升级node升级节点配置12345# kubeadm upgrade node config --kubelet-version v1.13.4[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[upgrade] The configuration for this node was successfully updated![upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. 将相关节点标记为不可调度123456# kubectl drain master --ignore-daemonsetsnode/master cordonedWARNING: Ignoring DaemonSet-managed pods: kube-flannel-ds-amd64-xz6bf, kube-proxy-dlck4node/master drained# kubectl get nodes |grep mastermaster Ready,SchedulingDisabled master 16d v1.13.3 更新相应软件包并解锁该节点12345678# cd kubernetes/server/bin/# systemctl stop kubelet# cp kubeadm kubectl kubelet /usr/bin/# systemctl start kubelet# kubectl uncordon masternode/master uncordoned# kubectl get nodes|grep mastermaster Ready master 16d v1.13.4 更新其他节点123456789101112131415161718192021222324252627282930313233343536# kubectl drain node01 --ignore-daemonsetsnode/node01 cordonedWARNING: Ignoring DaemonSet-managed pods: kube-flannel-ds-amd64-8q4kq, kube-proxy-jvz4zpod/coredns-86c58d9df4-97fw8 evictedpod/rbd-provisioner-6447467945-jjz7j evictednode/node01 evicted# ansible node01 -a "systemctl stop kubelet"# ansible node01 -m copy -a 'src=kubeadm dest=/usr/bin/'# ansible node01 -m copy -a 'src=kubectl dest=/usr/bin/'# ansible node01 -m copy -a 'src=kubelet dest=/usr/bin/'# ansible node01 -a "systemctl start kubelet"# kubectl uncordon node01node/node01 uncordoned# kubectl get nodes|grep node01node01 Ready &lt;none&gt; 16d v1.13.4# kubectl drain node02 --ignore-daemonsetsnode/node02 cordonedWARNING: Ignoring DaemonSet-managed pods: kube-flannel-ds-amd64-8zw8z, kube-proxy-2hlp7pod/rbd-provisioner-6447467945-p2dgr evictedpod/coredns-86c58d9df4-6fp9p evictednode/node02 evicted# ansible node02 -a "systemctl stop kubelet"# ansible node02 -m copy -a 'src=kubeadm dest=/usr/bin/'# ansible node02 -m copy -a 'src=kubectl dest=/usr/bin/'# ansible node02 -m copy -a 'src=kubelet dest=/usr/bin/'# ansible node02 -a "systemctl start kubelet"# kubectl uncordon node02node/node02 uncordoned# kubectl get nodes|grep node02node02 Ready &lt;none&gt; 16d v1.13.4# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 16d v1.13.4node01 Ready &lt;none&gt; 16d v1.13.4node02 Ready &lt;none&gt; 16d v1.13.4]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过Dex和openLDAP进行Kubernetes身份验证]]></title>
    <url>%2Fblog%2Fdex%2F</url>
    <content type="text"><![CDATA[Dex是一种身份服务，它使用OpenID Connect(简称OIDC)来驱动其他应用程序的身份验证。 Dex通过“connectors.”充当其他身份提供商的门户。这使得dex可以将身份验证延迟(找不到很好的词来形容，只能硬翻了)到LDAP服务器、SAML提供程序或已建立的身份提供程序（如GitHub，Google和Active Directory）。客户端编写一次身份验证逻辑与dex通信，然后dex处理给定后端的协议。 OAuth2OAuth（开放授权）是一个开放标准，允许用户授权第三方移动应用访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方移动应用或分享他们数据的所有内容。 我相信大家都使用过类似“使用QQ登录”诸如此类的按钮来登录一些第三方的应用或者网站。在这些情况下，第三方应用程序(网站)选择让外部提供商（在这种情况下为QQ）证明您的身份，而不是让您使用应用程序本身设置用户名和密码。 服务器端应用程序的一般流程是： 新用户访问应用程序。 用户点击网站上的登录按钮(诸如“使用QQ登录”)，该应用程序将用户重定向到QQ。 用户登录QQ，然后QQ会提示该应用程序会获取的相应权限。 如果用户单击“授权并登录”，则QQ会连同获取的Access Token使用代码将用户重定向回该应用程序。 应用程序通过Access Token获取用户的OpenID；调用OpenAPI，来请求访问或修改用户授权的资源。 在这些情况下，dex充当QQ（在OpenID Connect中称为“provider”），而客户端应用程序重定向到它以获得最终用户的身份。 关于OAuth: https://oauth.net/2/ ID TokensID Tokens是OpenID Connect和dex主要功能引入的OAuth2扩展。ID Tokens是由dex签名的JSON Web令牌（JWT），作为OAuth2响应的一部分返回，用于证明最终用户的身份。 OpenID Connect的OAuth2主要扩展名是令牌响应中返回的额外令牌，称为ID Tokens。此令牌是由OpenID Connect服务器签名的JSON Web令牌，具有用户ID，名称，电子邮件等众所周知的字段。 Connectors当用户通过dex登录时，用户的身份通常存储在另一个用户管理系统中：LDAP目录，GitHub组织等。Dex充当客户端应用程序和上游身份提供者之间的中间人。客户端只需要了解OpenID Connect来查询dex，而dex实现了一组用于查询其他用户管理系统的协议。 OpenID Connect TokensOpenID Connect 1.0是OAuth 2.0协议之上的简单身份层。它允许客户端根据授权服务器执行的身份验证来验证最终用户的身份，以及以可互操作和类似REST的方式获取有关最终用户的基本配置文件信息。 OpenID Connect允许所有类型的客户端（包括基于Web，移动和JavaScript客户端）请求和接收有关经过身份验证的会话和最终用户的信息。规范套件是可扩展的，允许参与者在对它们有意义时使用可选功能，例如身份数据加密，OpenID提供程序的发现和会话管理。 协议的OAuth2的主要扩展是返回的附加字段，其中访问令牌称为ID Token。此令牌是JSON Web令牌（JWT），具有由服务器签名的众所周知的字段，例如用户的电子邮件。 为了识别用户，验证者使用OAuth2 令牌响应中的id_token（而不是access_token） 作为承载令牌。 来自OpenID Connect提供商的令牌响应包括一个称为ID令牌的签名JWT。ID令牌包含名称，电子邮件，唯一标识符，在dex的情况下，包含一组可用于标识用户的组。像dex这样的OpenID Connect提供程序发布公钥; Kubernetes API服务器了解如何使用它们来验证ID令牌。 关于OpenID Connect: https://openid.net/connect/ 身份验证流程如下所示： OAuth2客户端通过dex登录用户。 在与Kubernetes API通信时，该客户端使用返回的ID令牌作为承载令牌。 Kubernetes使用dex的公钥来验证ID令牌。 指定为用户名（以及可选的组信息）的声明将与该请求相关联。 用户名和组信息可以与Kubernetes 授权插件（例如基于角色的访问控制（RBAC））结合使用以实施策略。 dex有自己的用户概念，但它允许它们以不同的方式进行身份验证，称为connectors。目前，dex提供两种类型的连接器：local连接器和OIDC连接器。使用local连接器进行身份验证时，用户使用电子邮件和密码登录，并使用dex本身提供的可自定义UI。使用OIDC连接器，用户可以通过登录到另一个OIDC身份提供商（如Google或Salesforce）进行身份验证。 从dex请求ID令牌直接使用dex对用户进行身份验证的应用程序使用OAuth2代码流来请求令牌响应。采取的确切步骤是： 用户访问客户端应用。 客户端应用程序通过OAuth2请求将用户重定向到dex。 Dex确定用户的身份。 Dex使用代码将用户重定向到客户端。 客户端使用dex为id_token交换代码。 部署 openLDAP配置PVC12345678910111213141516171819202122232425262728293031323334# vim openldap-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-data namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi"---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-config namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "1Gi"# kubectl create -f openldap-pvc.yaml persistentvolumeclaim/ldap-data createdpersistentvolumeclaim/ldap-config created 部署openLDAP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# vim openldap.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ldap labels: app: ldapspec: replicas: 1 template: metadata: labels: app: ldap spec: containers: - name: ldap image: 192.168.100.100/library/openldap:1.2.2 volumeMounts: - name: ldap-data mountPath: /var/lib/ldap - name: ldap-config mountPath: /etc/ldap/slapd.d - name: ldap-certs mountPath: /container/service/slapd/assets/certs ports: - containerPort: 389 name: openldap env: - name: LDAP_LOG_LEVEL value: "256" - name: LDAP_ORGANISATION value: "zhi" - name: LDAP_DOMAIN value: "flywzj.com" - name: LDAP_ADMIN_PASSWORD value: "admin" - name: LDAP_CONFIG_PASSWORD value: "config" - name: LDAP_READONLY_USER value: "false" - name: LDAP_READONLY_USER_USERNAME value: "readonly" - name: LDAP_READONLY_USER_PASSWORD value: "readonly" - name: LDAP_RFC2307BIS_SCHEMA value: "false" - name: LDAP_BACKEND value: "mdb" - name: LDAP_TLS value: "true" - name: LDAP_TLS_CRT_FILENAME value: "ldap.crt" - name: LDAP_TLS_KEY_FILENAME value: "ldap.key" - name: LDAP_TLS_CA_CRT_FILENAME value: "ca.crt" - name: LDAP_TLS_ENFORCE value: "false" - name: LDAP_TLS_CIPHER_SUITE value: "SECURE256:+SECURE128:-VERS-TLS-ALL:+VERS-TLS1.2:-RSA:-DHE-DSS:-CAMELLIA-128-CBC:-CAMELLIA-256-CBC" - name: LDAP_TLS_VERIFY_CLIENT value: "demand" - name: LDAP_REPLICATION value: "false" - name: LDAP_REPLICATION_CONFIG_SYNCPROV value: "binddn=\"cn=admin,cn=config\" bindmethod=simple credentials=$LDAP_CONFIG_PASSWORD searchbase=\"cn=config\" type=refreshAndPersist retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_DB_SYNCPROV value: "binddn=\"cn=admin,$LDAP_BASE_DN\" bindmethod=simple credentials=$LDAP_ADMIN_PASSWORD searchbase=\"$LDAP_BASE_DN\" type=refreshAndPersist interval=00:00:00:10 retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_HOSTS value: "#PYTHON2BASH:['ldap://ldap-one-service', 'ldap://ldap-two-service']" - name: KEEP_EXISTING_CONFIG value: "false" - name: LDAP_REMOVE_CONFIG_AFTER_SETUP value: "true" - name: LDAP_SSL_HELPER_PREFIX value: "ldap" volumes: - name: ldap-data persistentVolumeClaim: claimName: ldap-data #hostPath: # path: "/data/ldap/db" - name: ldap-config persistentVolumeClaim: claimName: ldap-config #hostPath: # path: "/data/ldap/config" - name: ldap-certs hostPath: path: "/data/ldap/certs"---apiVersion: v1kind: Servicemetadata: labels: app: ldap name: ldapspec: type: NodePort ports: - port: 389 nodePort: 38989 selector: app: ldap# kubectl create -f openldap.yaml deployment.extensions/ldap createdservice/ldap created# kubectl get pods -l app=ldapNAME READY STATUS RESTARTS AGEldap-65f5786ff8-xrtkk 1/1 Running 0 28d 部署openldapadmin12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# vim phpldapadmin.yaml apiVersion: v1kind: ReplicationControllermetadata: name: phpldapadmin-controller labels: app: phpldapadminspec: replicas: 1 selector: app: phpldapadmin template: metadata: labels: app: phpldapadmin spec: containers: - name: phpldapadmin image: 192.168.100.100/library/phpldapadmin:0.7.2 volumeMounts: - name: phpldapadmin-certs mountPath: /container/service/phpldapadmin/assets/apache2/certs - name: ldap-client-certs mountPath: /container/service/ldap-client/assets/certs ports: - containerPort: 443 env: - name: PHPLDAPADMIN_LDAP_HOSTS #value: "#PYTHON2BASH:[&#123;'ldap-service': [&#123;'server': [&#123;'tls': 'true'&#125;]&#125;]&#125;]" value: "ldap" - name: PHPLDAPADMIN_SERVER_ADMIN value: "wangzhijiansd@qq.com" - name: PHPLDAPADMIN_SERVER_PATH value: "/phpldapadmin" - name: PHPLDAPADMIN_HTTPS value: "true" - name: PHPLDAPADMIN_HTTPS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_HTTPS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_HTTPS_CA_CRT_FILENAME value: "ca.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS value: "true" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_REQCERT value: "demand" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CA_CRT_FILENAME value: "ca.crt" volumes: - name: phpldapadmin-certs hostPath: path: "/data/phpldapadmin/ssl/" - name: ldap-client-certs hostPath: path: "/data/phpldapadmin/ldap-client-certs/"---apiVersion: v1kind: Servicemetadata: labels: app: phpldapadmin name: phpldapadminspec: type: NodePort ports: - port: 443 nodePort: 32001 selector: app: phpldapadmin# kubectl create -f phpldapadmin.yaml replicationcontroller/phpldapadmin-controller createdservice/phpldapadmin created# kubectl get pods -l app=phpldapadminNAME READY STATUS RESTARTS AGEphpldapadmin-controller-4cwlb 1/1 Running 0 28d 关于 openLDAP 的相关镜像详见: https://github.com/osixia/docker-openldap https://github.com/osixia/docker-openldap-backup https://github.com/osixia/docker-phpLDAPadmin 配置 openLDAP使用 phpldapadmin 进行配置管理 输入 https://nodeip:nodeport 登录 login: Login DN: cn=admin,dc=flywzj,dc=com Password: admin 点击 Authenticate 登录 登录LDAP容器进行配置管理 注1:Dex目前允许不安全的连接，但是Dex官方强烈建议使用TLS，通过使用端口636而不是389来实现。这里使用的是不安全的389端口来实现，请知悉。 注2:这里配置两个组，组k8s关联用户wang，组test关联用户zhi 查看当前LDAP配置1234567891011121314151617181920212223242526272829303132# kubectl exec -it ldap-65f5786ff8-xrtkk /bin/bashroot@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NFpVZU5IaGhDSzZ4OWF2KzBCSjlZOUY4SzRhWTdpWUk=# search resultsearch: 2result: 0 Success# numResponses: 3# numEntries: 2 配置新建OU和组123456789101112131415161718192021222324252627282930313233root@ldap-65f5786ff8-xrtkk:/# cat &lt;&lt;EOF &gt; container/service/slapd/assets/groups.ldifdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: topdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: topdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberuid: wangdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberuid: zhiEOFroot@ldap-65f5786ff8-xrtkk:/# ldapadd -x -D "cn=admin,dc=flywzj,dc=com" -w admin -f /container/service/slapd/assets/groups.ldif -H ldap:/// adding new entry "ou=Groups,dc=flywzj,dc=com"adding new entry "ou=People,dc=flywzj,dc=com"adding new entry "cn=k8s,ou=Groups,dc=flywzj,dc=com"adding new entry "cn=test,ou=Groups,dc=flywzj,dc=com" 如上可以看出，这里新建了两个OU: ou=Groups,dc=flywzj,dc=com ou=People,dc=flywzj,dc=com 同时新建了两个组: cn=k8s,ou=Groups,dc=flywzj,dc=com cn=test,ou=Groups,dc=flywzj,dc=com 特别说明，所有配置是都可以放在一个 ldif 文件中来进行配置的，这里分成两个 ldif 文件来配置就是为了方便理解和排版。 查看配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859root@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NHNid2tMZCtnSS9LazJieGRsdWdhZFN1OGR0ZE4wVjA=# Groups, flywzj.comdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, flywzj.comdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, flywzj.comdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberUid: wang# test, Groups, flywzj.comdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberUid: zhi# search resultsearch: 2result: 0 Success# numResponses: 9# numEntries: 8 配置用户123456789101112131415161718192021222324252627282930313233343536373839# cat &lt;&lt;EOF &gt; /container/service/slapd/assets/users.ldif dn: uid=wang,ou=People,dc=flywzj,dc=comcn: wanggidnumber: 500givenname: wanghomedirectory: /home/users/wangloginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: wangzhijiansd@qq.comsn: wanguid: wanguidnumber: 1000userpassword: wangzhijiandn: uid=zhi,ou=People,dc=flywzj,dc=comhomedirectory: /home/users/zhiloginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: zhijiansd@163.comcn: zhigivenname: zhisn: zhiuid: zhiuidnumber: 1001gidnumber: 501userpassword: zhijianEOFroot@ldap-65f5786ff8-xrtkk:/# ldapadd -x -D "cn=admin,dc=flywzj,dc=com" -w admin -f /container/service/slapd/assets/users.ldif -H ldap:/// adding new entry "uid=wang,ou=People,dc=flywzj,dc=com"adding new entry "uid=zhi,ou=People,dc=flywzj,dc=com" 查看配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495root@ldap-65f5786ff8-xrtkk:/# ldapsearch -x -H ldap:// -b dc=flywzj,dc=com -D "cn=admin,dc=flywzj,dc=com" -w admin# extended LDIF## LDAPv3# base &lt;dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## flywzj.comdn: dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhidc: flywzj# admin, flywzj.comdn: cn=admin,dc=flywzj,dc=comobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e1NTSEF9NHNid2tMZCtnSS9LazJieGRsdWdhZFN1OGR0ZE4wVjA=# Groups, flywzj.comdn: ou=Groups,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, flywzj.comdn: ou=People,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, flywzj.comdn: cn=k8s,ou=Groups,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topmemberUid: wang# test, Groups, flywzj.comdn: cn=test,ou=Groups,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: topmemberUid: zhi# wang, People, flywzj.comdn: uid=wang,ou=People,dc=flywzj,dc=comcn: wanggidNumber: 500givenName: wanghomeDirectory: /home/users/wangloginShell: /bin/shobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: wangzhijiansd@qq.comsn: wanguid: wanguidNumber: 1000userPassword:: d2FuZ3poaWppYW4=# zhi, People, flywzj.comdn: uid=zhi,ou=People,dc=flywzj,dc=comhomeDirectory: /home/users/zhiloginShell: /bin/shobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonmail: zhijiansd@163.comcn: zhigivenName: zhisn: zhiuid: zhiuidNumber: 1001gidNumber: 501userPassword:: emhpamlhbg==# search resultsearch: 2result: 0 Success# numResponses: 9# numEntries: 8 这时候你也可以使用 phpldapadmin 登录查看，特别说明，如果登录进去出现了一些”?”提示，那是因为某些模板没有导入导致的，可忽略。 部署 Dex Dex详见:https://github.com/dexidp/dex 生成证书并配置secret 生成dex server和login application相关证书和secret 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# vim gencert.sh#!/bin/bashmkdir -p sslcat &lt;&lt; EOF &gt; ssl/req.cnf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = dexDNS.2 = dex.svc.cluster.localDNS.3 = loginappDNS.4 = loginapp.svc.cluster.localDNS.5 = login.flywzj.comIP.1 = 192.168.100.181IP.2 = 192.168.100.182IP.3 = 192.168.100.183EOFopenssl genrsa -out ssl/dex-ca-key.pem 2048openssl req -x509 -new -nodes -key ssl/dex-ca-key.pem -days 1000 -out ssl/dex-ca.pem -subj "/CN=kube-ca"openssl genrsa -out ssl/dex-app-key.pem 2048openssl req -new -key ssl/dex-app-key.pem -out ssl/dex-app-csr.pem -subj "/CN=kube-ca" -config ssl/req.cnfopenssl x509 -req -in ssl/dex-app-csr.pem -CA ssl/dex-ca.pem -CAkey ssl/dex-ca-key.pem -CAcreateserial -out ssl/dex-app.pem -days 1000 -extensions v3_req -extfile ssl/req.cnfkubectl create secret tls dex --cert=ssl/dex-app.pem --key=ssl/dex-app-key.pem kubectl create secret tls loginapp --cert=ssl/dex-app.pem --key=ssl/dex-app-key.pem # ./gencert.sh Generating RSA private key, 2048 bit long modulus........................+++.......................................+++e is 65537 (0x10001)Generating RSA private key, 2048 bit long modulus...........................................+++........................+++e is 65537 (0x10001)Signature oksubject=/CN=kube-caGetting CA Private Keysecret/dex createdsecret/loginapp created# kubectl get secret dexNAME TYPE DATA AGEdex kubernetes.io/tls 2 8h# kubectl get secret loginappNAME TYPE DATA AGEloginapp kubernetes.io/tls 2 8h 复制证书 用于为Dex签署SSL证书的CA文件需要复制到apiserver可以读取的位置 1# cp ssl/dex-ca.pem /etc/kubernetes/ssl/ 部署 Dex123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163# wget https://raw.githubusercontent.com/dexidp/dex/master/examples/k8s/dex.yaml# vim dex.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: dex name: dex namespace: default spec: replicas: 1 template: metadata: labels: app: dex spec: serviceAccountName: dex # This is created below containers: - image: 192.168.100.100/coreos/dex:v2.10.0 name: dex command: ["/usr/local/bin/dex", "serve", "/etc/dex/cfg/config.yaml"] ports: - name: https containerPort: 5556 volumeMounts: - name: config mountPath: /etc/dex/cfg - name: tls mountPath: /etc/dex/tls volumes: - name: config configMap: name: dex items: - key: config.yaml path: config.yaml - name: tls secret: secretName: dex---kind: ConfigMapapiVersion: v1metadata: name: dex namespace: default data: config.yaml: | issuer: https://192.168.100.185:32000 storage: type: kubernetes config: inCluster: true web: https: 0.0.0.0:5556 tlsCert: /etc/dex/tls/tls.crt tlsKey: /etc/dex/tls/tls.key logger: level: "debug" format: text connectors: - type: ldap id: ldap name: LDAP config: host: ldap:389 insecureNoSSL: true insecureSkipVerify: true bindDN: cn=admin,dc=flywzj,dc=com bindPW: admin userSearch: baseDN: ou=People,dc=flywzj,dc=com filter: "(objectClass=posixAccount)" username: mail idAttr: uid emailAttr: mail nameAttr: uid groupSearch: baseDN: ou=Groups,dc=flywzj,dc=com filter: "(objectClass=posixGroup)" userAttr: uid groupAttr: memberUid nameAttr: cn oauth2: skipApprovalScreen: true staticClients: - id: login redirectURIs: - 'https://192.168.100.185:32002/callback' name: 'Login App' secret: 4TORGiNV9M54BTk1v7dNuFSaI6hUjfjr enablePasswordDB: true staticPasswords: - email: "wangzhijiansd@qq.com" # bcrypt hash of the string "password" hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W" username: "admin" userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"---apiVersion: v1kind: Servicemetadata: name: dex namespace: default spec: type: NodePort ports: - name: dex port: 5556 protocol: TCP targetPort: 5556 nodePort: 32000 selector: app: dex---apiVersion: v1kind: ServiceAccountmetadata: labels: app: dex name: dex namespace: default ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: dexrules:- apiGroups: ["dex.coreos.com"] # API group created by dex resources: ["*"] verbs: ["*"]- apiGroups: ["apiextensions.k8s.io"] resources: ["customresourcedefinitions"] verbs: ["create"] # To manage its own resources, dex must be able to create customresourcedefinitions---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: dexroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: dexsubjects:- kind: ServiceAccount name: dex # Service account assigned to the dex pod, created above namespace: default # The namespace dex is running in# kubectl create -f dex.yaml deployment.extensions/dex createdconfigmap/dex createdservice/dex createdserviceaccount/dex createdclusterrole.rbac.authorization.k8s.io/dex createdclusterrolebinding.rbac.authorization.k8s.io/dex created# kubectl get pod --show-labels -l app=dex# kubectl get services dex 注意1: yaml文件虽然是从官方的，但是做了一些改动，特别是这里将官方的3个副本更改为了1个副本，因为使用3个副本事会产生错误，我在issue中翻阅到有说是NTP时钟不同步造成的。 注意2: 这里的 connectors 是LDAP，详细文档见:https://github.com/dexidp/dex/blob/master/Documentation/connectors/ldap.md 查看 OpenID Connect 发现12345678910111213141516171819202122232425262728293031323334353637# curl -k https://192.168.100.185:32000/.well-known/openid-configuration&#123; "issuer": "https://192.168.100.185:32000", "authorization_endpoint": "https://192.168.100.185:32000/auth", "token_endpoint": "https://192.168.100.185:32000/token", "jwks_uri": "https://192.168.100.185:32000/keys", "response_types_supported": [ "code" ], "subject_types_supported": [ "public" ], "id_token_signing_alg_values_supported": [ "RS256" ], "scopes_supported": [ "openid", "email", "groups", "profile", "offline_access" ], "token_endpoint_auth_methods_supported": [ "client_secret_basic" ], "claims_supported": [ "aud", "email", "email_verified", "exp", "iat", "iss", "locale", "name", "sub" ]&#125; 查看 JSON Web Key123456789101112# curl -k https://192.168.100.185:32000/keys&#123; "keys": [ &#123; "use": "sig", "kty": "RSA", "kid": "a813de5c6100949abc59317714e3b09abecf8641", "alg": "RS256", "n": "u7G_RoZEuDwiW7kLBCMjjJMm1NgnHIXiTznxABe3uW8GsdASqRhUsDH2zFceZZObKchHWrKpkPZS4SjvcThF785xoJ4-FlAcrsUd4agyN9uwrAeL_luOrXvl-i0QAUKIHlqbTfZmzBIaFhHnG0yXKgqkXzTarQxDeynWVrVTdWsm7P_BYjQ5dnIlZu1xeRzw-NWf5UAi9Csh1x82XMtlAbMgWlJoWI36yVCCGUdJYintSp-tOfjkPBUghIO7ju8fb22X5uOgRFMq_RkIpXs2asf5FapVQMpcX_WAK3vUhmfH5F0lQZ9Cv9U__k3rHKRS7XwkcSQ4OKf7Vxrx4LQEcQ", "e": "AQAB" &#125;&#125; 配置 Login App 一旦启动并运行dex，下一步就是编写使用dex驱动身份验证的应用程序。具体详见:https://github.com/dexidp/dex/blob/master/Documentation/using-dex.md。 这里我们使用 loginapp 来配置:https://github.com/fydrah/loginapp。 配置 Configmap12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# cat ssl/dex-ca.pem -----BEGIN CERTIFICATE-----MIIC9zCCAd+gAwIBAgIJAJPzlo1fzzxqMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNVBAMMB2t1YmUtY2EwHhcNMTkwMTA0MDYyNzM5WhcNMjEwOTMwMDYyNzM5WjASMRAwDgYDVQQDDAdrdWJlLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxnFM6sykmKdmr1Z4DADujeIUZXvz5ajy+xdjpfjrNhnr3002GsthAlN6lWjJP6dnuVN20M1W/oozF7OzMuSRNO6zLepAG2TrRl/1DZG9EFSN7m65HtxK0DA3RyZc4CuDj3ADT899yziyaVTvUjR8MxLWHeibgAanZ2Bdc8icd4zt7QCGmy3hYbZZMw6UhSlgKUT24m6hw+W167knbjG/U64x1Qzik0DCx0fqY26jdJVZN6AnOpAFoIJ6RNIS1X/fbPa5kzUeTNSbgCW64LS0uOtXBh9uICxaxhHthgKtcOXFn7UrLWVnMfmd0fVi3nV3Tu5j8swrXlDVX+vaTJvDawIDAQABo1AwTjAdBgNVHQ4EFgQUv7Ta12WZE11NQhjpFOWxHG+wcZUwHwYDVR0jBBgwFoAUv7Ta12WZE11NQhjpFOWxHG+wcZUwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAsixLWA/28uVQMPfsWAv2cfdJMMCCvI6Y7uXsLUrNiONG6Ay/pU+6Qc/2kmZJlo0bHLx7P8ncKNHyTPzO1IzMDvvu65TUFdNAnKkqK90IRfjwY+RJv2J2NEKXmWRCxaJG522Uc0aGaYo6BPfrrYInr1fTh6i+/ovF4smyCo2bMX1dI5i7TlhAD9qsOA36XVYg+w5w7jXoJTYh14oKsE23pXpTiuSah6KdnvrjilwuBpV/SG40TsnITwyeaQFWMwbzJkhbvMgRK2crRruUhcIumIE5TTfLVgKzJ5oisjgjrZ3oFw871L2HTPFI3C6Xpp6yV6LNpFGOizT8HTLp63CwQw==-----END CERTIFICATE-----# vim ca-cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: ca namespace: defaultdata: ca.pem: | -----BEGIN CERTIFICATE----- MIIC9zCCAd+gAwIBAgIJAJPzlo1fzzxqMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV BAMMB2t1YmUtY2EwHhcNMTkwMTA0MDYyNzM5WhcNMjEwOTMwMDYyNzM5WjASMRAw DgYDVQQDDAdrdWJlLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA xnFM6sykmKdmr1Z4DADujeIUZXvz5ajy+xdjpfjrNhnr3002GsthAlN6lWjJP6dn uVN20M1W/oozF7OzMuSRNO6zLepAG2TrRl/1DZG9EFSN7m65HtxK0DA3RyZc4CuD j3ADT899yziyaVTvUjR8MxLWHeibgAanZ2Bdc8icd4zt7QCGmy3hYbZZMw6UhSlg KUT24m6hw+W167knbjG/U64x1Qzik0DCx0fqY26jdJVZN6AnOpAFoIJ6RNIS1X/f bPa5kzUeTNSbgCW64LS0uOtXBh9uICxaxhHthgKtcOXFn7UrLWVnMfmd0fVi3nV3 Tu5j8swrXlDVX+vaTJvDawIDAQABo1AwTjAdBgNVHQ4EFgQUv7Ta12WZE11NQhjp FOWxHG+wcZUwHwYDVR0jBBgwFoAUv7Ta12WZE11NQhjpFOWxHG+wcZUwDAYDVR0T BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAsixLWA/28uVQMPfsWAv2cfdJMMCC vI6Y7uXsLUrNiONG6Ay/pU+6Qc/2kmZJlo0bHLx7P8ncKNHyTPzO1IzMDvvu65TU FdNAnKkqK90IRfjwY+RJv2J2NEKXmWRCxaJG522Uc0aGaYo6BPfrrYInr1fTh6i+ /ovF4smyCo2bMX1dI5i7TlhAD9qsOA36XVYg+w5w7jXoJTYh14oKsE23pXpTiuSa h6KdnvrjilwuBpV/SG40TsnITwyeaQFWMwbzJkhbvMgRK2crRruUhcIumIE5TTfL VgKzJ5oisjgjrZ3oFw871L2HTPFI3C6Xpp6yV6LNpFGOizT8HTLp63CwQw== -----END CERTIFICATE-----# kubectl create -f ca-cm.yaml configmap/ca created# kubectl get configmap caNAME DATA AGEca 1 27d 配置 Login App Configmap123456789101112131415161718192021222324252627# vim loginapp-cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: loginapp namespace: defaultdata: config.yaml: | debug: false client_id: "login" client_secret: 4TORGiNV9M54BTk1v7dNuFSaI6hUjfjr issuer_url: "https://192.168.100.185:32000" issuer_root_ca: "/etc/ssl/ca.pem" redirect_url: "https://192.168.100.185:32002/callback" tls_enabled: true tls_cert: "/etc/loginapp/tls/tls.crt" tls_key: "/etc/loginapp/tls/tls.key" listen: "https://0.0.0.0:5555" disable_choices: false extra_scopes: "groups" name: "Kubernetes Auth"# kubectl create -f loginapp-cm.yaml configmap/loginapp created# kubectl get configmap loginappNAME DATA AGEloginapp 1 24d 部署 Login App1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# vim loginapp-deploy.yml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: loginapp namespace: defaultspec: replicas: 3 template: metadata: labels: app: loginapp spec: containers: - image: 192.168.100.100/library/login-app:latest name: loginapp ports: - name: https containerPort: 5555 volumeMounts: - name: ca mountPath: /etc/ssl/ - name: config mountPath: /app/ - name: tls mountPath: /etc/loginapp/tls volumes: - name: ca configMap: name: ca items: - key: ca.pem path: ca.pem - name: config configMap: name: loginapp items: - key: config.yaml path: config.yaml - name: tls secret: secretName: loginapp---apiVersion: v1kind: Servicemetadata: name: loginapp namespace: defaultspec: type: NodePort ports: - name: loginapp port: 5555 protocol: TCP targetPort: 5555 nodePort: 32002 selector: app: loginapp# kubectl create -f loginapp-deploy.yml# kubectl get pod --show-labels -l app=loginapp# kubectl get service loginapp 配置 kubernetes 配置K8s Apiserver以使用OpenID Connect 身份验证插件，详见:https://github.com/dexidp/dex/blob/master/Documentation/kubernetes.md 配置 kube-apiserver12345--oidc-issuer-url=https://192.168.100.185:32000--oidc-client-id=loginapp--oidc-ca-file=/etc/kubernetes/ssl/dex-ca.pem--oidc-username-claim=email--oidc-groups-claim=groups 配置 RBAC赋予 k8s 组 cluster-admin 角色123456789101112131415# vim k8s.yml apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: ldap-cluster-admin namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: Group name: k8s# kubectl create -f k8s.yml clusterrolebinding.rbac.authorization.k8s.io/ldap-cluster-admin created 赋予 test 组相应权限123456789101112131415161718192021222324252627# vim test.yaml kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: ldap-test namespace: testrules:- apiGroups: [""] resources: ["pods"] verbs: ["get", "watch", "list"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: ldap-test namespace: testsubjects:- kind: Group name: test apiGroup: ""roleRef: kind: Role name: ldap-test apiGroup: ""# kubectl create -f test.yaml role.rbac.authorization.k8s.io/ldap-test createdrolebinding.rbac.authorization.k8s.io/ldap-test created 登录 Dex 获取 ID Tokens 浏览器输入 https://192.168.100.185:32002/ (loginapp的nodeport)进行登录 Authentication for clients : login(login 对应之前 dex 和 Login App Configmap 的配置) 点击 “Request Token” 进行登录 之后，会跳转至 Dex 的网址 https://192.168.100.185:32000 点击 “Log in with LDAP” 输入 Username 和 Password ，点击 “Login” 登录生成 id-token 根据提示将文件复制至~/.kube/config ID Tokens 简单解释 id-token 实际上有三个部分，每个部分都是Base64编码的JSON，以”.”来分割。第一部分提供令牌的元数据。第二部分提供身份信息，这称为有效负载。第三部分是签名，用于验证令牌是否由可信方发出。 安装包以使用 jq 命令1# yum -y install jq 解码第一部分12345# echo eyJhbGciOiJSUzI1NiIsImtpZCI6Ijg1MzQ0ZTZlYjk4N2Y5ODA2MjRhODY2MTM2ZWFmOTFmNjFkNDNlYWEifQ | base64 -d | jq&#123; "alg": "RS256", "kid": "85344e6eb987f980624a866136eaf91f61d43eaa"&#125; 解码第二部分12345678910111213141516# echo eyJpc3MiOiJodHRwczovLzE5Mi4xNjguMTAwLjE4NTozMjAwMCIsInN1YiI6IkNnUjNZVzVuRWdSc1pHRnciLCJhdWQiOiJsb2dpbiIsImV4cCI6MTU0Nzc5MTAzMCwiaWF0IjoxNTQ3NzA0NjMwLCJhenAiOiJsb2dpbiIsImF0X2hhc2giOiJvTzBLcTBTYy1qdE9ybVpUbTJpbG5RIiwiZW1haWwiOiJ3YW5nemhpamlhbnNkQHFxLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJncm91cHMiOlsiazhzIl0sIm5hbWUiOiJ3YW5nIn0 | base64 -d | jq&#123; "iss": "https://192.168.100.185:32000", "sub": "CgR3YW5nEgRsZGFw", "aud": "login", "exp": 1547791030, "iat": 1547704630, "azp": "login", "at_hash": "oO0Kq0Sc-jtOrmZTm2ilnQ", "email": "wangzhijiansd@qq.com", "email_verified": true, "groups": [ "k8s" ], "name": "wang"&#125; 检查用户权限 根据之前配置的 RBAC 来测试用户拥有的权限 测试用户 wang12345# kubectl get nodes --user=wangNAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 174d v1.13.0node02 Ready &lt;none&gt; 174d v1.13.0node03 Ready &lt;none&gt; 174d v1.13.0 测试用户 zhi123456# kubectl get nodes --user=zhiError from server (Forbidden): nodes is forbidden: User &quot;zhijiansd@163.com&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope# kubectl get pod -n test --user=zhiNAME READY STATUS RESTARTS AGEtest-nginx-75677f8b58-p8w6d 1/1 Running 0 35h 最后要感谢如下文章及其作者: https://kairen.github.io/2018/04/15/kubernetes/k8s-integration-ldap/ https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/ https://github.com/ObjectifLibre/k8s-ldap https://thenewstack.io/author/joel-speed/]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>dex</tag>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署openLDAP]]></title>
    <url>%2Fblog%2Fopenldap%2F</url>
    <content type="text"><![CDATA[LDAP 代表 轻量级目录访问协议。顾名思义，它是一种用于访问目录服务的基于X.500协议的轻量级目录服务。 LDAP信息模型基于条目。条目是具有全局唯一性的属性集合专有名称（DN）。DN用于明确指代条目。每个条目的属性都有一个类型和一个或多个值。这些类型通常是助记符字符串，例如“ cn ”表示公用名，或“ mail ”表示电子邮件地址。值的语法取决于属性类型。 在LDAP中，目录条目以分层树状结构排列。此外，LDAP允许您通过使用名为objectClass的特殊属性来控制条目中所需和允许的属性。objectClass属性的值确定条目必须遵守的模式规则。 更多简介请访问:http://www.openldap.org/doc/admin24/intro.html 部署 openLDAP安装 openLDAP12# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo# yum -y install openldap-servers openldap-clients openldap-devel OpenLDAP 2.3及更高版本已转换为使用动态运行时配置引擎slapd-config:http://www.openldap.org/doc/admin24/slapdconf2.html 生成LDAP密码12# slappasswd -s zhijian&#123;SSHA&#125;pQdy+1y8IfIw9ZgIExIdOsjC/tqsmb86 复制相关文件12# cp /usr/share/openldap-servers/slapd.ldif /etc/openldap/# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG 配置 slapd.ldif12345678910111213141516171819202122232425# vim /etc/openldap/slapd.ldif ## Server status monitoring#dn: olcDatabase=monitor,cn=configobjectClass: olcDatabaseConfigolcDatabase: monitorolcAccess: to * by dn.base="gidNumber=0+uidNumber=0,cn=peercred,cn=external,c n=auth" read by dn.base="cn=admin,dc=openldap,dc=flywzj,dc=com" read by * none## Backend database definitions#dn: olcDatabase=hdb,cn=configobjectClass: olcDatabaseConfigobjectClass: olcHdbConfigolcDatabase: hdbolcSuffix: dc=openldap,dc=flywzj,dc=comolcRootDN: cn=admin,dc=openldap,dc=flywzj,dc=comolcRootPW: &#123;SSHA&#125;OLvPaV6PzzgRSCDivSzY8xVwk4fEWfZ9olcDbDirectory: /var/lib/ldapolcDbIndex: objectClass eq,presolcDbIndex: ou,cn,mail,surname,givenname eq,pres,sub 测试配置12# slaptest -u config file testing succeeded 删除原始配置文件并重置配置123# rm -rf /etc/openldap/slapd.d/*# slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif# chown -R ldap:ldap /etc/openldap/slapd.d/ 启用 openLDAP12# systemctl start slapd# systemctl status slapd 检查服务器是否正在运行并正确配置12345678910111213141516171819# ldapsearch -x -b '' -s base '(objectclass=*)' namingContexts# extended LDIF## LDAPv3# base &lt;&gt; with scope baseObject# filter: (objectclass=*)# requesting: namingContexts ##dn:namingContexts: dc=openldap,dc=flywzj,dc=com# search resultsearch: 2result: 0 Success# numResponses: 2# numEntries: 1 部署 phpldapadmin安装 phpldapadmin1# yum -y install httpd php-ldap phpldapadmin 配置访问控制123456# vim /etc/httpd/conf.d/phpldapadmin.conf &lt;IfModule mod_authz_core.c&gt; # Apache 2.4 Require local Require ip 192.168.100 &lt;/IfModule&gt; 注:如上为允许本地和192.168.100.0网段访问phpldapadmin，其他Apache Require访问控制指令请自行搜索。 更改登录方式1234# vim /etc/phpldapadmin/config.php// line 397$servers-&gt;setValue('login','attr','dn');// $servers-&gt;setValue('login','attr','uid'); 导入默认 schema 模块12345678910111213141516171819202122232425262728# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=cosine,cn=schema,cn=config"# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=nis,cn=schema,cn=config"# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=inetorgperson,cn=schema,cn=config"# ldapsearch -LLLQY EXTERNAL -H ldapi:/// -b cn=schema,cn=config "(objectClass=olcSchemaConfig)" dndn: cn=schema,cn=configdn: cn=&#123;0&#125;core,cn=schema,cn=configdn: cn=&#123;1&#125;cosine,cn=schema,cn=configdn: cn=&#123;2&#125;nis,cn=schema,cn=configdn: cn=&#123;3&#125;inetorgperson,cn=schema,cn=config 注1:这里的模版对应在 phpldapadmin 里 “创建一个子条目” 下的 “Select a template for the creation process” 模版。 注2: 如出现 “Automatically removed objectClass from templateCourier Mail: Account: courierMailAccount removed from template as it is not defined in the schema” 问题，那既是没有导入相关 objectClass 模版的原因。我曾试着将余下的 schema 也导入进去，但是没有成功，余下的几个模版依然是disable的。 配置 DN12345678910# vim /etc/openldap/base.ldif dn: dc=openldap,dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: flywzj # ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/base.ldif Enter LDAP Password: adding new entry "dc=openldap,dc=flywzj,dc=com" 注:如未配置默认DC即登录 phpldapadmin 会提示 “This base cannot be created with PLA.”问题。 启用 phpldapadmin1# systemctl start httpd 浏览器使用 http://yourip/ldapadmin 进入 phpldapadmin,点击登录，在“登录DN”中输入”cn=admin,dc=openldap,dc=flywzj,dc=com”并输入相应密码即可登录创建条目。 使用命令创建 OU1234567891011121314151617181920212223242526272829303132# vim /etc/openldap/group.ldif dn: ou=Groups,dc=openldap,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: topdn: ou=People,dc=openldap,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: topdn: cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: topdn: cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: top# ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/group.ldif Enter LDAP Password: adding new entry "ou=Groups,dc=openldap,dc=flywzj,dc=com"adding new entry "ou=People,dc=openldap,dc=flywzj,dc=com"adding new entry "cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=com"adding new entry "cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=com" 注:可使用 phpldapadmin 来创建。 创建用户12345678910111213141516171819202122232425262728293031323334353637383940# vim /etc/openldap/user.ldif dn: uid=wang,ou=People,dc=openldap,dc=flywzj,dc=comcn: wanggidnumber: 500givenname: wanghomedirectory: /home/users/wangloginshell: /bin/shmail: wangzhijiansd@qq.comobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: wanguid: wanguidnumber: 1000userpassword: wangzhijiandn: uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=comcn: zhigidnumber: 501givenname: zhihomedirectory: /home/users/zhiloginshell: /bin/shmail: wangzhijiansd@qq.comobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: zhiuid: zhiuidnumber: 1001userpassword: zhijian# ldapadd -x -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -W -f /etc/openldap/user.ldif Enter LDAP Password: adding new entry "uid=wang,ou=People,dc=openldap,dc=flywzj,dc=com"adding new entry "uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=com" 查看具体用户123456789101112131415# ldapsearch -x -W -D "uid=wang,ou=People,dc=openldap,dc=flywzj,dc=com" -b "uid=wang,dc=openldap,ou=People,dc=flywzj,dc=com"Enter LDAP Password: # extended LDIF## LDAPv3# base &lt;uid=wang,dc=openldap,ou=People,dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## search resultsearch: 2result: 32 No such object# numResponses: 1 查看该CN下的所有条目1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# ldapsearch -x -H ldap:/// -b dc=openldap,dc=flywzj,dc=com -D "cn=admin,dc=openldap,dc=flywzj,dc=com" -WEnter LDAP Password: # extended LDIF## LDAPv3# base &lt;dc=openldap,dc=flywzj,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## openldap.flywzj.comdn: dc=openldap,dc=flywzj,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: flywzjdc: openldap# Groups, openldap.flywzj.comdn: ou=Groups,dc=openldap,dc=flywzj,dc=comou: GroupsobjectClass: organizationalUnitobjectClass: top# People, openldap.flywzj.comdn: ou=People,dc=openldap,dc=flywzj,dc=comou: PeopleobjectClass: organizationalUnitobjectClass: top# k8s, Groups, openldap.flywzj.comdn: cn=k8s,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: k8sgidNumber: 500objectClass: posixGroupobjectClass: top# test, Groups, openldap.flywzj.comdn: cn=test,ou=Groups,dc=openldap,dc=flywzj,dc=comcn: testgidNumber: 501objectClass: posixGroupobjectClass: top# wang, People, openldap.flywzj.comdn: uid=wang,ou=People,dc=openldap,dc=flywzj,dc=comcn: wanggidNumber: 500givenName: wanghomeDirectory: /home/users/wangloginShell: /bin/shmail: wangzhijiansd@qq.comobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: wanguid: wanguidNumber: 1000userPassword:: d2FuZ3poaWppYW4=# zhi, People, openldap.flywzj.comdn: uid=zhi,ou=People,dc=openldap,dc=flywzj,dc=comcn: zhigidNumber: 501givenName: zhihomeDirectory: /home/users/zhiloginShell: /bin/shmail: wangzhijiansd@qq.comobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: zhiuid: zhiuidNumber: 1001userPassword:: emhpamlhbg==# search resultsearch: 2result: 0 Success# numResponses: 8# numEntries: 7 注: 亦可使用 phpldapadmin 创建。 配置 zabbix 使用 LDAP 这里以配置 zabbix 使用 LDAP 来进行测试。 配置 zabbix 的 “管理” ===&gt; “认证” 下的 “LDAP settings”项: Enable LDAP authentication: √ LDAP主机: yourip 端口: 389(默认端口,如使用使用 ldaps 协议则为636) 基于 DN: dc=openldap,dc=flywzj,dc=com 搜索属性: uid 绑定 DN: cn=admin,dc=openldap,dc=flywzj,dc=com 绑定密码: 输入该DN的密码 测试认证[必需为一个正确的LDAP用户]-登录: 如如上配置的wang 测试认证[必需为一个正确的LDAP用户]-用户密码:如如上配置的wangzhijian 点击 “测试”，成功则会提示“LDAP登录成功”。 注: 如要使用 LDAP 登录 zabbix，还需在 zabbix 的 “用户群组” 下创建同名的用户并赋予其权限，之后配置”更新”默认认证方式为“LDAP”即可使用LDAP下的该用户进行登录。 使用 kubernetes 部署 openLDAP 详询: docker-openldap，在部署时请查看对比构建方的环境变量。如需进行定制构建，构建方也给了构建方法。 配置 PVC123456789101112131415161718192021222324252627282930313233# vim openldap-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-data namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi"---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: ldap-config namespace: default labels: app: ldapspec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "1Gi"# kubectl create -f openldap-pvc.yaml persistentvolumeclaim/ldap-data createdpersistentvolumeclaim/ldap-config created 配置openLDAP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# vim openldap.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ldap labels: app: ldapspec: replicas: 1 template: metadata: labels: app: ldap spec: containers: - name: ldap image: 192.168.100.100/library/openldap:1.2.2 volumeMounts: - name: ldap-data mountPath: /var/lib/ldap - name: ldap-config mountPath: /etc/ldap/slapd.d - name: ldap-certs mountPath: /container/service/slapd/assets/certs ports: - containerPort: 389 name: openldap env: - name: LDAP_LOG_LEVEL value: "256" - name: LDAP_ORGANISATION value: "zhi" - name: LDAP_DOMAIN value: "flywzj.com" - name: LDAP_ADMIN_PASSWORD value: "admin" - name: LDAP_CONFIG_PASSWORD value: "config" - name: LDAP_READONLY_USER value: "false" - name: LDAP_READONLY_USER_USERNAME value: "readonly" - name: LDAP_READONLY_USER_PASSWORD value: "readonly" - name: LDAP_RFC2307BIS_SCHEMA value: "false" - name: LDAP_BACKEND value: "mdb" - name: LDAP_TLS value: "true" - name: LDAP_TLS_CRT_FILENAME value: "ldap.crt" - name: LDAP_TLS_KEY_FILENAME value: "ldap.key" - name: LDAP_TLS_CA_CRT_FILENAME value: "ca.crt" - name: LDAP_TLS_ENFORCE value: "false" - name: LDAP_TLS_CIPHER_SUITE value: "SECURE256:+SECURE128:-VERS-TLS-ALL:+VERS-TLS1.2:-RSA:-DHE-DSS:-CAMELLIA-128-CBC:-CAMELLIA-256-CBC" - name: LDAP_TLS_VERIFY_CLIENT value: "demand" - name: LDAP_REPLICATION value: "false" - name: LDAP_REPLICATION_CONFIG_SYNCPROV value: "binddn=\"cn=admin,cn=config\" bindmethod=simple credentials=$LDAP_CONFIG_PASSWORD searchbase=\"cn=config\" type=refreshAndPersist retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_DB_SYNCPROV value: "binddn=\"cn=admin,$LDAP_BASE_DN\" bindmethod=simple credentials=$LDAP_ADMIN_PASSWORD searchbase=\"$LDAP_BASE_DN\" type=refreshAndPersist interval=00:00:00:10 retry=\"60 +\" timeout=1 starttls=critical" - name: LDAP_REPLICATION_HOSTS value: "#PYTHON2BASH:['ldap://ldap-one-service', 'ldap://ldap-two-service']" - name: KEEP_EXISTING_CONFIG value: "false" - name: LDAP_REMOVE_CONFIG_AFTER_SETUP value: "true" - name: LDAP_SSL_HELPER_PREFIX value: "ldap" volumes: - name: ldap-data persistentVolumeClaim: claimName: ldap-data #hostPath: # path: "/data/ldap/db" - name: ldap-config persistentVolumeClaim: claimName: ldap-config #hostPath: # path: "/data/ldap/config" - name: ldap-certs hostPath: path: "/data/ldap/certs"---apiVersion: v1kind: Servicemetadata: labels: app: ldap name: ldap-servicespec: type: NodePort ports: - port: 389 nodePort: 38989 selector: app: ldap# kubectl create -f openldap.yaml deployment.extensions/ldap createdservice/ldap-service created# kubectl get pods -l app=ldapNAME READY STATUS RESTARTS AGEldap-65f5786ff8-b9dnx 1/1 Running 1 11d 使用 kubernetes 部署 openldapadmin 详询: docker-phpLDAPadmin，部署时请详细查看对比构建方给定的环境变量。如需进行定制构建，构建方也给出了构建方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# vim phpldapadmin.yaml apiVersion: v1kind: ReplicationControllermetadata: name: phpldapadmin-controller labels: app: phpldapadminspec: replicas: 1 selector: app: phpldapadmin template: metadata: labels: app: phpldapadmin spec: containers: - name: phpldapadmin image: 192.168.100.100/library/phpldapadmin:0.7.2 volumeMounts: - name: phpldapadmin-certs mountPath: /container/service/phpldapadmin/assets/apache2/certs - name: ldap-client-certs mountPath: /container/service/ldap-client/assets/certs ports: - containerPort: 443 env: - name: PHPLDAPADMIN_LDAP_HOSTS #value: "#PYTHON2BASH:[&#123;'ldap-service': [&#123;'server': [&#123;'tls': 'true'&#125;]&#125;]&#125;]" value: "ldap-service" - name: PHPLDAPADMIN_SERVER_ADMIN value: "wangzhijiansd@qq.com" - name: PHPLDAPADMIN_SERVER_PATH value: "/phpldapadmin" - name: PHPLDAPADMIN_HTTPS value: "true" - name: PHPLDAPADMIN_HTTPS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_HTTPS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_HTTPS_CA_CRT_FILENAME value: "ca.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS value: "true" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_REQCERT value: "demand" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CRT_FILENAME value: "cert.crt" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_KEY_FILENAME value: "cert.key" - name: PHPLDAPADMIN_LDAP_CLIENT_TLS_CA_CRT_FILENAME value: "ca.crt" volumes: - name: phpldapadmin-certs hostPath: path: "/data/phpldapadmin/ssl/" - name: ldap-client-certs hostPath: path: "/data/phpldapadmin/ldap-client-certs/"---apiVersion: v1kind: Servicemetadata: labels: app: phpldapadmin name: phpldapadmin-servicespec: type: NodePort ports: - port: 443 nodePort: 32001 selector: app: phpldapadmin# kubectl create -f phpldapadmin.yaml replicationcontroller/phpldapadmin-controller createdservice/phpldapadmin-service created# kubectl get pods -l app=phpldapadminNAME READY STATUS RESTARTS AGEphpldapadmin-controller-mprjw 1/1 Running 2 14d 输入 https://nodeip:nodeport 点击”login”进行登陆: Login DN: cn=admin,dc=flywzj,dc=com Password: admin 点击 Authenticate 登陆 不在演示如何在 kubernetes 的 openLDAP 上创建用户，既然安装了 phpldapadmin ,用web来创建比较简单(phpldapadmin界面粗糙)。]]></content>
      <categories>
        <category>openldap</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[切换 zabbix 认证模式]]></title>
    <url>%2Fblog%2Fb5448413%2F</url>
    <content type="text"><![CDATA[以 zabbix 配置了 openLDAP 认证，现在想切换为默认的认证方式为例。 登陆数据库查看当前认证方式1234567891011121314151617# mysql -u root -pMariaDB [(none)]&gt; use zabbix;Database changedMariaDB [zabbix]&gt; show tables like 'config';+---------------------------+| Tables_in_zabbix (config) |+---------------------------+| config |+---------------------------+1 row in set (0.01 sec)MariaDB [zabbix]&gt; select authentication_type from config ;+---------------------+| authentication_type |+---------------------+| 1 |+---------------------+1 row in set (0.00 sec) 0 代表Internal,1 代表LDAP，2 代表HTTP。 更改认证方式为默认认证方式123456MariaDB [zabbix]&gt; update config set authentication_type=0;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0MariaDB [zabbix]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 更新 Admin 密码(如需要)12345678910# 查询Admin用户的IDMariaDB [zabbix]&gt; select * from users;# 更新Admin密码MariaDB [zabbix]&gt; update users set passwd=md5("zabbix") where userid='1';Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0MariaDB [zabbix]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)]]></content>
      <categories>
        <category>openldap</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署基于 Web 的 Kubernetes 多集群管理平台 -- 360 Wayne]]></title>
    <url>%2Fblog%2F360wayne%2F</url>
    <content type="text"><![CDATA[Wayne 是一个通用的、基于 Web 的 Kubernetes 多集群管理平台。通过可视化 Kubernetes 对象模板编辑的方式，降低业务接入成本， 拥有完整的权限管理系统，适应多租户场景，是一款适合企业级集群使用的发布平台。 特性 基于 RBAC（Role based access control）的权限管理：用户通过角色与部门和项目关联，拥有部门角色允许操作部门资源，拥有项目角色允许操作项目资源，更加适合多租户场景。 简化 Kubernetes 对象创建：提供基础 Kubernetes 对象配置文件添加方式，同时支持高级模式直接编辑 Json/Yaml文件创建 Kubernetes 对象。 LDAP/OAuth 2.0/DB 多种登录模式支持：集成企业级 LDAP 登录及 DB 登录模式，同时还可以实现 OAuth2 登录。 支持多集群、多租户：可以同时管理多个 Kubernetes 集群，并针对性添加特定配置，更方便的多集群、多租户管理。 提供完整审计模块：每次操作都会有完整的审计功能，追踪用于操作历史，同时支持用户自定义 webhook。 提供基于 APIKey 的开放接口调用：用户可自主申请相关 APIKey 并管理自己的部门和项目，运维人员也可以申请全局 APIKey 进行特定资源的全局管理。 保留完整的发布历史：用户可以便捷的找到任何一次历史发布，并可轻松进行回滚，以及基于特定历史版本更新 Kubernetes 资源。 具备完善的资源报表：用户可以轻松获取各项目的资源使用占比和历史上线频次（天级）以及其他基础数据的报表和图表。 提供基于严密权限校验的 Web shell：用户可以通过 Web shell 的形式进入发布的 Pod 进行操作，自带完整的权限校验。 提供站内通知系统：方便管理员推送集群、业务通知和故障处理报告等。 架构 部署 Wayne下载相关文件并部署 Wayne 依赖 MySQL 和 RabbitMQ，其中 MySQL 是必须的服务，用户存储系统的各种数据，RabbitMQ 是可选的，主要用户扩展审计功能使用。 这里使用了 ceph 进行数据持久化 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# git clone https://github.com/Qihoo360/wayne.git# cd wayne/hack/kubernetes/# vim dependency/mysql-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-wayne-pvc namespace: default labels: app: mysql-waynespec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "10Gi"# vim dependency/rabbitmq-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: rabbitmq-wayne-pvc namespace: default labels: app: rabbitmq-waynespec: storageClassName: default-rbd accessModes: - ReadWriteOnce resources: requests: storage: "5Gi" # kubectl apply -f dependency/persistentvolumeclaim/mysql-wayne-pvc createddeployment.extensions/mysql-wayne createdservice/mysql-wayne createdpersistentvolumeclaim/rabbitmq-wayne-pvc createddeployment.extensions/rabbitmq-wayne createdservice/rabbitmq-wayne created# kubectl get podNAME READY STATUS RESTARTS AGEmysql-wayne-75947575d-mc972 1/1 Running 0 107srabbitmq-wayne-7c6dd8f475-l4pqj 1/1 Running 0 106s# kubectl apply -f wayne/configmap/infra-wayne createddeployment.extensions/infra-wayne createddeployment.extensions/infra-wayne-woker createddeployment.extensions/infra-wayne-webhook createdservice/infra-wayne created# kubectl get podNAME READY STATUS RESTARTS AGEinfra-wayne-5d84cf49b4-lggzs 1/1 Running 0 7m44sinfra-wayne-webhook-85dcf87c48-w4tcj 1/1 Running 0 7m44sinfra-wayne-woker-84bff6f8c9-mt7h5 1/1 Running 0 7m44s 现在可以通过 http://yourip:NodePort 访问 Wayne 平台，默认管理员账号 密码admin:admin。 注: 项目启动后还需要配置集群和 Namespace 等信息才可正常使用。 配置集群和 Namespace配置集群 进入后台创建集群并将 .kube/config 复制并粘贴至该集群下 1# cat .kube/config 配置 namespace 在 wayne 后台创建命名空间(需在 kubernetes 集群中进行创建,然后与 wayne 进行绑定) 12# kubectl create namespace testnamespace/test created 查看资源状况 配置完成,在左侧边栏的 kubernetes 菜单栏可以查看当前集群的相关 node 信息、deployment 信息以及 PV 信息。 应用 Wayne创建项目 返回前台，切换至当前集群的选项卡”创建项目” 创建部署 进入该项目部署页“创建部署”进行部署，之后“创建部署模板”，之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 部署成功后，可以选择“重启”、“下线”: 点击上线机房，通过弹出的选择框可以“进入容器”、“查看日志”: 创建负载均衡 点击该项目下左侧边栏的“负载均衡”项，之后“创建负载均衡”，配置“名称”和“机房”并提交。之后“创建负载均衡模板”,之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 创建ingress 点击该项目下左侧边栏的“ingress”项，之后“创建ingress”，配置“名称”和“机房”并提交。之后“创建ingress模板”,之后点击“发布”，在弹出的选择框中选择需要部署的机房并“确认”即可部署到kubernetes。 确认部署情况 注: 这里使用了 https 是因为我之前部署了 TLS 认证的 traefik。 更多详情请访问 Wayne 官方 wiki: wayne]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>wayne</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对etcd集群及kubernetes集群进行升级]]></title>
    <url>%2Fblog%2F819d304e%2F</url>
    <content type="text"><![CDATA[我的etcd集群和kubernetes集群都是二进制安装的，所以升级主要就是替换二进制文件。 这里我将原版本为3.3.8的etcd集群升级到3.3.10版本，将原版本为v1.11.1的kubernetes集群升级到v1.13.0版本，而我这里的kubernetes集群使用keepalived+haproxy做了双master的高可用、负载均衡，所以并无集群下线之忧。 升级 Etcd 集群升级检查查看集群健康状况12345678# ETCDCTL_API=3 # etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy 查看版本12# curl https://192.168.100.181:2379/version&#123;"etcdserver":"3.3.8","etcdcluster":"3.3.0"&#125; 使用快照备份 Etcd 集群 etcd leader拥有最新的应用程序数据，从leader获取快照etcd_server_is_leader 是1即为leader，否则为0。 123456789# curl -sL https://192.168.100.181:2379/metrics | grep etcd_server_is_leader# HELP etcd_server_is_leader Whether or not this member is a leader. 1 if is, 0 otherwise.# TYPE etcd_server_is_leader gaugeetcd_server_is_leader 0# curl -sL https://192.168.100.182:2379/metrics | grep etcd_server_is_leader# HELP etcd_server_is_leader Whether or not this member is a leader. 1 if is, 0 otherwise.# TYPE etcd_server_is_leader gaugeetcd_server_is_leader 1 当然，也可以使用该命令查看谁是leader 123456# etcdctl --endpoints=https://192.168.100.181:2379 member list3a406a85e3de7ef5: name=etcd-184 peerURLs=https://192.168.100.184:2380 clientURLs=https://192.168.100.184:2379 isLeader=false695714eeb38cebbe: name=etcd-181 peerURLs=https://192.168.100.181:2380 clientURLs=https://192.168.100.181:2379 isLeader=falseab8f0f710ce0bf85: name=etcd-183 peerURLs=https://192.168.100.183:2380 clientURLs=https://192.168.100.183:2379 isLeader=falsec5cb8024e23348b6: name=etcd-182 peerURLs=https://192.168.100.182:2380 clientURLs=https://192.168.100.182:2379 isLeader=trueceb2db537a9ec20d: name=etcd-185 peerURLs=https://192.168.100.185:2380 clientURLs=https://192.168.100.185:2379 isLeader=false 使用快照备份集群12345678# ETCDCTL_API=3 etcdctl --endpoints https://192.168.100.182:2379 snapshot save snapshotdbSnapshot saved at snapshotdb# ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb+----------+----------+------------+------------+| HASH | REVISION | TOTAL KEYS | TOTAL SIZE |+----------+----------+------------+------------+| c09e95e0 | 11794749 | 1226 | 19 MB |+----------+----------+------------+------------+ 下载并解压 Etcd1# tar -zxvf etcd-v3.3.10-linux-amd64.tar.gz 停止一个现有 Etcd 服务器1# systemctl stop etcd 替换 Etcd 二进制文件，使用相同配置重启 Etcd 服务器123456789101112# cp etcd-v3.3.10-linux-amd64/etcd /usr/bin/# cp etcd-v3.3.10-linux-amd64/etcdctl /usr/bin/# systemctl start etcd# systemctl status etcd# etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy 对其余成员重复如上步骤 在未升级的成员将记录以下警告，直到升级整个集群 123# systemctl status etcdthe local etcd version 3.3.8 is not up-to-datemember 695714eeb38cebbe has a higher version 3.3.10 查看集群成员健康状况和版本123456789101112131415161718# etcdctl --endpoints=https://192.168.100.181:2379 cluster-healthmember 3a406a85e3de7ef5 is healthy: got healthy result from https://192.168.100.184:2379member 695714eeb38cebbe is healthy: got healthy result from https://192.168.100.181:2379member ab8f0f710ce0bf85 is healthy: got healthy result from https://192.168.100.183:2379member c5cb8024e23348b6 is healthy: got healthy result from https://192.168.100.182:2379member ceb2db537a9ec20d is healthy: got healthy result from https://192.168.100.185:2379cluster is healthy# curl https://192.168.100.181:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.182:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.183:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.184:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125;# curl https://192.168.100.185:2379/version&#123;"etcdserver":"3.3.10","etcdcluster":"3.3.0"&#125; 升级 Kubernetes 集群查看当前集群版本12345# kubectl get node NAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 131d v1.11.1node02 Ready &lt;none&gt; 131d v1.11.1node03 Ready &lt;none&gt; 131d v1.11.1 下载并解压文件12# tar -zxvf kubernetes-server-linux-amd64.tar.gz# cd kubernetes/server/bin 升级 Master 节点停止 Master 节点相关组件123# systemctl stop kube-apiserver# systemctl stop kube-controller-manager# systemctl stop kube-scheduler 替换 Master 节点二进制组件1# cp kube-apiserver kube-controller-manager kube-scheduler kubeadm /usr/bin/ 重新启用 Master 节点12345678# systemctl start kube-apiserver# systemctl status kube-apiserver# systemctl start kube-controller-manager# systemctl status kube-controller-manager# systemctl start kube-scheduler# systemctl status kube-scheduler 在其他 Master 节点重复如上步骤进行升级 升级 Node 节点标记节点为不可调度 设置为不可调度后，新的 pod 不会迁移或者部署在该节点 12345# kubectl cordon node01node/node01 cordoned# kubectl get node | grep node01node01 Ready,SchedulingDisabled &lt;none&gt; 131d v1.11.1 迁移该节点的 Pod 迁移时注意系统瓶颈，当其他节点的CPU、内存或者本地存储资源不足，kubernetes都不会调用pod，pod会处于pending状态，直到重新上线该节点(或者扩容节点资源)，pod才会重新上线。 1234567# kubectl drain --ignore-daemonsets --delete-local-data node01 kubectl drain node01 --ignore-daemonsets --delete-local-datanode/node01 already cordonedWARNING: Ignoring DaemonSet-managed pods: ......; Deleting pods with local storage: ......pod/my-nginx-7ff9b54467-vk572 evicted......node/node01 evicted 注:对于DaemonSet-managed pods需要使用参数–ignore-daemonsets;迁移使用本地存储的pods需要使用参数–delete-local-data(移动到其他节点将清空数据)。 查看节点上是否还存在 Pods(DaemonSet pods忽略)1# kubectl get pod -o wide --all-namespaces | grep node01 查看 Pods 是否已移动到其他节点1# kubectl get pod -o wide --all-namespaces 停用该节点 Kubelet 和 Kube-proxy12# systemctl stop kubelet# systemctl stop kube-proxy 复制并替换相应二进制文件12# scp root@master1:/root/kubernetes/server/bin/kubelet /usr/bin/# scp root@master1:/root/kubernetes/server/bin/kube-proxy /usr/bin/ 启用该 Node 节点1234# systemctl start kubelet# systemctl status kubelet# systemctl start kube-proxy# systemctl status kube-proxy 在 Master 节点上解锁(重新上线)该 Node 节点12345# kubectl uncordon node01node/node01 uncordoned# kubectl get node | grep node01node01 Ready &lt;none&gt; 131d v1.13.0 在其他 Node 节点重复如上步骤以升级 Node 节点 查看系统是否升级成功12345# kubectl get node NAME STATUS ROLES AGE VERSIONnode01 Ready &lt;none&gt; 131d v1.13.0node02 Ready &lt;none&gt; 131d v1.13.0node03 Ready &lt;none&gt; 131d v1.13.0]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Harbor从1.5.1升级和迁移到1.6.2]]></title>
    <url>%2Fblog%2Fharbor%2F</url>
    <content type="text"><![CDATA[这次升级还算比较顺利，以前我从1.2版本升级到1.5版本没有升级成功，镜像全洗白了，所以这次升级我及其谨慎，官方文档看了又看(主要是文档排版太糟糕了)，生怕又给洗白了，当然结果是好的，成功升级。 官方改了三次数据库，从最早使用的MySQL迁移到MariaDB，从1.6.0开始又迁移到了Postgresql 在1.5.1版中我并没有安装运行Notary和Clair这两个组件 升级到1.6.2版后我新部署了Notary，Clair和Helm Chart这3个组件 备份Harbor停止Harbor12# cd harbor# docker-compose down 备份Harbor的当前文件,以便在必要时回滚到当前版本12# cd ..# mv harbor harbor-backup 下载迁移工具12# docker pull goharbor/harbor-migrator:v1.6.0goharbor/harbor-migrator v1.6.0 22775c4e4066 2 months ago 803MB 备份数据1234567# mkdir backup# docker run -it --rm -e DB_USR=root -e DB_PWD=root123 -v /data/database:/var/lib/mysql -v /root/harbor-backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg -v /root/backup:/harbor-migration/backup goharbor/harbor-migrator:v1.6.0 backup......Backup performed.Success to backup harbor.cfg.# ls backupharbor.cfg registry.sql 命令参考: docker run -it –rm -e DB_USR=root -e DB_PWD={db_pwd} -v ${harbor_db_path}:/var/lib/mysql -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg -v ${backup_path}:/harbor-migration/backup goharbor/harbor-migrator:[tag] backup 升级数据库架构、harbor.cfg并迁移数据 注意：您必须在启动Harbor之前运行Notary和Clair的DB的迁移。注意：在v1.6.0中，您需要执行三个连续步骤才能完全迁移Harbor，Notary和Clair的DB。 1234567891011# docker run -it --rm -e DB_USR=root -e DB_PWD=root123 -v /data/database:/var/lib/mysql -v /root/backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:v1.6.0 upPlease backup before upgrade, Enter y to continue updating or n to abort: yTrying to start mysql server...Waiting for MySQL start.........server stoppedThe path of the migrated harbor.cfg is not set, the input file will be overwritten.input version: 1.5.0, migrator chain: ['1.6.0']migrating to version 1.6.0Written new values to /harbor-migration/harbor-cfg/harbor.cfg 命令参考: docker run -it –rm -e DB_USR=root -e DB_PWD={db_pwd} -v ${harbor_db_path}:/var/lib/mysql -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg -v ${backup_path}:/harbor-migration/backup goharbor/harbor-migrator:[tag] backup 将harbor.cfg迁移至新版本的安装目录123456# docker run -it --rm -v /root/backup/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:v1.6.0 --cfg up# grep ^[a-z] backup/harbor.cfg# tar -zxvf harbor-offline-installer-v1.6.2.tgz # cd harbor# mv harbor.cfg harbor.cfg.bak# cp /root/backup/harbor.cfg /root/harbor 命令参考: docker run -it –rm -v ${harbor_cfg}:/harbor-migration/harbor-cfg/harbor.cfg goharbor/harbor-migrator:[tag] –cfg up 安装Harbor载入镜像12# docker load -i harbor.v1.6.2.tar.gz# docker images|grep 1.6.2 安装Notary，Clair和Helm Chart服务1234567# ./install.sh --with-notary --with-clair --with-chartmuseum......✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at https://192.168.100.100. For more details, please visit https://github.com/goharbor/harbor . 在安装升级过程中我又重新使用docker-compose命令安装了一次，供参考123456789101112131415161718192021# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml down -v# vim harbor.cfg# ./prepare --with-notary --with-clair --with-chartmuseum......The configuration files are ready, please use docker-compose to start the service.# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml up -d# docker-compose -f ./docker-compose.yml -f ./docker-compose.notary.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml psName Command State Ports -----------------------------------------------------------------------------chartmuseum /docker-entrypoint.sh Up (healthy) 9999/tcp clair /docker-entrypoint.sh Up (healthy) 6060/tcp, 6061/tcp harbor-adminserver /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-ui /harbor/start.sh Up (healthy) nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpnotary-server /bin/server-start.sh Up notary-signer /bin/signer-start.sh Up redis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp 如果要同时安装Notary，Clair和Helm Chart服务，则应在docker-compose和prepare命令中包含所有组件. 如上，harbor已经完成升级，可使用浏览器登陆harbor查看是否成功升级. Notary 使用 如果要启用内容信任以确保图像已签名，请在推送或拉取任何图像之前在命令行中设置两个环境变量： 12# export DOCKER_CONTENT_TRUST=1# export DOCKER_CONTENT_TRUST_SERVER=https://192.168.100.100:4443 这里以上传kubernetes-dashboard为例子说明notary的使用. 1234567891011121314151617181920212223# docker push 192.168.100.100/google_containers/kubernetes-dashboard-amd64:v1.10.0The push refers to repository [192.168.100.100/google_containers/kubernetes-dashboard-amd64]5f222ffea122: Pushed v1.10.0: digest: sha256:1d2e1229a918f4bc38b5a3f9f5f11302b3e71f8397b492afac7f273a0008776a size: 529Signing and pushing trust metadataYou are about to create a new root signing key passphrase. This passphrasewill be used to protect the most sensitive key in your signing system. Pleasechoose a long, complex passphrase and be careful to keep the password and thekey file itself secure and backed up. It is highly recommended that you use apassword manager to generate the passphrase and keep it safe. There will be noway to recover this key. You can find the key in your config directory.## 第一次push镜像，系统将要求您输入根密钥密码Enter passphrase for new root key with ID 7ffe68f: Repeat passphrase for new root key with ID 7ffe68f: ## 密码设置弱系统会进行提示Enter passphrase for new repository key with ID e8c208d: Passphrase is too short. Please use a password manager to generate and store a good random passphrase.Enter passphrase for new repository key with ID e8c208d: Repeat passphrase for new repository key with ID e8c208d: Finished initializing "192.168.100.100/google_containers/kubernetes-dashboard-amd64"Successfully signed 192.168.100.100/google_containers/kubernetes-dashboard-amd64:v1.10.0 注1: 根密钥生成于: /root/.docker/trust/private/镜像密码生成于: /root/.docker/trust/tuf/[registry name]/[imagepath]注2: 要使用notary，必须在Harbor中启用HTTPS.注3: 当镜像被签名时，它在UI中显示勾号; 否则，显示交叉符号（X）。注4:如果您省略标签，则跳过内容信任。提示”No tag specified, skipping trust metadata push”，所以即便是 latest 也需要提供镜像 tag 值。 通过Clair进行漏洞扫描Clair依靠漏洞元数据来完成分析过程。第一次初始安装后，Clair将自动开始从不同的漏洞存储库更新元数据数据库。更新过程可能需要一段时间，具体取决于数据大小和网络连接。如果数据库尚未完全填充，则存储库数据网格视图的页脚会显示警告消息。 数据库准备就绪后，整个数据库更新的时间戳将显示在“管理”下“ 配置”部分的“漏洞”选项卡中。这时候就可以进行漏洞扫描了。 注意：只有具有“项目管理员”角色的用户才有权启动分析过程。 分析过程可能显示如下状态： 未扫描：标签从未被扫描过。 排队：扫描任务已安排但尚未执行。 扫描：扫描过程正在进行中。 错误：扫描过程未能完成。 完成：扫描过程已成功完成。 关于漏洞的严重级别: 红色： 高安全漏洞的级别 橙色： 中等级别的漏洞 黄色： 漏洞程度低 灰色： 未知级别的漏洞 绿色： 没有漏洞 由于Harbor是由VMware中国的团队研发并开源的，对中文支持友好，对于使用问题无需过多担心。 附:有关Notary和Docker Content Trust的更多信息，请参阅Docker的文档：https://docs.docker.com/engine/security/trust/content_trust/关于Clair:https://github.com/coreos/clairHarbor用户指南: https://github.com/goharbor/harbor/blob/master/docs/user_guide.md]]></content>
      <categories>
        <category>镜像仓库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>harbor</tag>
        <tag>registry</tag>
        <tag>notary</tag>
        <tag>clair</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github构建博客]]></title>
    <url>%2Fblog%2Fhexo%2F</url>
    <content type="text"><![CDATA[在开始构建博客前，你需要在Github拥有一个账号,之后新建一个存储库(比如:zhijiansd.github.io),这里我就不再赘述了.接下来,我们需要为GitHub添加SSH key. 配置SSH key在本地创建秘钥,并将该秘钥复制下来12# ssh-keygen -t rsa -C "wangzhijiansd@qq.com"# cat /root/.ssh/id_rsa.pub 登录GitHub进行配置打开Github主页，依次点击Settings -&gt; SSH and GPG keys -&gt; New SSH key设置,自定义好Title,然后将上面复制的秘钥粘贴在Key下.进行测试1# ssh -T git@github.com 如果提示Are you sure you want to continue connecting (yes/no)?,输入yes.看到如下信息说明SSH已配置成功: Hi zhijiansd! You’’ve successfully authenticated, but GitHub does not provide shell access. 配置用户信息12# git config --global user.name "zhijiansd"# git config --global user.email "wangzhijiansd@qq.com" 安装Hexo并下载Next主题123456# yum -y install git nodejs # npm install hexo-cli -g# hexo init blog# cd blog# npm install# git clone https://github.com/theme-next/hexo-theme-next themes/next 更多主题详见Hexo. 配置Hexo更改默认主题为Next1# sed -i "s/landscape/next/g" _config.yml ###更改默认语言为汉语 12# grep language _config.yml language: zh-CN 配置Next更改Next主题外观123# grep scheme themes/next/_config.yml|grep Pisces scheme: Pisces# Only fit scheme Pisces 设置菜单1234567# vim themes/next/_config.ymlmenu: home: / || home //首页 #about: /about/ || user //关于 tags: /tags/ || tags //标签 categories: /categories/ || th //分类 archives: /archives/ || archive //归档 创建标签文件夹并添加type1234# hexo new page "tags"# vim source/tags/index.mdtype: "tags"comments: false 创建分类文件夹并添加type1234# hexo new page "categories"# vim source/categories/index.mdtype: "categories"comments: false 创建归档文件夹并添加type1234# hexo new page "archives"# vim source/archives/index.mdtype: archivescomments: false 设置头像123456# mkdir source/images# ls source/images/avatar.jpg# vim themes/next/_config.ymlavatar: url: /images/avatar.jpg 请自行将头像图片上传至source/images/文件夹下 修改文章内链接文本样式1234567891011# vim themes/next/source/css/_common/components/post/post.styl.post-body p a&#123; color: #0593d3; //原始链接颜色 border-bottom: none; border-bottom: 1px solid #0593d3; //底部分割线颜色 &amp;:hover &#123; color: #fc6423; //鼠标经过颜色 border-bottom: none; border-bottom: 1px solid #fc6423; //底部分割线颜色 &#125;&#125; 在文章末尾添加结束语 新建并配置passage-end-tag.swig文件 123456# vim themes/next/layout/_macro/passage-end-tag.swig&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt;-------------本文结束,感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 在post.swig文件的post-body之后，post-footer之前添加以下代码 123456# vim themes/next/layout/_macro/post.swig &lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125; &lt;/div&gt; 修改主题配置文件_config.yml，在末尾添加:如下: 123# vim themes/next/_config.ymlpassage_end_tag: enabled: true 实现文章统计功能 安装插件 1# npm install hexo-symbols-count-time --save 配置启用hexo配置文件的symbols项 1234567# vim _config.yml# Writingsymbols_count_time: symbols: true time: true total_symbols: true total_time: true 配置启用next主题配置文件的symbols项 1234567# vim themes/next/_config.ymlsymbols_count_time: separated_meta: true item_text_post: true item_text_total: true awl: 2 ##平均单词长度（单词的计数）。默认值:4。CN≈2 EN≈5 wpm: 300 ##每分钟的单词。默认值:275。缓慢≈200 正常≈275 快≈350 设置页面文章的篇数1234567891011# vim themes/next/_config.ymlindex_generator: per_page: 5archive_generator: per_page: 20 yearly: true monthly: truetag_generator: per_page: 10 启用访客量以及文章阅读量统计123456789101112131415161718192021# vim themes/next/_config.yml busuanzi_count: enable: true...... site_uv: true site_uv_header: 本站访客数 site_uv_footer: 人次 site_pv: true site_pv_header: 本站总访问量 site_pv_footer: 次 page_pv: true page_pv_header: 本文总阅读量 page_pv_footer: 次# vim themes/next/layout/_third-party/analytics/busuanzi-counter.swig ...... 本站总访问量&lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次...... 本站总访问量&lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次 给文章增加阴影效果12345678# vim themes/next/source/css/_custom/custom.styl.post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);&#125; 添加站内搜索12345# npm install hexo-generator-search --save# npm install hexo-generator-searchdb --save# vim themes/next/_config.yml local_search: enable: true 设置动态背景 配置启用next主题配置文件的canvas_nest项 123# vim themes/next/_config.yml canvas_nest: enable: true 在如下文件的行尾之前添加代码 1234# vim themes/next/layout/_layout.swig &#123;% if theme.canvas_nest %&#125; &lt;script type="text/javascript" color="255,0,255" opacity='0.7' zIndex="-2" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"&gt;&lt;/script&gt; &#123;% endif %&#125; 修改标签样式12# vim themes/next/layout/_macro/post.swig&lt;a href="&#123;&#123; url_for(tag.path) &#125;&#125;" rel="tag"&gt;&lt;i class="fa fa-tag"&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt; 在文件中搜索 rel=”tag”&gt;#,将 # 换成 ,不过这里的注释会直接显示改后的样式，上面就是更改后的样式，请参考. 在文章中插入图片123# npm install hexo-asset-image --save# vim _config.ymlpost_asset_folder: true 新建文章,/source/_posts文件夹内除了abcd.md文件还有一个同名的文件夹，在文章中按照默认格式即可在文章中插入图片(图片地址使用相对地址即可) 绘制流程图1# npm install --save hexo-filter-mermaid-diagrams 实验很多遍都没绘制出来,在这里只是想告诉大家 hexo 可以绘制流程图.详询:mermaid 修改永久链接的默认格式12345678# npm install hexo-abbrlink --save# vim _config.yml#permalink: :year/:month/:day/:title/#permalink_defaults:permalink: blog/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex 使用了该插件的话，同时使用了本地图片插件，注意图片路径的变化. 配置使用评论系统这里使用 valine 来部署，文档页详见:https://valine.js.org/quickstart.html 登录LeanCloud进行注册以获取APP ID 和 APP Key 进入控制台后点击 “应用” 下拉菜单 “创建新应用” 输入新应用名称,选择 “开发版” 并创建 应用创建好后，进入刚刚创建的应用 “设置” 页 进入 “设置” 项下的 “应用 Key” 项就能看到该应用的 APP ID 和 APP Key 最后配置启用 valine 并添加 APP ID 和 APP Key 即可启用评论系统 123456789101112# vim themes/next/_config.ymlvaline: enable: true appid: # your leancloud application appid appkey: # your leancloud application appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 欢迎评论 # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size visitor: false 分类页、标签页去评论: 在对应的md文件的顶部，加上 comments: false。如: 1234567# cat tags/index.md ---title: tagsdate: 2018-11-18 22:15:29type: "tags"comments: false--- 生成Hexo1# hexo g 开启预览访问端口123# hexo server -i 192.168.100.122INFO Start processingINFO Hexo is running at http://192.168.100.122:4000 . Press Ctrl+C to stop. 浏览器输入如上IP和端口即可在本地访问该博客 配置部署Hexo博客到GitHub 配置Hexo配置文件 12345# vim _config.ymldeploy: type: git repository: https://github.com/zhijiansd/zhijiansd.github.io branch: master 安装插件 1# npm install hexo-deployer-git --save 将Hexo部署到GitHub，之后浏览器输入zhijiansd.github.io查看 1# hexo d 绑定自己的域名 当部署后 hexo 会将相关内容进行复制并 push 到远程 master 分支的根目录下，这里我的是 source 文件夹 新建 CNAME 文件1234# cd source# touch CNAME# vim CNAME ###不要http以及www等前缀flywzj.com 添加两条类型为 “CNAME” 的记录，大致如下: 主机记录 记录类型 线路类型 记录值 @ CNAME 默认 zhijiansd.github.io. www CNAME 默认 zhijiansd.github.io. 注意: 我这里域名后的 “.” 是解析商自动加上去的 hexo的常用命令如下: 命令 解释 hexo init 初始化 hexo g 生成静态网页 hexo s 启动服务预览 hexo d 部署hexo hexo clean 清除缓存 hexo n 新建文章 hexo publish 草稿 注: NexT中文文档 Hexo中文文档]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
